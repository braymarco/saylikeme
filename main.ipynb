{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-SwE9oClj-mN",
        "outputId": "8334baca-06cf-41c0-b85b-23baa726e0f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9657 sha256=167249f6dbc1fe684f85817d3d533564e7406f725f0bf1c1c6bbc250d19baaea\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/f1/7f/5c94f0a7a505ca1c81cd1d9208ae2064675d97582078e6c769\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/m-bain/whisperx.git\n",
            "  Cloning https://github.com/m-bain/whisperx.git to /tmp/pip-req-build-k3uxnoje\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/m-bain/whisperx.git /tmp/pip-req-build-k3uxnoje\n",
            "  Resolved https://github.com/m-bain/whisperx.git to commit 46b416296fc576f48d70ead5d59783987c6f0478\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pyannote.audio@ git+https://github.com/pyannote/pyannote-audio@11b56a137a578db9335efc00298f6ec1932e6317 (from whisperx==3.1.0)\n",
            "  Cloning https://github.com/pyannote/pyannote-audio (to revision 11b56a137a578db9335efc00298f6ec1932e6317) to /tmp/pip-install-xd2m2x_o/pyannote-audio_f7d6aabf3d6c498fbd4c95a3be583b20\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/pyannote/pyannote-audio /tmp/pip-install-xd2m2x_o/pyannote-audio_f7d6aabf3d6c498fbd4c95a3be583b20\n",
            "  Running command git rev-parse -q --verify 'sha^11b56a137a578db9335efc00298f6ec1932e6317'\n",
            "  Running command git fetch -q https://github.com/pyannote/pyannote-audio 11b56a137a578db9335efc00298f6ec1932e6317\n",
            "  Running command git checkout -q 11b56a137a578db9335efc00298f6ec1932e6317\n",
            "  Resolved https://github.com/pyannote/pyannote-audio to commit 11b56a137a578db9335efc00298f6ec1932e6317\n",
            "  Running command git submodule update --init --recursive -q\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch==2.0.0 in /usr/local/lib/python3.10/dist-packages (from whisperx==3.1.0) (2.0.0+cu118)\n",
            "Requirement already satisfied: torchaudio==2.0.1 in /usr/local/lib/python3.10/dist-packages (from whisperx==3.1.0) (2.0.1+cu118)\n",
            "Collecting faster-whisper (from whisperx==3.1.0)\n",
            "  Downloading faster_whisper-0.5.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers (from whisperx==3.1.0)\n",
            "  Downloading transformers-4.29.0-py3-none-any.whl (7.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m45.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ffmpeg-python==0.2.0 (from whisperx==3.1.0)\n",
            "  Downloading ffmpeg_python-0.2.0-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from whisperx==3.1.0) (1.5.3)\n",
            "Collecting setuptools==65.6.3 (from whisperx==3.1.0)\n",
            "  Downloading setuptools-65.6.3-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m68.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from whisperx==3.1.0) (3.8.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from ffmpeg-python==0.2.0->whisperx==3.1.0) (0.18.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->whisperx==3.1.0) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->whisperx==3.1.0) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->whisperx==3.1.0) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->whisperx==3.1.0) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->whisperx==3.1.0) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->whisperx==3.1.0) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.0->whisperx==3.1.0) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.0->whisperx==3.1.0) (16.0.3)\n",
            "Collecting av==10.* (from faster-whisper->whisperx==3.1.0)\n",
            "  Downloading av-10.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.0/31.0 MB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ctranslate2<4,>=3.10 (from faster-whisper->whisperx==3.1.0)\n",
            "  Downloading ctranslate2-3.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.0/32.0 MB\u001b[0m \u001b[31m48.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub>=0.13 (from faster-whisper->whisperx==3.1.0)\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers==0.13.* (from faster-whisper->whisperx==3.1.0)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m59.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting onnxruntime==1.14.* (from faster-whisper->whisperx==3.1.0)\n",
            "  Downloading onnxruntime-1.14.1-cp310-cp310-manylinux_2_27_x86_64.whl (5.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m101.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting coloredlogs (from onnxruntime==1.14.*->faster-whisper->whisperx==3.1.0)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime==1.14.*->faster-whisper->whisperx==3.1.0) (23.3.3)\n",
            "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from onnxruntime==1.14.*->faster-whisper->whisperx==3.1.0) (1.22.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxruntime==1.14.*->faster-whisper->whisperx==3.1.0) (23.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime==1.14.*->faster-whisper->whisperx==3.1.0) (3.20.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->whisperx==3.1.0) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->whisperx==3.1.0) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->whisperx==3.1.0) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->whisperx==3.1.0) (4.65.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->whisperx==3.1.0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->whisperx==3.1.0) (2022.7.1)\n",
            "Collecting asteroid-filterbanks>=0.4 (from pyannote.audio@ git+https://github.com/pyannote/pyannote-audio@11b56a137a578db9335efc00298f6ec1932e6317->whisperx==3.1.0)\n",
            "  Downloading asteroid_filterbanks-0.4.0-py3-none-any.whl (29 kB)\n",
            "Collecting einops>=0.6.0 (from pyannote.audio@ git+https://github.com/pyannote/pyannote-audio@11b56a137a578db9335efc00298f6ec1932e6317->whisperx==3.1.0)\n",
            "  Downloading einops-0.6.1-py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting lightning>=2.0.1 (from pyannote.audio@ git+https://github.com/pyannote/pyannote-audio@11b56a137a578db9335efc00298f6ec1932e6317->whisperx==3.1.0)\n",
            "  Downloading lightning-2.0.2-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m73.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting omegaconf<3.0,>=2.1 (from pyannote.audio@ git+https://github.com/pyannote/pyannote-audio@11b56a137a578db9335efc00298f6ec1932e6317->whisperx==3.1.0)\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyannote.core>=5.0.0 (from pyannote.audio@ git+https://github.com/pyannote/pyannote-audio@11b56a137a578db9335efc00298f6ec1932e6317->whisperx==3.1.0)\n",
            "  Downloading pyannote.core-5.0.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.5/58.5 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyannote.database>=5.0.1 (from pyannote.audio@ git+https://github.com/pyannote/pyannote-audio@11b56a137a578db9335efc00298f6ec1932e6317->whisperx==3.1.0)\n",
            "  Downloading pyannote.database-5.0.1-py3-none-any.whl (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.1/48.1 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyannote.metrics>=3.2 (from pyannote.audio@ git+https://github.com/pyannote/pyannote-audio@11b56a137a578db9335efc00298f6ec1932e6317->whisperx==3.1.0)\n",
            "  Downloading pyannote.metrics-3.2.1-py3-none-any.whl (51 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.4/51.4 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyannote.pipeline>=2.3 (from pyannote.audio@ git+https://github.com/pyannote/pyannote-audio@11b56a137a578db9335efc00298f6ec1932e6317->whisperx==3.1.0)\n",
            "  Downloading pyannote.pipeline-2.3-py3-none-any.whl (30 kB)\n",
            "Collecting pytorch_metric_learning>=2.1.0 (from pyannote.audio@ git+https://github.com/pyannote/pyannote-audio@11b56a137a578db9335efc00298f6ec1932e6317->whisperx==3.1.0)\n",
            "  Downloading pytorch_metric_learning-2.1.1-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.8/110.8 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: rich>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio@ git+https://github.com/pyannote/pyannote-audio@11b56a137a578db9335efc00298f6ec1932e6317->whisperx==3.1.0) (13.3.4)\n",
            "Collecting semver>=3.0.0 (from pyannote.audio@ git+https://github.com/pyannote/pyannote-audio@11b56a137a578db9335efc00298f6ec1932e6317->whisperx==3.1.0)\n",
            "  Downloading semver-3.0.0-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio@ git+https://github.com/pyannote/pyannote-audio@11b56a137a578db9335efc00298f6ec1932e6317->whisperx==3.1.0) (0.12.1)\n",
            "Collecting speechbrain>=0.5.14 (from pyannote.audio@ git+https://github.com/pyannote/pyannote-audio@11b56a137a578db9335efc00298f6ec1932e6317->whisperx==3.1.0)\n",
            "  Downloading speechbrain-0.5.14-py3-none-any.whl (519 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.0/519.0 kB\u001b[0m \u001b[31m56.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorboardX>=2.6 (from pyannote.audio@ git+https://github.com/pyannote/pyannote-audio@11b56a137a578db9335efc00298f6ec1932e6317->whisperx==3.1.0)\n",
            "  Downloading tensorboardX-2.6-py2.py3-none-any.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch_audiomentations>=0.11.0 (from pyannote.audio@ git+https://github.com/pyannote/pyannote-audio@11b56a137a578db9335efc00298f6ec1932e6317->whisperx==3.1.0)\n",
            "  Downloading torch_audiomentations-0.11.0-py3-none-any.whl (47 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.9/47.9 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchmetrics>=0.11.0 (from pyannote.audio@ git+https://github.com/pyannote/pyannote-audio@11b56a137a578db9335efc00298f6ec1932e6317->whisperx==3.1.0)\n",
            "  Downloading torchmetrics-0.11.4-py3-none-any.whl (519 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.2/519.2 kB\u001b[0m \u001b[31m51.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers->whisperx==3.1.0) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers->whisperx==3.1.0) (2.27.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13->faster-whisper->whisperx==3.1.0) (2023.4.0)\n",
            "Collecting arrow<3.0,>=1.2.0 (from lightning>=2.0.1->pyannote.audio@ git+https://github.com/pyannote/pyannote-audio@11b56a137a578db9335efc00298f6ec1932e6317->whisperx==3.1.0)\n",
            "  Downloading arrow-1.2.3-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: beautifulsoup4<6.0,>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from lightning>=2.0.1->pyannote.audio@ git+https://github.com/pyannote/pyannote-audio@11b56a137a578db9335efc00298f6ec1932e6317->whisperx==3.1.0) (4.11.2)\n",
            "Collecting croniter<1.4.0,>=1.3.0 (from lightning>=2.0.1->pyannote.audio@ git+https://github.com/pyannote/pyannote-audio@11b56a137a578db9335efc00298f6ec1932e6317->whisperx==3.1.0)\n",
            "  Downloading croniter-1.3.14-py2.py3-none-any.whl (18 kB)\n",
            "Collecting dateutils<2.0 (from lightning>=2.0.1->pyannote.audio@ git+https://github.com/pyannote/pyannote-audio@11b56a137a578db9335efc00298f6ec1932e6317->whisperx==3.1.0)\n",
            "  Downloading dateutils-0.6.12-py2.py3-none-any.whl (5.7 kB)\n",
            "Collecting deepdiff<8.0,>=5.7.0 (from lightning>=2.0.1->pyannote.audio@ git+https://github.com/pyannote/pyannote-audio@11b56a137a578db9335efc00298f6ec1932e6317->whisperx==3.1.0)\n",
            "  Downloading deepdiff-6.3.0-py3-none-any.whl (69 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.7/69.7 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastapi<0.89.0,>=0.69.0 (from lightning>=2.0.1->pyannote.audio@ git+https://github.com/pyannote/pyannote-audio@11b56a137a578db9335efc00298f6ec1932e6317->whisperx==3.1.0)\n",
            "  Downloading fastapi-0.88.0-py3-none-any.whl (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.5/55.5 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting inquirer<5.0,>=2.10.0 (from lightning>=2.0.1->pyannote.audio@ git+https://github.com/pyannote/pyannote-audio@11b56a137a578db9335efc00298f6ec1932e6317->whisperx==3.1.0)\n",
            "  Downloading inquirer-3.1.3-py3-none-any.whl (18 kB)\n",
            "Collecting lightning-cloud>=0.5.34 (from lightning>=2.0.1->pyannote.audio@ git+https://github.com/pyannote/pyannote-audio@11b56a137a578db9335efc00298f6ec1932e6317->whisperx==3.1.0)\n",
            "  Downloading lightning_cloud-0.5.34-py3-none-any.whl (557 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 kB\u001b[0m \u001b[31m55.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting lightning-utilities<2.0,>=0.7.0 (from lightning>=2.0.1->pyannote.audio@ git+https://github.com/pyannote/pyannote-audio@11b56a137a578db9335efc00298f6ec1932e6317->whisperx==3.1.0)\n",
            "  Downloading lightning_utilities-0.8.0-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: psutil<7.0 in /usr/local/lib/python3.10/dist-packages (from lightning>=2.0.1->pyannote.audio@ git+https://github.com/pyannote/pyannote-audio@11b56a137a578db9335efc00298f6ec1932e6317->whisperx==3.1.0) (5.9.5)\n",
            "Requirement already satisfied: pydantic<4.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from lightning>=2.0.1->pyannote.audio@ git+https://github.com/pyannote/pyannote-audio@11b56a137a578db9335efc00298f6ec1932e6317->whisperx==3.1.0) (1.10.7)\n",
            "Collecting starlette (from lightning>=2.0.1->pyannote.audio@ git+https://github.com/pyannote/pyannote-audio@11b56a137a578db9335efc00298f6ec1932e6317->whisperx==3.1.0)\n",
            "  Downloading starlette-0.26.1-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.9/66.9 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting starsessions<2.0,>=1.2.1 (from lightning>=2.0.1->pyannote.audio@ git+https://github.com/pyannote/pyannote-audio@11b56a137a578db9335efc00298f6ec1932e6317->whisperx==3.1.0)\n",
            "  Downloading starsessions-1.3.0-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: traitlets<7.0,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from lightning>=2.0.1->pyannote.audio@ git+https://github.com/pyannote/pyannote-audio@11b56a137a578db9335efc00298f6ec1932e6317->whisperx==3.1.0) (5.7.1)\n",
            "Requirement already satisfied: urllib3<3.0 in /usr/local/lib/python3.10/dist-packages (from lightning>=2.0.1->pyannote.audio@ git+https://github.com/pyannote/pyannote-audio@11b56a137a578db9335efc00298f6ec1932e6317->whisperx==3.1.0) (1.26.15)\n",
            "Collecting uvicorn<2.0 (from lightning>=2.0.1->pyannote.audio@ git+https://github.com/pyannote/pyannote-audio@11b56a137a578db9335efc00298f6ec1932e6317->whisperx==3.1.0)\n",
            "  Downloading uvicorn-0.22.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: websocket-client<3.0 in /usr/local/lib/python3.10/dist-packages (from lightning>=2.0.1->pyannote.audio@ git+https://github.com/pyannote/pyannote-audio@11b56a137a578db9335efc00298f6ec1932e6317->whisperx==3.1.0) (1.5.1)\n",
            "Collecting websockets<12.0 (from lightning>=2.0.1->pyannote.audio@ git+https://github.com/pyannote/pyannote-audio@11b56a137a578db9335efc00298f6ec1932e6317->whisperx==3.1.0)\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pytorch-lightning (from lightning>=2.0.1->pyannote.audio@ git+https://github.com/pyannote/pyannote-audio@11b56a137a578db9335efc00298f6ec1932e6317->whisperx==3.1.0)\n",
            "  Downloading pytorch_lightning-2.0.2-py3-none-any.whl (719 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m719.0/719.0 kB\u001b[0m \u001b[31m67.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.0->whisperx==3.1.0) (2.1.2)\n",
            "Collecting antlr4-python3-runtime==4.9.* (from omegaconf<3.0,>=2.1->pyannote.audio@ git+https://github.com/pyannote/pyannote-audio@11b56a137a578db9335efc00298f6ec1932e6317->whisperx==3.1.0)\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: sortedcontainers>=2.0.4 in /usr/local/lib/python3.10/dist-packages (from pyannote.core>=5.0.0->pyannote.audio@ git+https://github.com/pyannote/pyannote-audio@11b56a137a578db9335efc00298f6ec1932e6317->whisperx==3.1.0) (2.4.0)\n",
            "Requirement already satisfied: scipy>=1.1 in /usr/local/lib/python3.10/dist-packages (from pyannote.core>=5.0.0->pyannote.audio@ git+https://github.com/pyannote/pyannote-audio@11b56a137a578db9335efc00298f6ec1932e6317->whisperx==3.1.0) (1.10.1)\n",
            "Requirement already satisfied: typer[all]>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from pyannote.database>=5.0.1->pyannote.audio@ git+https://github.com/pyannote/pyannote-audio@11b56a137a578db9335efc00298f6ec1932e6317->whisperx==3.1.0) (0.7.0)\n",
            "Requirement already satisfied: scikit-learn>=0.17.1 in /usr/local/lib/python3.10/dist-packages (from pyannote.metrics>=3.2->pyannote.audio@ git+https://github.com/pyannote/pyannote-audio@11b56a137a578db9335efc00298f6ec1932e6317->whisperx==3.1.0) (1.2.2)\n",
            "Collecting docopt>=0.6.2 (from pyannote.metrics>=3.2->pyannote.audio@ git+https://github.com/pyannote/pyannote-audio@11b56a137a578db9335efc00298f6ec1932e6317->whisperx==3.1.0)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from pyannote.metrics>=3.2->pyannote.audio@ git+https://github.com/pyannote/pyannote-audio@11b56a137a578db9335efc00298f6ec1932e6317->whisperx==3.1.0) (0.8.10)\n",
            "Requirement already satisfied: matplotlib>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pyannote.metrics>=3.2->pyannote.audio@ git+https://github.com/pyannote/pyannote-audio@11b56a137a578db9335efc00298f6ec1932e6317->whisperx==3.1.0) (3.7.1)\n",
            "Collecting optuna>=1.4 (from pyannote.pipeline>=2.3->pyannote.audio@ git+https://github.com/pyannote/pyannote-audio@11b56a137a578db9335efc00298f6ec1932e6317->whisperx==3.1.0)\n",
            "  Downloading optuna-3.1.1-py3-none-any.whl (365 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m365.7/365.7 kB\u001b[0m \u001b[31m45.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->whisperx==3.1.0) (1.16.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->whisperx==3.1.0) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->whisperx==3.1.0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->whisperx==3.1.0) (3.4)\n",
            "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.0.0->pyannote.audio@ git+https://github.com/pyannote/pyannote-audio@11b56a137a578db9335efc00298f6ec1932e6317->whisperx==3.1.0) (2.2.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.0.0->pyannote.audio@ git+https://github.com/pyannote/pyannote-audio@11b56a137a578db9335efc00298f6ec1932e6317->whisperx==3.1.0) (2.14.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.12.1->pyannote.audio@ git+https://github.com/pyannote/pyannote-audio@11b56a137a578db9335efc00298f6ec1932e6317->whisperx==3.1.0) (1.15.1)\n",
            "Collecting hyperpyyaml (from speechbrain>=0.5.14->pyannote.audio@ git+https://github.com/pyannote/pyannote-audio@11b56a137a578db9335efc00298f6ec1932e6317->whisperx==3.1.0)\n",
            "  Downloading HyperPyYAML-1.2.0-py3-none-any.whl (16 kB)\n",
            "Collecting sentencepiece (from speechbrain>=0.5.14->pyannote.audio@ git+https://github.com/pyannote/pyannote-audio@11b56a137a578db9335efc00298f6ec1932e6317->whisperx==3.1.0)\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m88.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.0->whisperx==3.1.0) (1.3.0)\n",
            "Collecting julius<0.3,>=0.2.3 (from torch_audiomentations>=0.11.0->pyannote.audio@ git+https://github.com/pyannote/pyannote-audio@11b56a137a578db9335efc00298f6ec1932e6317->whisperx==3.1.0)\n",
            "  Downloading julius-0.2.7.tar.gz (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.6/59.6 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: librosa>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from torch_audiomentations>=0.11.0->pyannote.audio@ git+https://github.com/pyannote/pyannote-audio@11b56a137a578db9335efc00298f6ec1932e6317->whisperx==3.1.0) (0.10.0.post2)\n",
            "Collecting torch-pitch-shift>=1.2.2 (from torch_audiomentations>=0.11.0->pyannote.audio@ git+https://github.com/pyannote/pyannote-audio@11b56a137a578db9335efc00298f6ec1932e6317->whisperx==3.1.0)\n",
            "  Downloading torch_pitch_shift-1.2.4-py3-none-any.whl (4.9 kB)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<6.0,>=4.8.0->lightning>=2.0.1->pyannote.audio@ git+https://github.com/pyannote/pyannote-audio@11b56a137a578db9335efc00298f6ec1932e6317->whisperx==3.1.0) (2.4.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.12.1->pyannote.audio@ git+https://github.com/pyannote/pyannote-audio@11b56a137a578db9335efc00298f6ec1932e6317->whisperx==3.1.0) (2.21)\n",
            "Collecting ordered-set<4.2.0,>=4.0.2 (from deepdiff<8.0,>=5.7.0->lightning>=2.0.1->pyannote.audio@ git+https://github.com/pyannote/pyannote-audio@11b56a137a578db9335efc00298f6ec1932e6317->whisperx==3.1.0)\n",
            "  Downloading ordered_set-4.1.0-py3-none-any.whl (7.6 kB)\n",
            "Collecting starlette (from lightning>=2.0.1->pyannote.audio@ git+https://github.com/pyannote/pyannote-audio@11b56a137a578db9335efc00298f6ec1932e6317->whisperx==3.1.0)\n",
            "  Downloading starlette-0.22.0-py3-none-any.whl (64 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.3/64.3 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette->lightning>=2.0.1->pyannote.audio@ git+https://github.com/pyannote/pyannote-audio@11b56a137a578db9335efc00298f6ec1932e6317->whisperx==3.1.0) (3.6.2)\n",
            "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec->huggingface-hub>=0.13->faster-whisper->whisperx==3.1.0)\n",
            "  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m59.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting blessed>=1.19.0 (from inquirer<5.0,>=2.10.0->lightning>=2.0.1->pyannote.audio@ git+https://github.com/pyannote/pyannote-audio@11b56a137a578db9335efc00298f6ec1932e6317->whisperx==3.1.0)\n",
            "  Downloading blessed-1.20.0-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.4/58.4 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-editor>=1.0.4 (from inquirer<5.0,>=2.10.0->lightning>=2.0.1->pyannote.audio@ git+https://github.com/pyannote/pyannote-audio@11b56a137a578db9335efc00298f6ec1932e6317->whisperx==3.1.0)\n",
            "  Downloading python_editor-1.0.4-py3-none-any.whl (4.9 kB)\n",
            "Collecting readchar>=3.0.6 (from inquirer<5.0,>=2.10.0->lightning>=2.0.1->pyannote.audio@ git+https://github.com/pyannote/pyannote-audio@11b56a137a578db9335efc00298f6ec1932e6317->whisperx==3.1.0)\n",
            "  Downloading readchar-4.0.5-py3-none-any.whl (8.5 kB)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.6.0->torch_audiomentations>=0.11.0->pyannote.audio@ git+https://github.com/pyannote/pyannote-audio@11b56a137a578db9335efc00298f6ec1932e6317->whisperx==3.1.0) (3.0.0)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.6.0->torch_audiomentations>=0.11.0->pyannote.audio@ git+https://github.com/pyannote/pyannote-audio@11b56a137a578db9335efc00298f6ec1932e6317->whisperx==3.1.0) (4.4.2)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.6.0->torch_audiomentations>=0.11.0->pyannote.audio@ git+https://github.com/pyannote/pyannote-audio@11b56a137a578db9335efc00298f6ec1932e6317->whisperx==3.1.0) (0.56.4)\n",
            "Requirement already satisfied: pooch<1.7,>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.6.0->torch_audiomentations>=0.11.0->pyannote.audio@ git+https://github.com/pyannote/pyannote-audio@11b56a137a578db9335efc00298f6ec1932e6317->whisperx==3.1.0) (1.6.0)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.6.0->torch_audiomentations>=0.11.0->pyannote.audio@ git+https://github.com/pyannote/pyannote-audio@11b56a137a578db9335efc00298f6ec1932e6317->whisperx==3.1.0) (0.3.5)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.6.0->torch_audiomentations>=0.11.0->pyannote.audio@ git+https://github.com/pyannote/pyannote-audio@11b56a137a578db9335efc00298f6ec1932e6317->whisperx==3.1.0) (0.2)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.6.0->torch_audiomentations>=0.11.0->pyannote.audio@ git+https://github.com/pyannote/pyannote-audio@11b56a137a578db9335efc00298f6ec1932e6317->whisperx==3.1.0) (1.0.5)\n",
            "Collecting pyjwt (from lightning-cloud>=0.5.34->lightning>=2.0.1->pyannote.audio@ git+https://github.com/pyannote/pyannote-audio@11b56a137a578db9335efc00298f6ec1932e6317->whisperx==3.1.0)\n",
            "  Downloading PyJWT-2.7.0-py3-none-any.whl (22 kB)\n",
            "Collecting python-multipart (from lightning-cloud>=0.5.34->lightning>=2.0.1->pyannote.audio@ git+https://github.com/pyannote/pyannote-audio@11b56a137a578db9335efc00298f6ec1932e6317->whisperx==3.1.0)\n",
            "  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py<3.0.0,>=2.2.0->rich>=12.0.0->pyannote.audio@ git+https://github.com/pyannote/pyannote-audio@11b56a137a578db9335efc00298f6ec1932e6317->whisperx==3.1.0) (0.1.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio@ git+https://github.com/pyannote/pyannote-audio@11b56a137a578db9335efc00298f6ec1932e6317->whisperx==3.1.0) (1.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio@ git+https://github.com/pyannote/pyannote-audio@11b56a137a578db9335efc00298f6ec1932e6317->whisperx==3.1.0) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio@ git+https://github.com/pyannote/pyannote-audio@11b56a137a578db9335efc00298f6ec1932e6317->whisperx==3.1.0) (4.39.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio@ git+https://github.com/pyannote/pyannote-audio@11b56a137a578db9335efc00298f6ec1932e6317->whisperx==3.1.0) (1.4.4)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio@ git+https://github.com/pyannote/pyannote-audio@11b56a137a578db9335efc00298f6ec1932e6317->whisperx==3.1.0) (8.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio@ git+https://github.com/pyannote/pyannote-audio@11b56a137a578db9335efc00298f6ec1932e6317->whisperx==3.1.0) (3.0.9)\n",
            "Collecting alembic>=1.5.0 (from optuna>=1.4->pyannote.pipeline>=2.3->pyannote.audio@ git+https://github.com/pyannote/pyannote-audio@11b56a137a578db9335efc00298f6ec1932e6317->whisperx==3.1.0)\n",
            "  Downloading alembic-1.10.4-py3-none-any.whl (212 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.9/212.9 kB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cmaes>=0.9.1 (from optuna>=1.4->pyannote.pipeline>=2.3->pyannote.audio@ git+https://github.com/pyannote/pyannote-audio@11b56a137a578db9335efc00298f6ec1932e6317->whisperx==3.1.0)\n",
            "  Downloading cmaes-0.9.1-py3-none-any.whl (21 kB)\n",
            "Collecting colorlog (from optuna>=1.4->pyannote.pipeline>=2.3->pyannote.audio@ git+https://github.com/pyannote/pyannote-audio@11b56a137a578db9335efc00298f6ec1932e6317->whisperx==3.1.0)\n",
            "  Downloading colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna>=1.4->pyannote.pipeline>=2.3->pyannote.audio@ git+https://github.com/pyannote/pyannote-audio@11b56a137a578db9335efc00298f6ec1932e6317->whisperx==3.1.0) (2.0.10)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.17.1->pyannote.metrics>=3.2->pyannote.audio@ git+https://github.com/pyannote/pyannote-audio@11b56a137a578db9335efc00298f6ec1932e6317->whisperx==3.1.0) (3.1.0)\n",
            "Requirement already satisfied: itsdangerous<3.0.0,>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from starsessions<2.0,>=1.2.1->lightning>=2.0.1->pyannote.audio@ git+https://github.com/pyannote/pyannote-audio@11b56a137a578db9335efc00298f6ec1932e6317->whisperx==3.1.0) (2.1.2)\n",
            "Collecting primePy>=1.3 (from torch-pitch-shift>=1.2.2->torch_audiomentations>=0.11.0->pyannote.audio@ git+https://github.com/pyannote/pyannote-audio@11b56a137a578db9335efc00298f6ec1932e6317->whisperx==3.1.0)\n",
            "  Downloading primePy-1.3-py3-none-any.whl (4.0 kB)\n",
            "Collecting colorama<0.5.0,>=0.4.3 (from typer[all]>=0.2.1->pyannote.database>=5.0.1->pyannote.audio@ git+https://github.com/pyannote/pyannote-audio@11b56a137a578db9335efc00298f6ec1932e6317->whisperx==3.1.0)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Collecting shellingham<2.0.0,>=1.3.0 (from typer[all]>=0.2.1->pyannote.database>=5.0.1->pyannote.audio@ git+https://github.com/pyannote/pyannote-audio@11b56a137a578db9335efc00298f6ec1932e6317->whisperx==3.1.0)\n",
            "  Downloading shellingham-1.5.0.post1-py2.py3-none-any.whl (9.4 kB)\n",
            "Collecting rich>=12.0.0 (from pyannote.audio@ git+https://github.com/pyannote/pyannote-audio@11b56a137a578db9335efc00298f6ec1932e6317->whisperx==3.1.0)\n",
            "  Downloading rich-12.6.0-py3-none-any.whl (237 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m237.5/237.5 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting commonmark<0.10.0,>=0.9.0 (from rich>=12.0.0->pyannote.audio@ git+https://github.com/pyannote/pyannote-audio@11b56a137a578db9335efc00298f6ec1932e6317->whisperx==3.1.0)\n",
            "  Downloading commonmark-0.9.1-py2.py3-none-any.whl (51 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.1/51.1 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11>=0.8 (from uvicorn<2.0->lightning>=2.0.1->pyannote.audio@ git+https://github.com/pyannote/pyannote-audio@11b56a137a578db9335efc00298f6ec1932e6317->whisperx==3.1.0)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting humanfriendly>=9.1 (from coloredlogs->onnxruntime==1.14.*->faster-whisper->whisperx==3.1.0)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ruamel.yaml>=0.17.8 (from hyperpyyaml->speechbrain>=0.5.14->pyannote.audio@ git+https://github.com/pyannote/pyannote-audio@11b56a137a578db9335efc00298f6ec1932e6317->whisperx==3.1.0)\n",
            "  Downloading ruamel.yaml-0.17.26-py3-none-any.whl (109 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.1/109.1 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec->huggingface-hub>=0.13->faster-whisper->whisperx==3.1.0) (23.1.0)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec->huggingface-hub>=0.13->faster-whisper->whisperx==3.1.0)\n",
            "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec->huggingface-hub>=0.13->faster-whisper->whisperx==3.1.0)\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting yarl<2.0,>=1.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec->huggingface-hub>=0.13->faster-whisper->whisperx==3.1.0)\n",
            "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec->huggingface-hub>=0.13->faster-whisper->whisperx==3.1.0)\n",
            "  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec->huggingface-hub>=0.13->faster-whisper->whisperx==3.1.0)\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna>=1.4->pyannote.pipeline>=2.3->pyannote.audio@ git+https://github.com/pyannote/pyannote-audio@11b56a137a578db9335efc00298f6ec1932e6317->whisperx==3.1.0)\n",
            "  Downloading Mako-1.2.4-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.7/78.7 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette->lightning>=2.0.1->pyannote.audio@ git+https://github.com/pyannote/pyannote-audio@11b56a137a578db9335efc00298f6ec1932e6317->whisperx==3.1.0) (1.3.0)\n",
            "Requirement already satisfied: wcwidth>=0.1.4 in /usr/local/lib/python3.10/dist-packages (from blessed>=1.19.0->inquirer<5.0,>=2.10.0->lightning>=2.0.1->pyannote.audio@ git+https://github.com/pyannote/pyannote-audio@11b56a137a578db9335efc00298f6ec1932e6317->whisperx==3.1.0) (0.2.6)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa>=0.6.0->torch_audiomentations>=0.11.0->pyannote.audio@ git+https://github.com/pyannote/pyannote-audio@11b56a137a578db9335efc00298f6ec1932e6317->whisperx==3.1.0) (0.39.1)\n",
            "Requirement already satisfied: appdirs>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from pooch<1.7,>=1.0->librosa>=0.6.0->torch_audiomentations>=0.11.0->pyannote.audio@ git+https://github.com/pyannote/pyannote-audio@11b56a137a578db9335efc00298f6ec1932e6317->whisperx==3.1.0) (1.4.4)\n",
            "Collecting ruamel.yaml.clib>=0.2.7 (from ruamel.yaml>=0.17.8->hyperpyyaml->speechbrain>=0.5.14->pyannote.audio@ git+https://github.com/pyannote/pyannote-audio@11b56a137a578db9335efc00298f6ec1932e6317->whisperx==3.1.0)\n",
            "  Downloading ruamel.yaml.clib-0.2.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (485 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.6/485.6 kB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna>=1.4->pyannote.pipeline>=2.3->pyannote.audio@ git+https://github.com/pyannote/pyannote-audio@11b56a137a578db9335efc00298f6ec1932e6317->whisperx==3.1.0) (2.0.2)\n",
            "Building wheels for collected packages: whisperx, pyannote.audio, antlr4-python3-runtime, docopt, julius\n",
            "  Building wheel for whisperx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for whisperx: filename=whisperx-3.1.0-py3-none-any.whl size=28953 sha256=058611afd626129798eb1675104839ff8b2be744691502571cb20d60e38ccd02\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-_hw7bkxo/wheels/27/fb/53/682b85073a466f1866910d7257233e53b0cc126ab50e7c5373\n",
            "  Building wheel for pyannote.audio (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyannote.audio: filename=pyannote.audio-2.1.1-py2.py3-none-any.whl size=400157 sha256=4d64f9f690ba38b9628edc1bc1d113069e886d481fcb7e1dab99caeb514923d3\n",
            "  Stored in directory: /root/.cache/pip/wheels/1e/f2/4f/ffd6fd3ec96beb935d2f006678342850a3527eca2a9e522df3\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=68d5d72b1565e1bf8f28748f43dc3d26a1d66a5e3c69cc116cf9193abd3dd726\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13707 sha256=e772e87e52a4faeac7c0d08046a723b234a0a8548c7efbcd1994b031b2aa558c\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "  Building wheel for julius (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for julius: filename=julius-0.2.7-py3-none-any.whl size=21878 sha256=a4793124f0f97e0bf7069ec3161304a73409891a61820774911bf577be1cd8d9\n",
            "  Stored in directory: /root/.cache/pip/wheels/b9/b2/05/f883527ffcb7f2ead5438a2c23439aa0c881eaa9a4c80256f4\n",
            "Successfully built whisperx pyannote.audio antlr4-python3-runtime docopt julius\n",
            "Installing collected packages: tokenizers, sentencepiece, python-editor, primePy, docopt, commonmark, av, antlr4-python3-runtime, websockets, tensorboardX, shellingham, setuptools, semver, ruamel.yaml.clib, rich, python-multipart, pyjwt, ordered-set, omegaconf, multidict, Mako, lightning-utilities, humanfriendly, h11, frozenlist, ffmpeg-python, einops, ctranslate2, colorlog, colorama, cmaes, blessed, async-timeout, yarl, uvicorn, starlette, ruamel.yaml, readchar, pyannote.core, huggingface-hub, deepdiff, dateutils, croniter, coloredlogs, arrow, alembic, aiosignal, transformers, starsessions, pyannote.database, optuna, onnxruntime, inquirer, hyperpyyaml, fastapi, aiohttp, pyannote.pipeline, pyannote.metrics, lightning-cloud, faster-whisper, torchmetrics, torch-pitch-shift, pytorch-lightning, julius, torch_audiomentations, speechbrain, pytorch_metric_learning, lightning, asteroid-filterbanks, pyannote.audio, whisperx\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 67.7.2\n",
            "    Uninstalling setuptools-67.7.2:\n",
            "      Successfully uninstalled setuptools-67.7.2\n",
            "  Attempting uninstall: rich\n",
            "    Found existing installation: rich 13.3.4\n",
            "    Uninstalling rich-13.3.4:\n",
            "      Successfully uninstalled rich-13.3.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Mako-1.2.4 aiohttp-3.8.4 aiosignal-1.3.1 alembic-1.10.4 antlr4-python3-runtime-4.9.3 arrow-1.2.3 asteroid-filterbanks-0.4.0 async-timeout-4.0.2 av-10.0.0 blessed-1.20.0 cmaes-0.9.1 colorama-0.4.6 coloredlogs-15.0.1 colorlog-6.7.0 commonmark-0.9.1 croniter-1.3.14 ctranslate2-3.13.0 dateutils-0.6.12 deepdiff-6.3.0 docopt-0.6.2 einops-0.6.1 fastapi-0.88.0 faster-whisper-0.5.1 ffmpeg-python-0.2.0 frozenlist-1.3.3 h11-0.14.0 huggingface-hub-0.14.1 humanfriendly-10.0 hyperpyyaml-1.2.0 inquirer-3.1.3 julius-0.2.7 lightning-2.0.2 lightning-cloud-0.5.34 lightning-utilities-0.8.0 multidict-6.0.4 omegaconf-2.3.0 onnxruntime-1.14.1 optuna-3.1.1 ordered-set-4.1.0 primePy-1.3 pyannote.audio-2.1.1 pyannote.core-5.0.0 pyannote.database-5.0.1 pyannote.metrics-3.2.1 pyannote.pipeline-2.3 pyjwt-2.7.0 python-editor-1.0.4 python-multipart-0.0.6 pytorch-lightning-2.0.2 pytorch_metric_learning-2.1.1 readchar-4.0.5 rich-12.6.0 ruamel.yaml-0.17.26 ruamel.yaml.clib-0.2.7 semver-3.0.0 sentencepiece-0.1.99 setuptools-65.6.3 shellingham-1.5.0.post1 speechbrain-0.5.14 starlette-0.22.0 starsessions-1.3.0 tensorboardX-2.6 tokenizers-0.13.3 torch-pitch-shift-1.2.4 torch_audiomentations-0.11.0 torchmetrics-0.11.4 transformers-4.29.0 uvicorn-0.22.0 websockets-11.0.3 whisperx-3.1.0 yarl-1.9.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "_distutils_hack",
                  "pkg_resources",
                  "pydevd_plugins",
                  "setuptools"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting demucs\n",
            "  Cloning https://github.com/facebookresearch/demucs to /tmp/pip-install-1j4kt2z0/demucs_7c6985a2180844c28606ed9f43342ab3\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/demucs /tmp/pip-install-1j4kt2z0/demucs_7c6985a2180844c28606ed9f43342ab3\n",
            "  Resolved https://github.com/facebookresearch/demucs to commit e25cfeb76546c2bf436661adf18cea8fbecec9ea\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting dora-search (from demucs)\n",
            "  Downloading dora_search-0.1.11.tar.gz (87 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.0/87.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting diffq>=0.2.1 (from demucs)\n",
            "  Downloading diffq-0.2.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (418 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m418.8/418.8 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from demucs) (0.6.1)\n",
            "Requirement already satisfied: julius>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from demucs) (0.2.7)\n",
            "Collecting lameenc>=1.2 (from demucs)\n",
            "  Downloading lameenc-1.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (189 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m189.5/189.5 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting openunmix (from demucs)\n",
            "  Downloading openunmix-1.2.1-py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.7/46.7 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from demucs) (6.0)\n",
            "Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from demucs) (2.0.0+cu118)\n",
            "Requirement already satisfied: torchaudio>=0.8 in /usr/local/lib/python3.10/dist-packages (from demucs) (2.0.1+cu118)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from demucs) (4.65.0)\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.10/dist-packages (from diffq>=0.2.1->demucs) (0.29.34)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from diffq>=0.2.1->demucs) (1.22.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->demucs) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->demucs) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->demucs) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->demucs) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->demucs) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->demucs) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.1->demucs) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.1->demucs) (16.0.3)\n",
            "Requirement already satisfied: omegaconf in /usr/local/lib/python3.10/dist-packages (from dora-search->demucs) (2.3.0)\n",
            "Collecting retrying (from dora-search->demucs)\n",
            "  Downloading retrying-1.3.4-py3-none-any.whl (11 kB)\n",
            "Collecting submitit (from dora-search->demucs)\n",
            "  Downloading submitit-1.4.5-py3-none-any.whl (73 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.1/73.1 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting treetable (from dora-search->demucs)\n",
            "  Downloading treetable-0.2.5.tar.gz (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.1->demucs) (2.1.2)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.10/dist-packages (from omegaconf->dora-search->demucs) (4.9.3)\n",
            "Requirement already satisfied: six>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from retrying->dora-search->demucs) (1.16.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.1 in /usr/local/lib/python3.10/dist-packages (from submitit->dora-search->demucs) (2.2.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.1->demucs) (1.3.0)\n",
            "Building wheels for collected packages: demucs, dora-search, treetable\n",
            "  Building wheel for demucs (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for demucs: filename=demucs-4.0.0-py3-none-any.whl size=77098 sha256=711440461b2c9f3de1a2d0e10f875e96136790455f8b3a19b67adc0d3d8e10e0\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-kp9cl01k/wheels/45/e4/ca/7791f04b554e5433713e22900eaf11595e27c454fb65ac30ab\n",
            "  Building wheel for dora-search (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dora-search: filename=dora_search-0.1.11-py3-none-any.whl size=74991 sha256=bc30771854378c8196de9764ce70c72f405eeb03d5dfadf9e0d3005f27438996\n",
            "  Stored in directory: /root/.cache/pip/wheels/a3/e6/9c/47aaef1ea88bd63244dea69547ceff3e6eedf03a122d39d727\n",
            "  Building wheel for treetable (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for treetable: filename=treetable-0.2.5-py3-none-any.whl size=7332 sha256=4f9d295638d974a560a899f4802ecf450bb071ae6d31dcfa94f92b7399388900\n",
            "  Stored in directory: /root/.cache/pip/wheels/72/55/0e/91c3655bdb162446f8a7cd477579397544454a63ae7c599c0c\n",
            "Successfully built demucs dora-search treetable\n",
            "Installing collected packages: lameenc, treetable, submitit, retrying, openunmix, dora-search, diffq, demucs\n",
            "Successfully installed demucs-4.0.0 diffq-0.2.4 dora-search-0.1.11 lameenc-1.4.2 openunmix-1.2.1 retrying-1.3.4 submitit-1.4.5 treetable-0.2.5\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting deepmultilingualpunctuation\n",
            "  Downloading deepmultilingualpunctuation-1.0.1-py3-none-any.whl (5.4 kB)\n",
            "Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from deepmultilingualpunctuation) (2.0.0+cu118)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from deepmultilingualpunctuation) (4.29.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->deepmultilingualpunctuation) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->deepmultilingualpunctuation) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->deepmultilingualpunctuation) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->deepmultilingualpunctuation) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->deepmultilingualpunctuation) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->deepmultilingualpunctuation) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.1->deepmultilingualpunctuation) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.1->deepmultilingualpunctuation) (16.0.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from transformers->deepmultilingualpunctuation) (0.14.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers->deepmultilingualpunctuation) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers->deepmultilingualpunctuation) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers->deepmultilingualpunctuation) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->deepmultilingualpunctuation) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers->deepmultilingualpunctuation) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers->deepmultilingualpunctuation) (0.13.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers->deepmultilingualpunctuation) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers->deepmultilingualpunctuation) (2023.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.1->deepmultilingualpunctuation) (2.1.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->deepmultilingualpunctuation) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->deepmultilingualpunctuation) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->deepmultilingualpunctuation) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->deepmultilingualpunctuation) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.1->deepmultilingualpunctuation) (1.3.0)\n",
            "Installing collected packages: deepmultilingualpunctuation\n",
            "Successfully installed deepmultilingualpunctuation-1.0.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting nemo_toolkit[asr]==1.17.0\n",
            "  Downloading nemo_toolkit-1.17.0-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[asr]==1.17.0) (0.14.1)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[asr]==1.17.0) (0.56.4)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[asr]==1.17.0) (1.22.4)\n",
            "Collecting onnx>=1.7.0 (from nemo_toolkit[asr]==1.17.0)\n",
            "  Downloading onnx-1.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m66.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[asr]==1.17.0) (2.8.2)\n",
            "Requirement already satisfied: ruamel.yaml in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[asr]==1.17.0) (0.17.26)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[asr]==1.17.0) (1.2.2)\n",
            "Collecting setuptools==65.5.1 (from nemo_toolkit[asr]==1.17.0)\n",
            "  Downloading setuptools-65.5.1-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m62.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[asr]==1.17.0) (2.12.2)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[asr]==1.17.0) (1.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[asr]==1.17.0) (2.0.0+cu118)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[asr]==1.17.0) (4.65.0)\n",
            "Requirement already satisfied: wget in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[asr]==1.17.0) (3.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[asr]==1.17.0) (1.14.1)\n",
            "Collecting braceexpand (from nemo_toolkit[asr]==1.17.0)\n",
            "  Downloading braceexpand-0.1.7-py2.py3-none-any.whl (5.9 kB)\n",
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[asr]==1.17.0) (0.6.2)\n",
            "Collecting g2p-en (from nemo_toolkit[asr]==1.17.0)\n",
            "  Downloading g2p_en-2.1.0-py3-none-any.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m66.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: inflect in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[asr]==1.17.0) (6.0.4)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[asr]==1.17.0) (7.7.1)\n",
            "Collecting jiwer (from nemo_toolkit[asr]==1.17.0)\n",
            "  Downloading jiwer-3.0.1-py3-none-any.whl (21 kB)\n",
            "Collecting kaldi-python-io (from nemo_toolkit[asr]==1.17.0)\n",
            "  Downloading kaldi-python-io-1.2.2.tar.gz (8.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting kaldiio (from nemo_toolkit[asr]==1.17.0)\n",
            "  Downloading kaldiio-2.18.0-py3-none-any.whl (28 kB)\n",
            "Requirement already satisfied: librosa>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[asr]==1.17.0) (0.10.0.post2)\n",
            "Collecting marshmallow (from nemo_toolkit[asr]==1.17.0)\n",
            "  Downloading marshmallow-3.19.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[asr]==1.17.0) (3.7.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[asr]==1.17.0) (23.1)\n",
            "Requirement already satisfied: pyannote.core in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[asr]==1.17.0) (5.0.0)\n",
            "Requirement already satisfied: pyannote.metrics in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[asr]==1.17.0) (3.2.1)\n",
            "Collecting pydub (from nemo_toolkit[asr]==1.17.0)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[asr]==1.17.0) (1.10.1)\n",
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[asr]==1.17.0) (0.12.1)\n",
            "Collecting sox (from nemo_toolkit[asr]==1.17.0)\n",
            "  Downloading sox-1.4.1-py2.py3-none-any.whl (39 kB)\n",
            "Collecting texterrors (from nemo_toolkit[asr]==1.17.0)\n",
            "  Downloading texterrors-0.4.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m86.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting hydra-core<1.3,>=1.2.0 (from nemo_toolkit[asr]==1.17.0)\n",
            "  Downloading hydra_core-1.2.0-py3-none-any.whl (151 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.1/151.1 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting omegaconf<2.3,>=2.2 (from nemo_toolkit[asr]==1.17.0)\n",
            "  Downloading omegaconf-2.2.3-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.3/79.3 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pytorch-lightning<=1.9.4,>=1.9.0 (from nemo_toolkit[asr]==1.17.0)\n",
            "  Downloading pytorch_lightning-1.9.4-py3-none-any.whl (827 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m827.8/827.8 kB\u001b[0m \u001b[31m61.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyyaml<6 (from nemo_toolkit[asr]==1.17.0)\n",
            "  Downloading PyYAML-5.4.1.tar.gz (175 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.1/175.1 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torchmetrics>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[asr]==1.17.0) (0.11.4)\n",
            "Requirement already satisfied: transformers>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[asr]==1.17.0) (4.29.0)\n",
            "Collecting wandb (from nemo_toolkit[asr]==1.17.0)\n",
            "  Downloading wandb-0.15.2-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m81.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting webdataset<=0.1.62,>=0.1.48 (from nemo_toolkit[asr]==1.17.0)\n",
            "  Downloading webdataset-0.1.62-py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[asr]==1.17.0) (1.5.3)\n",
            "Collecting sacremoses>=0.0.43 (from nemo_toolkit[asr]==1.17.0)\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m880.6/880.6 kB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: sentencepiece<1.0.0 in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[asr]==1.17.0) (0.1.99)\n",
            "Collecting youtokentome>=1.0.5 (from nemo_toolkit[asr]==1.17.0)\n",
            "  Downloading youtokentome-1.0.6.tar.gz (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.7/86.7 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.10/dist-packages (from hydra-core<1.3,>=1.2.0->nemo_toolkit[asr]==1.17.0) (4.9.3)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.9.0->nemo_toolkit[asr]==1.17.0) (3.0.0)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.9.0->nemo_toolkit[asr]==1.17.0) (1.2.0)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.9.0->nemo_toolkit[asr]==1.17.0) (4.4.2)\n",
            "Requirement already satisfied: pooch<1.7,>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.9.0->nemo_toolkit[asr]==1.17.0) (1.6.0)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.9.0->nemo_toolkit[asr]==1.17.0) (0.3.5)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.9.0->nemo_toolkit[asr]==1.17.0) (4.5.0)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.9.0->nemo_toolkit[asr]==1.17.0) (0.2)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.9.0->nemo_toolkit[asr]==1.17.0) (1.0.5)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->nemo_toolkit[asr]==1.17.0) (0.39.1)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx>=1.7.0->nemo_toolkit[asr]==1.17.0) (3.20.3)\n",
            "Requirement already satisfied: fsspec[http]>2021.06.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning<=1.9.4,>=1.9.0->nemo_toolkit[asr]==1.17.0) (2023.4.0)\n",
            "Requirement already satisfied: lightning-utilities>=0.6.0.post0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning<=1.9.4,>=1.9.0->nemo_toolkit[asr]==1.17.0) (0.8.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacremoses>=0.0.43->nemo_toolkit[asr]==1.17.0) (2022.10.31)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from sacremoses>=0.0.43->nemo_toolkit[asr]==1.17.0) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses>=0.0.43->nemo_toolkit[asr]==1.17.0) (8.1.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->nemo_toolkit[asr]==1.17.0) (3.1.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile->nemo_toolkit[asr]==1.17.0) (1.15.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->nemo_toolkit[asr]==1.17.0) (3.12.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->nemo_toolkit[asr]==1.17.0) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->nemo_toolkit[asr]==1.17.0) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->nemo_toolkit[asr]==1.17.0) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->nemo_toolkit[asr]==1.17.0) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->nemo_toolkit[asr]==1.17.0) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->nemo_toolkit[asr]==1.17.0) (16.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.0.1->nemo_toolkit[asr]==1.17.0) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.0.1->nemo_toolkit[asr]==1.17.0) (0.13.3)\n",
            "Requirement already satisfied: nltk>=3.2.4 in /usr/local/lib/python3.10/dist-packages (from g2p-en->nemo_toolkit[asr]==1.17.0) (3.8.1)\n",
            "Collecting distance>=0.1.3 (from g2p-en->nemo_toolkit[asr]==1.17.0)\n",
            "  Downloading Distance-0.1.3.tar.gz (180 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.3/180.3 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pydantic>=1.9.1 in /usr/local/lib/python3.10/dist-packages (from inflect->nemo_toolkit[asr]==1.17.0) (1.10.7)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->nemo_toolkit[asr]==1.17.0) (5.5.6)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->nemo_toolkit[asr]==1.17.0) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->nemo_toolkit[asr]==1.17.0) (5.7.1)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->nemo_toolkit[asr]==1.17.0) (3.6.4)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->nemo_toolkit[asr]==1.17.0) (7.34.0)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->nemo_toolkit[asr]==1.17.0) (3.0.7)\n",
            "Collecting rapidfuzz==2.13.7 (from jiwer->nemo_toolkit[asr]==1.17.0)\n",
            "  Downloading rapidfuzz-2.13.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->nemo_toolkit[asr]==1.17.0) (1.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->nemo_toolkit[asr]==1.17.0) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->nemo_toolkit[asr]==1.17.0) (4.39.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->nemo_toolkit[asr]==1.17.0) (1.4.4)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->nemo_toolkit[asr]==1.17.0) (8.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->nemo_toolkit[asr]==1.17.0) (3.0.9)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->nemo_toolkit[asr]==1.17.0) (2022.7.1)\n",
            "Requirement already satisfied: sortedcontainers>=2.0.4 in /usr/local/lib/python3.10/dist-packages (from pyannote.core->nemo_toolkit[asr]==1.17.0) (2.4.0)\n",
            "Requirement already satisfied: pyannote.database>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from pyannote.metrics->nemo_toolkit[asr]==1.17.0) (5.0.1)\n",
            "Requirement already satisfied: docopt>=0.6.2 in /usr/local/lib/python3.10/dist-packages (from pyannote.metrics->nemo_toolkit[asr]==1.17.0) (0.6.2)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from pyannote.metrics->nemo_toolkit[asr]==1.17.0) (0.8.10)\n",
            "Requirement already satisfied: ruamel.yaml.clib>=0.2.7 in /usr/local/lib/python3.10/dist-packages (from ruamel.yaml->nemo_toolkit[asr]==1.17.0) (0.2.7)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard->nemo_toolkit[asr]==1.17.0) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard->nemo_toolkit[asr]==1.17.0) (1.54.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard->nemo_toolkit[asr]==1.17.0) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard->nemo_toolkit[asr]==1.17.0) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard->nemo_toolkit[asr]==1.17.0) (3.4.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->nemo_toolkit[asr]==1.17.0) (0.7.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->nemo_toolkit[asr]==1.17.0) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard->nemo_toolkit[asr]==1.17.0) (2.3.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.10/dist-packages (from tensorboard->nemo_toolkit[asr]==1.17.0) (0.40.0)\n",
            "Collecting pybind11 (from texterrors->nemo_toolkit[asr]==1.17.0)\n",
            "  Downloading pybind11-2.10.4-py3-none-any.whl (222 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m222.3/222.3 kB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting plac (from texterrors->nemo_toolkit[asr]==1.17.0)\n",
            "  Downloading plac-1.3.5-py2.py3-none-any.whl (22 kB)\n",
            "Collecting loguru (from texterrors->nemo_toolkit[asr]==1.17.0)\n",
            "  Downloading loguru-0.7.0-py3-none-any.whl (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from texterrors->nemo_toolkit[asr]==1.17.0) (2.3.0)\n",
            "Collecting Levenshtein (from texterrors->nemo_toolkit[asr]==1.17.0)\n",
            "  Downloading Levenshtein-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (174 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m174.1/174.1 kB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting GitPython!=3.1.29,>=1.0.0 (from wandb->nemo_toolkit[asr]==1.17.0)\n",
            "  Downloading GitPython-3.1.31-py3-none-any.whl (184 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb->nemo_toolkit[asr]==1.17.0) (5.9.5)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb->nemo_toolkit[asr]==1.17.0)\n",
            "  Downloading sentry_sdk-1.22.2-py2.py3-none-any.whl (203 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.3/203.3 kB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb->nemo_toolkit[asr]==1.17.0)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting pathtools (from wandb->nemo_toolkit[asr]==1.17.0)\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting setproctitle (from wandb->nemo_toolkit[asr]==1.17.0)\n",
            "  Downloading setproctitle-1.3.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb->nemo_toolkit[asr]==1.17.0) (1.4.4)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile->nemo_toolkit[asr]==1.17.0) (2.21)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning<=1.9.4,>=1.9.0->nemo_toolkit[asr]==1.17.0) (3.8.4)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb->nemo_toolkit[asr]==1.17.0)\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->nemo_toolkit[asr]==1.17.0) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->nemo_toolkit[asr]==1.17.0) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->nemo_toolkit[asr]==1.17.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard->nemo_toolkit[asr]==1.17.0) (1.3.1)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets->nemo_toolkit[asr]==1.17.0) (6.1.12)\n",
            "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets->nemo_toolkit[asr]==1.17.0) (6.2)\n",
            "Collecting jedi>=0.16 (from ipython>=4.0.0->ipywidgets->nemo_toolkit[asr]==1.17.0)\n",
            "  Downloading jedi-0.18.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m90.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets->nemo_toolkit[asr]==1.17.0) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets->nemo_toolkit[asr]==1.17.0) (3.0.38)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets->nemo_toolkit[asr]==1.17.0) (2.14.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets->nemo_toolkit[asr]==1.17.0) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets->nemo_toolkit[asr]==1.17.0) (0.1.6)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets->nemo_toolkit[asr]==1.17.0) (4.8.0)\n",
            "Requirement already satisfied: typer[all]>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from pyannote.database>=4.0.1->pyannote.metrics->nemo_toolkit[asr]==1.17.0) (0.7.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.0.1->nemo_toolkit[asr]==1.17.0) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.0.1->nemo_toolkit[asr]==1.17.0) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.0.1->nemo_toolkit[asr]==1.17.0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.0.1->nemo_toolkit[asr]==1.17.0) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->nemo_toolkit[asr]==1.17.0) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard->nemo_toolkit[asr]==1.17.0) (2.1.2)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.10/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[asr]==1.17.0) (6.4.8)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<=1.9.4,>=1.9.0->nemo_toolkit[asr]==1.17.0) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<=1.9.4,>=1.9.0->nemo_toolkit[asr]==1.17.0) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<=1.9.4,>=1.9.0->nemo_toolkit[asr]==1.17.0) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<=1.9.4,>=1.9.0->nemo_toolkit[asr]==1.17.0) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<=1.9.4,>=1.9.0->nemo_toolkit[asr]==1.17.0) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<=1.9.4,>=1.9.0->nemo_toolkit[asr]==1.17.0) (1.3.1)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb->nemo_toolkit[asr]==1.17.0)\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets->nemo_toolkit[asr]==1.17.0) (0.8.3)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[asr]==1.17.0) (23.2.1)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[asr]==1.17.0) (21.3.0)\n",
            "Requirement already satisfied: jupyter-core>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[asr]==1.17.0) (5.3.0)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[asr]==1.17.0) (5.8.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[asr]==1.17.0) (6.5.4)\n",
            "Requirement already satisfied: nest-asyncio>=1.5 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[asr]==1.17.0) (1.5.6)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[asr]==1.17.0) (1.8.0)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[asr]==1.17.0) (0.17.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[asr]==1.17.0) (0.16.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=4.0.0->ipywidgets->nemo_toolkit[asr]==1.17.0) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets->nemo_toolkit[asr]==1.17.0) (0.2.6)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->nemo_toolkit[asr]==1.17.0) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard->nemo_toolkit[asr]==1.17.0) (3.2.2)\n",
            "Requirement already satisfied: colorama<0.5.0,>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from typer[all]>=0.2.1->pyannote.database>=4.0.1->pyannote.metrics->nemo_toolkit[asr]==1.17.0) (0.4.6)\n",
            "Requirement already satisfied: shellingham<2.0.0,>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer[all]>=0.2.1->pyannote.database>=4.0.1->pyannote.metrics->nemo_toolkit[asr]==1.17.0) (1.5.0.post1)\n",
            "Requirement already satisfied: rich<13.0.0,>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer[all]>=0.2.1->pyannote.database>=4.0.1->pyannote.metrics->nemo_toolkit[asr]==1.17.0) (12.6.0)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core>=4.6.1->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[asr]==1.17.0) (3.3.0)\n",
            "Requirement already satisfied: commonmark<0.10.0,>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from rich<13.0.0,>=10.11.0->typer[all]>=0.2.1->pyannote.database>=4.0.1->pyannote.metrics->nemo_toolkit[asr]==1.17.0) (0.9.1)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[asr]==1.17.0) (21.2.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[asr]==1.17.0) (4.9.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[asr]==1.17.0) (4.11.2)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[asr]==1.17.0) (6.0.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[asr]==1.17.0) (0.7.1)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[asr]==1.17.0) (0.4)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[asr]==1.17.0) (0.2.2)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[asr]==1.17.0) (0.8.4)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[asr]==1.17.0) (0.7.4)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[asr]==1.17.0) (1.5.0)\n",
            "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[asr]==1.17.0) (1.2.1)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.10/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[asr]==1.17.0) (2.16.3)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[asr]==1.17.0) (4.3.3)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[asr]==1.17.0) (0.19.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[asr]==1.17.0) (2.4.1)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit[asr]==1.17.0) (0.5.1)\n",
            "Building wheels for collected packages: pyyaml, sacremoses, youtokentome, kaldi-python-io, distance, pathtools\n",
            "  Building wheel for pyyaml (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyyaml: filename=PyYAML-5.4.1-cp310-cp310-linux_x86_64.whl size=45658 sha256=06579266ed7d99dc863db289a1b3125c068e17dbd69fc7c006896cbb9c8d9077\n",
            "  Stored in directory: /root/.cache/pip/wheels/c7/0d/22/696ee92245ad710f506eee79bb05c740d8abccd3ecdb778683\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895241 sha256=d0956b746e0ac106bb7212f3e9424d287563cc4038b52fc0981dd1bdbb2a22bd\n",
            "  Stored in directory: /root/.cache/pip/wheels/00/24/97/a2ea5324f36bc626e1ea0267f33db6aa80d157ee977e9e42fb\n",
            "\n",
            "  Building wheel for youtokentome (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for youtokentome: filename=youtokentome-1.0.6-cp310-cp310-linux_x86_64.whl size=1927582 sha256=ba8937f0efb9e8c86984fb08760a8da678bba8e7b2ce1ccf2cbcb9a82b95b120\n",
            "  Stored in directory: /root/.cache/pip/wheels/df/85/f8/301d2ba45f43f30bed2fe413efa760bc726b8b660ed9c2900c\n",
            "  Building wheel for kaldi-python-io (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kaldi-python-io: filename=kaldi_python_io-1.2.2-py3-none-any.whl size=8951 sha256=4c4c4da03d2bab0131154c1ec89eb1b22eabea1cd491a73e2b37fd4f86197324\n",
            "  Stored in directory: /root/.cache/pip/wheels/b7/23/5f/49d3a826be576faf61d84e8028e1914bb36a5586ee2613b087\n",
            "  Building wheel for distance (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for distance: filename=Distance-0.1.3-py3-none-any.whl size=16258 sha256=e80b431600ee15a0f87ad3dc7cbd5598dd5e0a59bcb17c76c78be220830e05a9\n",
            "  Stored in directory: /root/.cache/pip/wheels/e8/bb/de/f71bf63559ea9a921059a5405806f7ff6ed612a9231c4a9309\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8791 sha256=fa70d260cca4840950602066a448a9eb0436fb747e9ef30e352a8e1c936b88b1\n",
            "  Stored in directory: /root/.cache/pip/wheels/e7/f3/22/152153d6eb222ee7a56ff8617d80ee5207207a8c00a7aab794\n",
            "Successfully built pyyaml sacremoses youtokentome kaldi-python-io distance pathtools\n",
            "Installing collected packages: pydub, plac, pathtools, distance, braceexpand, youtokentome, webdataset, sox, smmap, setuptools, setproctitle, sentry-sdk, sacremoses, rapidfuzz, pyyaml, pybind11, onnx, marshmallow, loguru, kaldiio, kaldi-python-io, jedi, docker-pycreds, omegaconf, Levenshtein, jiwer, gitdb, texterrors, hydra-core, GitPython, g2p-en, wandb, pytorch-lightning, nemo_toolkit\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 65.6.3\n",
            "    Uninstalling setuptools-65.6.3:\n",
            "      Successfully uninstalled setuptools-65.6.3\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 6.0\n",
            "    Uninstalling PyYAML-6.0:\n",
            "      Successfully uninstalled PyYAML-6.0\n",
            "  Attempting uninstall: omegaconf\n",
            "    Found existing installation: omegaconf 2.3.0\n",
            "    Uninstalling omegaconf-2.3.0:\n",
            "      Successfully uninstalled omegaconf-2.3.0\n",
            "  Attempting uninstall: pytorch-lightning\n",
            "    Found existing installation: pytorch-lightning 2.0.2\n",
            "    Uninstalling pytorch-lightning-2.0.2:\n",
            "      Successfully uninstalled pytorch-lightning-2.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cvxpy 1.3.1 requires setuptools>65.5.1, but you have setuptools 65.5.1 which is incompatible.\n",
            "whisperx 3.1.0 requires setuptools==65.6.3, but you have setuptools 65.5.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed GitPython-3.1.31 Levenshtein-0.21.0 braceexpand-0.1.7 distance-0.1.3 docker-pycreds-0.4.0 g2p-en-2.1.0 gitdb-4.0.10 hydra-core-1.2.0 jedi-0.18.2 jiwer-3.0.1 kaldi-python-io-1.2.2 kaldiio-2.18.0 loguru-0.7.0 marshmallow-3.19.0 nemo_toolkit-1.17.0 omegaconf-2.2.3 onnx-1.14.0 pathtools-0.1.2 plac-1.3.5 pybind11-2.10.4 pydub-0.25.1 pytorch-lightning-1.9.4 pyyaml-5.4.1 rapidfuzz-2.13.7 sacremoses-0.0.53 sentry-sdk-1.22.2 setproctitle-1.3.2 setuptools-65.5.1 smmap-5.0.0 sox-1.4.1 texterrors-0.4.4 wandb-0.15.2 webdataset-0.1.62 youtokentome-1.0.6\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "_distutils_hack",
                  "pkg_resources",
                  "pydevd_plugins",
                  "setuptools"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.29.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (5.4.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install wget\n",
        "!pip install git+https://github.com/m-bain/whisperx.git\n",
        "!pip install git+https://github.com/facebookresearch/demucs#egg=demucs\n",
        "!pip install deepmultilingualpunctuation\n",
        "!pip install nemo_toolkit[asr]==1.17.0\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pPbwpVx2kSEw"
      },
      "outputs": [],
      "source": [
        "#!pip install faster-whisper==0.5.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R29hmbsBL1E8"
      },
      "outputs": [],
      "source": [
        "import locale\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "def utf():\n",
        "  locale.getpreferredencoding = getpreferredencoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X_HGDOP0kBJI",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title helpers\n",
        "import os\n",
        "import wget\n",
        "from omegaconf import OmegaConf\n",
        "import json\n",
        "import shutil\n",
        "\n",
        "punct_model_langs = [\n",
        "    \"en\",\n",
        "    \"fr\",\n",
        "    \"de\",\n",
        "    \"es\",\n",
        "    \"it\",\n",
        "    \"nl\",\n",
        "    \"pt\",\n",
        "    \"bg\",\n",
        "    \"pl\",\n",
        "    \"cs\",\n",
        "    \"sk\",\n",
        "    \"sl\",\n",
        "]\n",
        "wav2vec2_langs = [\n",
        "    \"en\",\n",
        "    \"fr\",\n",
        "    \"de\",\n",
        "    \"es\",\n",
        "    \"it\",\n",
        "    \"nl\",\n",
        "    \"pt\",\n",
        "    \"ja\",\n",
        "    \"zh\",\n",
        "    \"uk\",\n",
        "    \"pt\",\n",
        "    \"ar\",\n",
        "    \"ru\",\n",
        "    \"pl\",\n",
        "    \"hu\",\n",
        "    \"fi\",\n",
        "    \"fa\",\n",
        "    \"el\",\n",
        "    \"tr\",\n",
        "]\n",
        "\n",
        "\n",
        "def create_config():\n",
        "    data_dir = \"./\"\n",
        "    DOMAIN_TYPE = \"telephonic\"  # Can be meeting or telephonic based on domain type of the audio file\n",
        "    CONFIG_FILE_NAME = f\"diar_infer_{DOMAIN_TYPE}.yaml\"\n",
        "    CONFIG_URL = f\"https://raw.githubusercontent.com/NVIDIA/NeMo/main/examples/speaker_tasks/diarization/conf/inference/{CONFIG_FILE_NAME}\"\n",
        "    MODEL_CONFIG = os.path.join(data_dir, CONFIG_FILE_NAME)\n",
        "    if not os.path.exists(MODEL_CONFIG):\n",
        "        MODEL_CONFIG = wget.download(CONFIG_URL, data_dir)\n",
        "\n",
        "    config = OmegaConf.load(MODEL_CONFIG)\n",
        "\n",
        "    ROOT = os.getcwd()\n",
        "    data_dir = os.path.join(ROOT, \"data\")\n",
        "    os.makedirs(data_dir, exist_ok=True)\n",
        "\n",
        "    meta = {\n",
        "        \"audio_filepath\": \"mono_file.wav\",\n",
        "        \"offset\": 0,\n",
        "        \"duration\": None,\n",
        "        \"label\": \"infer\",\n",
        "        \"text\": \"-\",\n",
        "        \"rttm_filepath\": None,\n",
        "        \"uem_filepath\": None,\n",
        "    }\n",
        "    with open(\"data/input_manifest.json\", \"w\") as fp:\n",
        "        json.dump(meta, fp)\n",
        "        fp.write(\"\\n\")\n",
        "\n",
        "    pretrained_vad = \"vad_multilingual_marblenet\"\n",
        "    pretrained_speaker_model = \"titanet_large\"\n",
        "\n",
        "    config.num_workers = 1  # Workaround for multiprocessing hanging with ipython issue\n",
        "\n",
        "    output_dir = \"nemo_outputs\"  # os.path.join(ROOT, 'outputs')\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    config.diarizer.manifest_filepath = \"data/input_manifest.json\"\n",
        "    config.diarizer.out_dir = (\n",
        "        output_dir  # Directory to store intermediate files and prediction outputs\n",
        "    )\n",
        "\n",
        "    config.diarizer.speaker_embeddings.model_path = pretrained_speaker_model\n",
        "    config.diarizer.oracle_vad = (\n",
        "        False  # compute VAD provided with model_path to vad config\n",
        "    )\n",
        "    config.diarizer.clustering.parameters.oracle_num_speakers = False\n",
        "\n",
        "    # Here, we use our in-house pretrained NeMo VAD model\n",
        "    config.diarizer.vad.model_path = pretrained_vad\n",
        "    config.diarizer.vad.parameters.onset = 0.8\n",
        "    config.diarizer.vad.parameters.offset = 0.6\n",
        "    config.diarizer.vad.parameters.pad_offset = -0.05\n",
        "    config.diarizer.msdd_model.model_path = (\n",
        "        \"diar_msdd_telephonic\"  # Telephonic speaker diarization model\n",
        "    )\n",
        "\n",
        "    return config\n",
        "\n",
        "\n",
        "def get_word_ts_anchor(s, e, option=\"start\"):\n",
        "    if option == \"end\":\n",
        "        return e\n",
        "    elif option == \"mid\":\n",
        "        return (s + e) / 2\n",
        "    return s\n",
        "\n",
        "\n",
        "def get_words_speaker_mapping(wrd_ts, spk_ts, word_anchor_option=\"start\"):\n",
        "    s, e, sp = spk_ts[0]\n",
        "    wrd_pos, turn_idx = 0, 0\n",
        "    wrd_spk_mapping = []\n",
        "    for wrd_dict in wrd_ts:\n",
        "        ws, we, wrd = (\n",
        "            int(wrd_dict[\"start\"] * 1000),\n",
        "            int(wrd_dict[\"end\"] * 1000),\n",
        "            wrd_dict[\"word\"],\n",
        "        )\n",
        "        wrd_pos = get_word_ts_anchor(ws, we, word_anchor_option)\n",
        "        while wrd_pos > float(e):\n",
        "            turn_idx += 1\n",
        "            turn_idx = min(turn_idx, len(spk_ts) - 1)\n",
        "            s, e, sp = spk_ts[turn_idx]\n",
        "            if turn_idx == len(spk_ts) - 1:\n",
        "                e = get_word_ts_anchor(ws, we, option=\"end\")\n",
        "        wrd_spk_mapping.append(\n",
        "            {\"word\": wrd, \"start_time\": ws, \"end_time\": we, \"speaker\": sp}\n",
        "        )\n",
        "    return wrd_spk_mapping\n",
        "\n",
        "\n",
        "sentence_ending_punctuations = \".?!\"\n",
        "\n",
        "\n",
        "def get_first_word_idx_of_sentence(word_idx, word_list, speaker_list, max_words):\n",
        "    is_word_sentence_end = (\n",
        "        lambda x: x >= 0 and word_list[x][-1] in sentence_ending_punctuations\n",
        "    )\n",
        "    left_idx = word_idx\n",
        "    while (\n",
        "        left_idx > 0\n",
        "        and word_idx - left_idx < max_words\n",
        "        and speaker_list[left_idx - 1] == speaker_list[left_idx]\n",
        "        and not is_word_sentence_end(left_idx - 1)\n",
        "    ):\n",
        "        left_idx -= 1\n",
        "\n",
        "    return left_idx if left_idx == 0 or is_word_sentence_end(left_idx - 1) else -1\n",
        "\n",
        "\n",
        "def get_last_word_idx_of_sentence(word_idx, word_list, max_words):\n",
        "    is_word_sentence_end = (\n",
        "        lambda x: x >= 0 and word_list[x][-1] in sentence_ending_punctuations\n",
        "    )\n",
        "    right_idx = word_idx\n",
        "    while (\n",
        "        right_idx < len(word_list)\n",
        "        and right_idx - word_idx < max_words\n",
        "        and not is_word_sentence_end(right_idx)\n",
        "    ):\n",
        "        right_idx += 1\n",
        "\n",
        "    return (\n",
        "        right_idx\n",
        "        if right_idx == len(word_list) - 1 or is_word_sentence_end(right_idx)\n",
        "        else -1\n",
        "    )\n",
        "\n",
        "\n",
        "def get_realigned_ws_mapping_with_punctuation(\n",
        "    word_speaker_mapping, max_words_in_sentence=50\n",
        "):\n",
        "    is_word_sentence_end = (\n",
        "        lambda x: x >= 0\n",
        "        and word_speaker_mapping[x][\"word\"][-1] in sentence_ending_punctuations\n",
        "    )\n",
        "    wsp_len = len(word_speaker_mapping)\n",
        "\n",
        "    words_list, speaker_list = [], []\n",
        "    for k, line_dict in enumerate(word_speaker_mapping):\n",
        "        word, speaker = line_dict[\"word\"], line_dict[\"speaker\"]\n",
        "        words_list.append(word)\n",
        "        speaker_list.append(speaker)\n",
        "\n",
        "    k = 0\n",
        "    while k < len(word_speaker_mapping):\n",
        "        line_dict = word_speaker_mapping[k]\n",
        "        if (\n",
        "            k < wsp_len - 1\n",
        "            and speaker_list[k] != speaker_list[k + 1]\n",
        "            and not is_word_sentence_end(k)\n",
        "        ):\n",
        "            left_idx = get_first_word_idx_of_sentence(\n",
        "                k, words_list, speaker_list, max_words_in_sentence\n",
        "            )\n",
        "            right_idx = (\n",
        "                get_last_word_idx_of_sentence(\n",
        "                    k, words_list, max_words_in_sentence - k + left_idx - 1\n",
        "                )\n",
        "                if left_idx > -1\n",
        "                else -1\n",
        "            )\n",
        "            if min(left_idx, right_idx) == -1:\n",
        "                k += 1\n",
        "                continue\n",
        "\n",
        "            spk_labels = speaker_list[left_idx : right_idx + 1]\n",
        "            mod_speaker = max(set(spk_labels), key=spk_labels.count)\n",
        "            if spk_labels.count(mod_speaker) < len(spk_labels) // 2:\n",
        "                k += 1\n",
        "                continue\n",
        "\n",
        "            speaker_list[left_idx : right_idx + 1] = [mod_speaker] * (\n",
        "                right_idx - left_idx + 1\n",
        "            )\n",
        "            k = right_idx\n",
        "\n",
        "        k += 1\n",
        "\n",
        "    k, realigned_list = 0, []\n",
        "    while k < len(word_speaker_mapping):\n",
        "        line_dict = word_speaker_mapping[k].copy()\n",
        "        line_dict[\"speaker\"] = speaker_list[k]\n",
        "        realigned_list.append(line_dict)\n",
        "        k += 1\n",
        "\n",
        "    return realigned_list\n",
        "\n",
        "\n",
        "def get_sentences_speaker_mapping(word_speaker_mapping, spk_ts):\n",
        "    s, e, spk = spk_ts[0]\n",
        "    prev_spk = spk\n",
        "\n",
        "    snts = []\n",
        "    snt = {\"speaker\": f\"Speaker {spk}\", \"start_time\": s, \"end_time\": e, \"text\": \"\"}\n",
        "\n",
        "    for wrd_dict in word_speaker_mapping:\n",
        "        wrd, spk = wrd_dict[\"word\"], wrd_dict[\"speaker\"]\n",
        "        s, e = wrd_dict[\"start_time\"], wrd_dict[\"end_time\"]\n",
        "        if spk != prev_spk:\n",
        "            snts.append(snt)\n",
        "            snt = {\n",
        "                \"speaker\": f\"Speaker {spk}\",\n",
        "                \"start_time\": s,\n",
        "                \"end_time\": e,\n",
        "                \"text\": \"\",\n",
        "            }\n",
        "        else:\n",
        "            snt[\"end_time\"] = e\n",
        "        snt[\"text\"] += wrd + \" \"\n",
        "        prev_spk = spk\n",
        "\n",
        "    snts.append(snt)\n",
        "    return snts\n",
        "\n",
        "\n",
        "def get_speaker_aware_transcript(sentences_speaker_mapping, f):\n",
        "    for sentence_dict in sentences_speaker_mapping:\n",
        "        sp = sentence_dict[\"speaker\"]\n",
        "        text = sentence_dict[\"text\"]\n",
        "        f.write(f\"\\n\\n{sp}: {text}\")\n",
        "\n",
        "\n",
        "def format_timestamp(\n",
        "    milliseconds: float, always_include_hours: bool = False, decimal_marker: str = \".\"\n",
        "):\n",
        "    assert milliseconds >= 0, \"non-negative timestamp expected\"\n",
        "\n",
        "    hours = milliseconds // 3_600_000\n",
        "    milliseconds -= hours * 3_600_000\n",
        "\n",
        "    minutes = milliseconds // 60_000\n",
        "    milliseconds -= minutes * 60_000\n",
        "\n",
        "    seconds = milliseconds // 1_000\n",
        "    milliseconds -= seconds * 1_000\n",
        "\n",
        "    hours_marker = f\"{hours:02d}:\" if always_include_hours or hours > 0 else \"\"\n",
        "    return (\n",
        "        f\"{hours_marker}{minutes:02d}:{seconds:02d}{decimal_marker}{milliseconds:03d}\"\n",
        "    )\n",
        "\n",
        "\n",
        "def write_srt(transcript, file):\n",
        "    \"\"\"\n",
        "    Write a transcript to a file in SRT format.\n",
        "\n",
        "    \"\"\"\n",
        "    for i, segment in enumerate(transcript, start=1):\n",
        "        # write srt lines\n",
        "        print(\n",
        "            f\"{i}\\n\"\n",
        "            f\"{format_timestamp(segment['start_time'], always_include_hours=True, decimal_marker=',')} --> \"\n",
        "            f\"{format_timestamp(segment['end_time'], always_include_hours=True, decimal_marker=',')}\\n\"\n",
        "            f\"{segment['speaker']}: {segment['text'].strip().replace('-->', '->')}\\n\",\n",
        "            file=file,\n",
        "            flush=True,\n",
        "        )\n",
        "\n",
        "\n",
        "def cleanup(path: str):\n",
        "    \"\"\"path could either be relative or absolute.\"\"\"\n",
        "    # check if file or directory exists\n",
        "    if os.path.isfile(path) or os.path.islink(path):\n",
        "        # remove file\n",
        "        os.remove(path)\n",
        "    elif os.path.isdir(path):\n",
        "        # remove directory and all its content\n",
        "        shutil.rmtree(path)\n",
        "    else:\n",
        "        raise ValueError(\"Path {} is not a file or dir.\".format(path))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T8yjdBY5JuMy",
        "outputId": "603f5c74-9dca-430f-fd2c-782119c04a85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-05-11 16:54:35--  https://dft3h5i221ap1.cloudfront.net/OpenAI/chatgpt-prompt-eng/video/prompt_eng_01_intro_v2.mp4\n",
            "Resolving dft3h5i221ap1.cloudfront.net (dft3h5i221ap1.cloudfront.net)... 18.64.182.54, 18.64.182.190, 18.64.182.228, ...\n",
            "Connecting to dft3h5i221ap1.cloudfront.net (dft3h5i221ap1.cloudfront.net)|18.64.182.54|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 97372023 (93M) [video/mp4]\n",
            "Saving to: ‘video.mp4’\n",
            "\n",
            "video.mp4           100%[===================>]  92.86M   101MB/s    in 0.9s    \n",
            "\n",
            "2023-05-11 16:54:36 (101 MB/s) - ‘video.mp4’ saved [97372023/97372023]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://dft3h5i221ap1.cloudfront.net/OpenAI/chatgpt-prompt-eng/video/prompt_eng_01_intro_v2.mp4 -O video.mp4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xpGboeU6Jpx9",
        "outputId": "ac3fce29-030b-40b8-e25a-01ccfa53c662"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ffmpeg version 4.2.7-0ubuntu0.1 Copyright (c) 2000-2022 the FFmpeg developers\n",
            "  built with gcc 9 (Ubuntu 9.4.0-1ubuntu1~20.04.1)\n",
            "  configuration: --prefix=/usr --extra-version=0ubuntu0.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-avresample --disable-filter=resample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librsvg --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-nvenc --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
            "  libavutil      56. 31.100 / 56. 31.100\n",
            "  libavcodec     58. 54.100 / 58. 54.100\n",
            "  libavformat    58. 29.100 / 58. 29.100\n",
            "  libavdevice    58.  8.100 / 58.  8.100\n",
            "  libavfilter     7. 57.100 /  7. 57.100\n",
            "  libavresample   4.  0.  0 /  4.  0.  0\n",
            "  libswscale      5.  5.100 /  5.  5.100\n",
            "  libswresample   3.  5.100 /  3.  5.100\n",
            "  libpostproc    55.  5.100 / 55.  5.100\n",
            "Input #0, mov,mp4,m4a,3gp,3g2,mj2, from 'video.mp4':\n",
            "  Metadata:\n",
            "    major_brand     : mp42\n",
            "    minor_version   : 0\n",
            "    compatible_brands: mp42mp41\n",
            "    creation_time   : 2023-04-29T04:26:57.000000Z\n",
            "  Duration: 00:06:27.88, start: 0.000000, bitrate: 2008 kb/s\n",
            "    Stream #0:0(eng): Video: h264 (Main) (avc1 / 0x31637661), yuv420p(tv, bt709), 1920x1080, 1813 kb/s, 29.97 fps, 29.97 tbr, 30k tbn, 60k tbc (default)\n",
            "    Metadata:\n",
            "      creation_time   : 2023-04-29T04:26:57.000000Z\n",
            "      handler_name    : ?Mainconcept Video Media Handler\n",
            "      encoder         : AVC Coding\n",
            "    Stream #0:1(eng): Audio: aac (LC) (mp4a / 0x6134706D), 48000 Hz, stereo, fltp, 189 kb/s (default)\n",
            "    Metadata:\n",
            "      creation_time   : 2023-04-29T04:26:57.000000Z\n",
            "      handler_name    : #Mainconcept MP4 Sound Media Handler\n",
            "Stream mapping:\n",
            "  Stream #0:1 -> #0:0 (aac (native) -> pcm_s16le (native))\n",
            "Press [q] to stop, [?] for help\n",
            "Output #0, wav, to 'audio.wav':\n",
            "  Metadata:\n",
            "    major_brand     : mp42\n",
            "    minor_version   : 0\n",
            "    compatible_brands: mp42mp41\n",
            "    ISFT            : Lavf58.29.100\n",
            "    Stream #0:0(eng): Audio: pcm_s16le ([1][0][0][0] / 0x0001), 48000 Hz, stereo, s16, 1536 kb/s (default)\n",
            "    Metadata:\n",
            "      creation_time   : 2023-04-29T04:26:57.000000Z\n",
            "      handler_name    : #Mainconcept MP4 Sound Media Handler\n",
            "      encoder         : Lavc58.54.100 pcm_s16le\n",
            "size=   72724kB time=00:06:27.86 bitrate=1536.0kbits/s speed= 392x    \n",
            "video:0kB audio:72724kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.000105%\n"
          ]
        }
      ],
      "source": [
        "!ffmpeg -i video.mp4 -vn -acodec pcm_s16le audio.wav"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nPlZP62NAhqf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 457,
          "referenced_widgets": [
            "3773fb143de346e69212e6df96662f46",
            "5214b15778924e52b1d56c57c2348659",
            "4db24865b7f3465e8d4dddf5e33848f1",
            "cc0a3360cc0d4498a4db438cb5000fd5",
            "18cade3631fd4b91a5ba5201ce300832",
            "0249095d9236437d8760ea041bce6dab",
            "3e75f9740e7c4936ac1a71ceaae6eb50",
            "f9bec7c53e984f9dba4d41ad85ba895c",
            "c1d7b1cb65c94b92a167a23336386cb0",
            "c3b0bf3319e640d7a017f6be6c02ba45",
            "682a3957f82a48c6826855cea5aeadfc",
            "57aa0e9a55b347e895209fb9115c6513",
            "2cccf41982634129b51fd5dde7d48862",
            "7e84773afe36483a877403c1afdc8453",
            "b6f4254416144b228a6dd7164bae69d9",
            "429669e094b049d38485f0f4d9b4f9c1",
            "5d109e51c5f94965978a8c12f4d0861b",
            "51b161891a2c4c268385a723155da2c3",
            "8cfd49af459543c1b69772d937d9cee6",
            "6c9a9dc7aed742e68b982aad753a7dae",
            "5c6a159fe5474ee6b8f5993d6d260bc7",
            "b886f32c5aa84c479960b5b8e05895dd",
            "968be373cf3c424da921e4aa3d2ab8a7",
            "d2f04589571c4a4cad3321cb2dcc3bf9",
            "bfe1cf170bfa423d8d26a297f2aea398",
            "34685e5b4bcc409c82e96cac6d6a9ca3",
            "ac5ecd58c65946af8c6a3f9ba9546203",
            "7b3aff328b5d43b88d0a8399536f26a9",
            "c4d56e37dea24a6c812e698905e02794",
            "e187310dbedf42aebfec7c930e6f1329",
            "43768afed1c64e1da9cb9b5617497e4e",
            "9968d21744c147059f60f361b5a40aaa",
            "1efa7f9770e940e5824e04f18b1a208c",
            "3e746ab0734d45fc94725e370711fbf1",
            "45bc9895384b42a4870d0ec8a2b9f5b6",
            "5ffcc9fea8c24c61b627d9fef5bfb0ba",
            "ebfd0fa65d2446b48bc33b93fc8b3a02",
            "07c80d01f3814ef59e15939bf5703aa1",
            "9d1bb10333694b578af8cda92cc3859d",
            "6939d409875b41f49ef609090d32774b",
            "ad00042d90d94169bc128ecda22def06",
            "cb8606425d6f45878472e234996b622d",
            "58cdc726fb094aa8ae71490642403490",
            "8f6e70c1277a47449e3dde923cd878c7"
          ]
        },
        "id": "I7lDuZNvdSRk",
        "outputId": "fab7fafa-88ba-40c2-bf60-e6f7b876e0fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[NeMo W 2023-05-11 16:55:07 optimizers:54] Apex was not found. Using the lamb or fused_adam optimizer will error out.\n",
            "[NeMo W 2023-05-11 16:55:08 experimental:27] Module <class 'nemo.collections.asr.modules.audio_modules.SpectrogramToMultichannelFeatures'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)7a179508/config.json:   0%|          | 0.00/2.80k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3773fb143de346e69212e6df96662f46"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading model.bin:   0%|          | 0.00/3.09G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "57aa0e9a55b347e895209fb9115c6513"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)79508/vocabulary.txt:   0%|          | 0.00/460k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "968be373cf3c424da921e4aa3d2ab8a7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)79508/tokenizer.json:   0%|          | 0.00/2.20M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3e746ab0734d45fc94725e370711fbf1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No language specified, language will be first be detected for each audio file (increases inference time).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████| 16.9M/16.9M [00:01<00:00, 14.4MiB/s]\n",
            "INFO:pytorch_lightning.utilities.migration.utils:Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v1.9.4. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint --file ../root/.cache/torch/whisperx-vad-segmentation.bin`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model was trained with pyannote.audio 0.0.1, yours is 2.1.1. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
            "Model was trained with torch 1.10.0+cu102, yours is 2.0.0+cu118. Bad things might happen unless you revert torch to 1.x.\n",
            "Detected language: en (1.00) in first 30s of audio...\n",
            "[{'text': \" Welcome to this course on ChatGPT Prompt Engineering for Developers. I'm thrilled to have with me Iza Fulford to teach this along with me. She's a member of the technical staff of OpenAI and had built the popular ChatGPT Retrieval plugin. And a large part of her work has been teaching people how to use OLM or Large Language Model technology in products. She's also contributed to the OpenAI Cookbook that teaches people prompting. So thrilled to have you with me.\", 'start': 5.138, 'end': 32.645}, {'text': \" And I'm thrilled to be here and share some prompting best practices with you all. So there's been a lot of material on the internet for prompting with articles like 30 prompts everyone has to know. A lot of that has been focused on the chat GPT web user interface, which many people are using to do specific and often one-off tasks.\", 'start': 32.763, 'end': 54.903}, {'text': \" But I think the power of LLMs, Large Language Models, as a developer tool that is using API calls to LLMs to quickly build software applications, I think that is still very underappreciated. In fact, my team at AI Fund, which is a sister company to Deep Learning AI, has been working with many startups on applying these technologies to many different applications. And it's been exciting to see what LLM APIs can enable developers\", 'start': 54.903, 'end': 83.489}, {'text': \" to very quickly build. So in this course, we'll share with you some of the possibilities for what you can do as well as best practices for how you can do them. There's a lot of material to cover. First, you'll learn best, some prompting best practices for software development. Then we'll cover some common use cases, summarizing, inferring, transforming, expanding, and then you'll build a chatbot using an LLM.\", 'start': 83.489, 'end': 110.05}, {'text': \" We hope that this will spark your imagination about new applications that you can build. So in the development of large language models or LLMs, there have been broadly two types of LLMs, which I'm going to refer to as base LLMs and instruction-tuned LLMs. So base LLM has been trained to predict the next word based on text training data, often trained on a large amount of data from the internet and other sources to figure out what's the next most likely word to follow.\", 'start': 110.32, 'end': 139.396}, {'text': ' So for example, if you were to prompt this, once upon a time there was a unicorn, it may complete this, that is, it may predict the next several words are that live in a magical forest with all unicorn friends.', 'start': 139.396, 'end': 150.668}, {'text': \" But if you were to prompt us with what is the capital of France, then based on what articles on the internet might have, it's quite possible that the base LLM will complete this with what is France's largest city, what is France's population and so on, because articles on the internet could quite plausibly be lists of quiz questions about the country of France.\", 'start': 151.9, 'end': 174.58}, {'text': \" In contrast, an instruction-tuned LLM, which is where a lot of momentum of LLM research and practice has been going, an instruction-tuned LLM has been trained to follow instructions. So if you were to ask it, what is the capital of France? It's much more likely to output something like the capital of France is Paris.\", 'start': 175.576, 'end': 195.623}, {'text': \" So the way that instruction-tuned OLMs are typically trained is you start off with a base OLM that's been trained on a huge amount of text data and further train it, further fine-tune it with inputs and outputs that are instructions and good attempts to follow those instructions. And then often further refine using a technique called RLHF, reinforcement learning from human feedback, to make the system better able to be helpful and follow instructions.\", 'start': 195.623, 'end': 223.501}, {'text': \" Because instruction-tuned LLMs have been trained to be helpful, honest, and harmless, so, for example, they're less likely to output problematic text, such as toxic outputs, compared to base LLM, a lot of the practical usage scenarios have been shifting toward instruction-tuned LLMs.\", 'start': 223.501, 'end': 241.135}, {'text': ' Some of the best practices you find on the internet may be more suited for a base LLM, but for most practical applications today, we would recommend most people instead focus on instruction-tuned LLMs, which are easier to use, and also because of the work of OpenAI and other LLM companies becoming safer and more aligned. So this course will focus on best practices for instruction-tuned LLMs.', 'start': 241.135, 'end': 268.0}, {'text': ' which is what we recommend you use for most of your applications.', 'start': 268.102, 'end': 271.882}, {'text': \" Before moving on, I just want to acknowledge the team from OpenAI and DeepLearning.AI that had contributed to the materials that Yizi and I will be presenting. I'm very grateful to Andrew Main, Joe Palermo, Boris Power, Ted Sanders, and Lillian Wang from OpenAI that were very involved with us brainstorming materials, vetting the materials to put together the curriculum for this short course. And I'm also grateful on the deep learning side for the work of Jeff Ludwig, Eddie Hsu, and Tommy Nelson.\", 'start': 272.371, 'end': 301.261}, {'text': \" So when you use an instruction-tuned LLM, think of giving instructions to another person, say someone that's smart but doesn't know the specifics of your task. So when an LLM doesn't work, sometimes it's because the instructions weren't clear enough. For example, if you were to say, please write me something about Alan Turing. Well, in addition to that, it can be helpful to be clear about whether you want\", 'start': 301.261, 'end': 328.092}, {'text': ' the text to focus on his scientific work or his personal life or his role in history or something else and if you specify what you want the tone of the text to be should it take on the tone like a professional journalist would write', 'start': 328.092, 'end': 343.094}, {'text': ' Or is it more of a casual note that you dash off to a friend? That holds. The OM generate what you want. And of course, if you picture yourself asking, say, a fresh college graduate to carry out this task for you, if you can even specify what snippets of text they should read in advance to write this text about Alan Turing, then that even better sets up that fresh college grad for success to carry out this task for you.', 'start': 343.094, 'end': 368.947}, {'text': \" So in the next video, you see examples of how to be clear and specific, which is an important principle of prompting LLMs. And you also learn from Isa a second principle of prompting that is giving a DLM time to think. So with that, let's go on to the next video.\", 'start': 368.947, 'end': 387.56}]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/torchaudio/models/wav2vec2_fairseq_base_ls960_asr_ls960.pth\" to /root/.cache/torch/hub/checkpoints/wav2vec2_fairseq_base_ls960_asr_ls960.pth\n",
            "100%|██████████| 360M/360M [00:02<00:00, 185MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'start': 5.238, 'end': 9.499, 'text': ' Welcome to this course on ChatGPT Prompt Engineering for Developers.', 'words': [{'word': 'Welcome', 'start': 5.238, 'end': 5.538, 'score': 0.951}, {'word': 'to', 'start': 5.538, 'end': 5.638, 'score': 0.679}, {'word': 'this', 'start': 5.638, 'end': 5.778, 'score': 0.941}, {'word': 'course', 'start': 5.778, 'end': 6.118, 'score': 0.839}, {'word': 'on', 'start': 6.118, 'end': 6.318, 'score': 0.904}, {'word': 'ChatGPT', 'start': 6.318, 'end': 7.098, 'score': 0.804}, {'word': 'Prompt', 'start': 7.098, 'end': 7.419, 'score': 0.738}, {'word': 'Engineering', 'start': 7.419, 'end': 7.959, 'score': 0.94}, {'word': 'for', 'start': 7.959, 'end': 8.259, 'score': 0.915}, {'word': 'Developers.', 'start': 8.259, 'end': 8.919, 'score': 0.898}]}, {'start': 9.499, 'end': 14.62, 'text': \"I'm thrilled to have with me Iza Fulford to teach this along with me.\", 'words': [{'word': \"I'm\", 'start': 9.499, 'end': 9.639, 'score': 0.608}, {'word': 'thrilled', 'start': 9.639, 'end': 9.899, 'score': 0.437}, {'word': 'to', 'start': 9.899, 'end': 10.019, 'score': 0.544}, {'word': 'have', 'start': 10.019, 'end': 10.259, 'score': 0.643}, {'word': 'with', 'start': 10.259, 'end': 10.559, 'score': 0.847}, {'word': 'me', 'start': 10.559, 'end': 10.739, 'score': 0.905}, {'word': 'Iza', 'start': 10.739, 'end': 11.42, 'score': 0.689}, {'word': 'Fulford', 'start': 11.42, 'end': 11.92, 'score': 0.576}, {'word': 'to', 'start': 11.92, 'end': 12.58, 'score': 0.842}, {'word': 'teach', 'start': 12.58, 'end': 13.06, 'score': 0.897}, {'word': 'this', 'start': 13.06, 'end': 13.32, 'score': 0.775}, {'word': 'along', 'start': 13.32, 'end': 13.74, 'score': 0.86}, {'word': 'with', 'start': 13.74, 'end': 13.92, 'score': 0.931}, {'word': 'me.', 'start': 13.92, 'end': 14.1, 'score': 0.773}]}, {'start': 14.62, 'end': 21.182, 'text': \"She's a member of the technical staff of OpenAI and had built the popular ChatGPT Retrieval plugin.\", 'words': [{'word': \"She's\", 'start': 14.62, 'end': 14.9, 'score': 0.451}, {'word': 'a', 'start': 14.9, 'end': 14.981, 'score': 0.691}, {'word': 'member', 'start': 14.981, 'end': 15.341, 'score': 0.83}, {'word': 'of', 'start': 15.341, 'end': 15.421, 'score': 0.834}, {'word': 'the', 'start': 15.421, 'end': 15.561, 'score': 0.853}, {'word': 'technical', 'start': 15.561, 'end': 16.061, 'score': 0.865}, {'word': 'staff', 'start': 16.061, 'end': 16.361, 'score': 0.87}, {'word': 'of', 'start': 16.361, 'end': 16.481, 'score': 0.64}, {'word': 'OpenAI', 'start': 16.481, 'end': 17.141, 'score': 0.794}, {'word': 'and', 'start': 17.141, 'end': 17.501, 'score': 0.856}, {'word': 'had', 'start': 17.501, 'end': 17.761, 'score': 0.939}, {'word': 'built', 'start': 17.761, 'end': 18.041, 'score': 0.741}, {'word': 'the', 'start': 18.041, 'end': 18.201, 'score': 0.616}, {'word': 'popular', 'start': 18.201, 'end': 18.681, 'score': 0.879}, {'word': 'ChatGPT', 'start': 18.681, 'end': 19.522, 'score': 0.778}, {'word': 'Retrieval', 'start': 19.522, 'end': 20.162, 'score': 0.913}, {'word': 'plugin.', 'start': 20.162, 'end': 20.622, 'score': 0.897}]}, {'start': 21.182, 'end': 27.804, 'text': 'And a large part of her work has been teaching people how to use OLM or Large Language Model technology in products.', 'words': [{'word': 'And', 'start': 21.182, 'end': 21.282, 'score': 0.833}, {'word': 'a', 'start': 21.282, 'end': 21.322, 'score': 0.5}, {'word': 'large', 'start': 21.322, 'end': 21.702, 'score': 0.908}, {'word': 'part', 'start': 21.702, 'end': 21.902, 'score': 0.888}, {'word': 'of', 'start': 21.902, 'end': 21.962, 'score': 0.994}, {'word': 'her', 'start': 21.962, 'end': 22.042, 'score': 0.252}, {'word': 'work', 'start': 22.042, 'end': 22.302, 'score': 0.634}, {'word': 'has', 'start': 22.302, 'end': 22.482, 'score': 0.877}, {'word': 'been', 'start': 22.482, 'end': 22.662, 'score': 0.949}, {'word': 'teaching', 'start': 22.662, 'end': 23.003, 'score': 0.875}, {'word': 'people', 'start': 23.003, 'end': 23.363, 'score': 0.789}, {'word': 'how', 'start': 23.363, 'end': 23.923, 'score': 0.974}, {'word': 'to', 'start': 23.923, 'end': 24.043, 'score': 0.966}, {'word': 'use', 'start': 24.043, 'end': 24.343, 'score': 0.71}, {'word': 'OLM', 'start': 24.343, 'end': 24.903, 'score': 0.844}, {'word': 'or', 'start': 24.903, 'end': 25.123, 'score': 0.854}, {'word': 'Large', 'start': 25.123, 'end': 25.423, 'score': 0.849}, {'word': 'Language', 'start': 25.423, 'end': 25.803, 'score': 0.781}, {'word': 'Model', 'start': 25.803, 'end': 26.163, 'score': 0.772}, {'word': 'technology', 'start': 26.163, 'end': 26.784, 'score': 0.457}, {'word': 'in', 'start': 26.784, 'end': 26.944, 'score': 0.87}, {'word': 'products.', 'start': 26.944, 'end': 27.384, 'score': 0.881}]}, {'start': 27.804, 'end': 31.565, 'text': \"She's also contributed to the OpenAI Cookbook that teaches people prompting.\", 'words': [{'word': \"She's\", 'start': 27.804, 'end': 28.004, 'score': 0.561}, {'word': 'also', 'start': 28.004, 'end': 28.264, 'score': 0.93}, {'word': 'contributed', 'start': 28.264, 'end': 28.804, 'score': 0.78}, {'word': 'to', 'start': 28.804, 'end': 28.944, 'score': 0.734}, {'word': 'the', 'start': 28.944, 'end': 29.064, 'score': 0.8}, {'word': 'OpenAI', 'start': 29.064, 'end': 29.684, 'score': 0.779}, {'word': 'Cookbook', 'start': 29.684, 'end': 30.144, 'score': 0.865}, {'word': 'that', 'start': 30.144, 'end': 30.364, 'score': 0.969}, {'word': 'teaches', 'start': 30.364, 'end': 30.685, 'score': 0.762}, {'word': 'people', 'start': 30.685, 'end': 30.985, 'score': 0.863}, {'word': 'prompting.', 'start': 30.985, 'end': 31.545, 'score': 0.773}]}, {'start': 31.565, 'end': 32.525, 'text': 'So thrilled to have you with me.', 'words': [{'word': 'So', 'start': 31.565, 'end': 31.625, 'score': 0.046}, {'word': 'thrilled', 'start': 31.625, 'end': 31.925, 'score': 0.454}, {'word': 'to', 'start': 31.925, 'end': 32.025, 'score': 0.757}, {'word': 'have', 'start': 32.025, 'end': 32.185, 'score': 0.826}, {'word': 'you', 'start': 32.185, 'end': 32.305, 'score': 0.381}, {'word': 'with', 'start': 32.305, 'end': 32.425, 'score': 0.979}, {'word': 'me.', 'start': 32.425, 'end': 32.525, 'score': 0.343}]}, {'start': 32.883, 'end': 38.348, 'text': \" And I'm thrilled to be here and share some prompting best practices with you all.\", 'words': [{'word': 'And', 'start': 32.883, 'end': 32.983, 'score': 0.917}, {'word': \"I'm\", 'start': 32.983, 'end': 33.183, 'score': 0.847}, {'word': 'thrilled', 'start': 33.183, 'end': 33.444, 'score': 0.588}, {'word': 'to', 'start': 33.444, 'end': 33.544, 'score': 0.491}, {'word': 'be', 'start': 33.544, 'end': 33.704, 'score': 0.613}, {'word': 'here', 'start': 33.704, 'end': 33.964, 'score': 0.664}, {'word': 'and', 'start': 33.964, 'end': 34.465, 'score': 0.929}, {'word': 'share', 'start': 34.465, 'end': 34.745, 'score': 0.743}, {'word': 'some', 'start': 34.745, 'end': 34.945, 'score': 0.561}, {'word': 'prompting', 'start': 34.945, 'end': 35.345, 'score': 0.849}, {'word': 'best', 'start': 35.345, 'end': 35.606, 'score': 0.798}, {'word': 'practices', 'start': 35.606, 'end': 36.146, 'score': 0.885}, {'word': 'with', 'start': 36.146, 'end': 36.306, 'score': 0.942}, {'word': 'you', 'start': 36.306, 'end': 36.486, 'score': 0.711}, {'word': 'all.', 'start': 36.486, 'end': 36.687, 'score': 0.751}]}, {'start': 38.348, 'end': 44.914, 'text': \"So there's been a lot of material on the internet for prompting with articles like 30 prompts everyone has to know.\", 'words': [{'word': 'So', 'start': 38.348, 'end': 38.528, 'score': 0.894}, {'word': \"there's\", 'start': 38.528, 'end': 38.929, 'score': 0.903}, {'word': 'been', 'start': 38.929, 'end': 39.129, 'score': 0.885}, {'word': 'a', 'start': 39.129, 'end': 39.169, 'score': 0.5}, {'word': 'lot', 'start': 39.169, 'end': 39.609, 'score': 0.935}, {'word': 'of', 'start': 39.609, 'end': 39.689, 'score': 0.875}, {'word': 'material', 'start': 39.689, 'end': 40.21, 'score': 0.854}, {'word': 'on', 'start': 40.21, 'end': 40.37, 'score': 0.831}, {'word': 'the', 'start': 40.37, 'end': 40.47, 'score': 0.912}, {'word': 'internet', 'start': 40.47, 'end': 40.91, 'score': 0.626}, {'word': 'for', 'start': 40.91, 'end': 41.111, 'score': 0.936}, {'word': 'prompting', 'start': 41.111, 'end': 41.711, 'score': 0.828}, {'word': 'with', 'start': 41.711, 'end': 42.031, 'score': 0.868}, {'word': 'articles', 'start': 42.031, 'end': 42.632, 'score': 0.892}, {'word': 'like', 'start': 42.632, 'end': 42.872, 'score': 0.907}, {'word': '30', 'start': 42.872, 'end': 42.892, 'score': 0.998}, {'word': 'prompts', 'start': 42.892, 'end': 43.733, 'score': 0.678}, {'word': 'everyone', 'start': 43.733, 'end': 44.233, 'score': 0.869}, {'word': 'has', 'start': 44.233, 'end': 44.534, 'score': 0.792}, {'word': 'to', 'start': 44.534, 'end': 44.674, 'score': 0.791}, {'word': 'know.', 'start': 44.674, 'end': 44.894, 'score': 0.768}]}, {'start': 44.914, 'end': 54.342, 'text': 'A lot of that has been focused on the chat GPT web user interface, which many people are using to do specific and often one-off tasks.', 'words': [{'word': 'A', 'start': 44.914, 'end': 44.934, 'score': 0.0}, {'word': 'lot', 'start': 44.934, 'end': 46.095, 'score': 0.807}, {'word': 'of', 'start': 46.095, 'end': 46.195, 'score': 0.377}, {'word': 'that', 'start': 46.195, 'end': 46.415, 'score': 0.79}, {'word': 'has', 'start': 46.415, 'end': 46.616, 'score': 0.784}, {'word': 'been', 'start': 46.616, 'end': 46.796, 'score': 0.896}, {'word': 'focused', 'start': 46.796, 'end': 47.296, 'score': 0.765}, {'word': 'on', 'start': 47.296, 'end': 47.596, 'score': 0.834}, {'word': 'the', 'start': 47.596, 'end': 47.837, 'score': 0.864}, {'word': 'chat', 'start': 47.837, 'end': 48.197, 'score': 0.84}, {'word': 'GPT', 'start': 48.197, 'end': 48.717, 'score': 0.805}, {'word': 'web', 'start': 48.717, 'end': 49.218, 'score': 0.936}, {'word': 'user', 'start': 49.218, 'end': 49.558, 'score': 0.749}, {'word': 'interface,', 'start': 49.558, 'end': 50.179, 'score': 0.791}, {'word': 'which', 'start': 50.179, 'end': 50.499, 'score': 0.987}, {'word': 'many', 'start': 50.499, 'end': 50.779, 'score': 0.884}, {'word': 'people', 'start': 50.779, 'end': 51.04, 'score': 0.839}, {'word': 'are', 'start': 51.04, 'end': 51.16, 'score': 0.773}, {'word': 'using', 'start': 51.16, 'end': 51.56, 'score': 0.854}, {'word': 'to', 'start': 51.56, 'end': 51.98, 'score': 0.83}, {'word': 'do', 'start': 51.98, 'end': 52.181, 'score': 0.854}, {'word': 'specific', 'start': 52.181, 'end': 52.761, 'score': 0.924}, {'word': 'and', 'start': 52.761, 'end': 52.861, 'score': 0.901}, {'word': 'often', 'start': 52.861, 'end': 53.201, 'score': 0.715}, {'word': 'one-off', 'start': 53.201, 'end': 53.942, 'score': 0.906}, {'word': 'tasks.', 'start': 53.942, 'end': 54.342, 'score': 0.662}]}, {'start': 55.003, 'end': 68.346, 'text': ' But I think the power of LLMs, Large Language Models, as a developer tool that is using API calls to LLMs to quickly build software applications, I think that is still very underappreciated.', 'words': [{'word': 'But', 'start': 55.003, 'end': 55.143, 'score': 0.978}, {'word': 'I', 'start': 55.143, 'end': 55.603, 'score': 0.961}, {'word': 'think', 'start': 55.603, 'end': 55.763, 'score': 0.896}, {'word': 'the', 'start': 55.763, 'end': 55.863, 'score': 0.904}, {'word': 'power', 'start': 55.863, 'end': 56.343, 'score': 0.772}, {'word': 'of', 'start': 56.343, 'end': 56.523, 'score': 0.727}, {'word': 'LLMs,', 'start': 56.523, 'end': 57.163, 'score': 0.779}, {'word': 'Large', 'start': 57.163, 'end': 57.724, 'score': 0.668}, {'word': 'Language', 'start': 57.724, 'end': 58.304, 'score': 0.708}, {'word': 'Models,', 'start': 58.304, 'end': 58.684, 'score': 0.924}, {'word': 'as', 'start': 58.684, 'end': 58.984, 'score': 0.7}, {'word': 'a', 'start': 58.984, 'end': 59.084, 'score': 0.407}, {'word': 'developer', 'start': 59.084, 'end': 59.724, 'score': 0.778}, {'word': 'tool', 'start': 59.724, 'end': 59.984, 'score': 0.463}, {'word': 'that', 'start': 59.984, 'end': 60.624, 'score': 0.945}, {'word': 'is', 'start': 60.624, 'end': 60.804, 'score': 0.675}, {'word': 'using', 'start': 60.804, 'end': 61.164, 'score': 0.868}, {'word': 'API', 'start': 61.164, 'end': 61.704, 'score': 0.854}, {'word': 'calls', 'start': 61.704, 'end': 62.105, 'score': 0.86}, {'word': 'to', 'start': 62.105, 'end': 62.305, 'score': 0.786}, {'word': 'LLMs', 'start': 62.305, 'end': 62.765, 'score': 0.644}, {'word': 'to', 'start': 62.765, 'end': 62.985, 'score': 0.822}, {'word': 'quickly', 'start': 62.985, 'end': 63.385, 'score': 0.827}, {'word': 'build', 'start': 63.385, 'end': 63.705, 'score': 0.595}, {'word': 'software', 'start': 63.705, 'end': 64.105, 'score': 0.614}, {'word': 'applications,', 'start': 64.105, 'end': 64.765, 'score': 0.872}, {'word': 'I', 'start': 64.765, 'end': 64.945, 'score': 1.0}, {'word': 'think', 'start': 64.945, 'end': 65.165, 'score': 0.76}, {'word': 'that', 'start': 65.165, 'end': 65.465, 'score': 0.869}, {'word': 'is', 'start': 65.465, 'end': 66.025, 'score': 0.919}, {'word': 'still', 'start': 66.025, 'end': 66.285, 'score': 0.817}, {'word': 'very', 'start': 66.285, 'end': 66.605, 'score': 0.834}, {'word': 'underappreciated.', 'start': 66.605, 'end': 67.686, 'score': 0.915}]}, {'start': 68.346, 'end': 78.708, 'text': 'In fact, my team at AI Fund, which is a sister company to Deep Learning AI, has been working with many startups on applying these technologies to many different applications.', 'words': [{'word': 'In', 'start': 68.346, 'end': 68.426, 'score': 0.832}, {'word': 'fact,', 'start': 68.426, 'end': 68.626, 'score': 0.9}, {'word': 'my', 'start': 68.626, 'end': 68.826, 'score': 0.904}, {'word': 'team', 'start': 68.826, 'end': 69.106, 'score': 0.854}, {'word': 'at', 'start': 69.106, 'end': 69.226, 'score': 0.402}, {'word': 'AI', 'start': 69.226, 'end': 69.606, 'score': 0.915}, {'word': 'Fund,', 'start': 69.606, 'end': 69.946, 'score': 0.946}, {'word': 'which', 'start': 69.946, 'end': 70.186, 'score': 0.956}, {'word': 'is', 'start': 70.186, 'end': 70.326, 'score': 0.683}, {'word': 'a', 'start': 70.326, 'end': 70.406, 'score': 0.879}, {'word': 'sister', 'start': 70.406, 'end': 70.766, 'score': 0.771}, {'word': 'company', 'start': 70.766, 'end': 71.166, 'score': 0.837}, {'word': 'to', 'start': 71.166, 'end': 71.286, 'score': 0.767}, {'word': 'Deep', 'start': 71.286, 'end': 71.486, 'score': 0.774}, {'word': 'Learning', 'start': 71.486, 'end': 71.847, 'score': 0.3}, {'word': 'AI,', 'start': 71.847, 'end': 72.207, 'score': 0.867}, {'word': 'has', 'start': 72.207, 'end': 72.667, 'score': 0.989}, {'word': 'been', 'start': 72.667, 'end': 72.867, 'score': 0.771}, {'word': 'working', 'start': 72.867, 'end': 73.247, 'score': 0.832}, {'word': 'with', 'start': 73.247, 'end': 73.467, 'score': 0.801}, {'word': 'many', 'start': 73.467, 'end': 73.767, 'score': 0.911}, {'word': 'startups', 'start': 73.767, 'end': 74.367, 'score': 0.779}, {'word': 'on', 'start': 74.367, 'end': 74.607, 'score': 0.894}, {'word': 'applying', 'start': 74.607, 'end': 75.067, 'score': 0.685}, {'word': 'these', 'start': 75.067, 'end': 75.607, 'score': 0.828}, {'word': 'technologies', 'start': 75.607, 'end': 76.528, 'score': 0.679}, {'word': 'to', 'start': 76.528, 'end': 76.968, 'score': 0.88}, {'word': 'many', 'start': 76.968, 'end': 77.248, 'score': 0.849}, {'word': 'different', 'start': 77.248, 'end': 77.608, 'score': 0.924}, {'word': 'applications.', 'start': 77.608, 'end': 78.448, 'score': 0.919}]}, {'start': 78.708, 'end': 83.369, 'text': \"And it's been exciting to see what LLM APIs can enable developers\", 'words': [{'word': 'And', 'start': 78.708, 'end': 78.828, 'score': 0.884}, {'word': \"it's\", 'start': 78.828, 'end': 79.148, 'score': 0.498}, {'word': 'been', 'start': 79.148, 'end': 79.328, 'score': 0.845}, {'word': 'exciting', 'start': 79.328, 'end': 79.788, 'score': 0.828}, {'word': 'to', 'start': 79.788, 'end': 79.968, 'score': 0.887}, {'word': 'see', 'start': 79.968, 'end': 80.268, 'score': 0.827}, {'word': 'what', 'start': 80.268, 'end': 80.788, 'score': 0.919}, {'word': 'LLM', 'start': 80.788, 'end': 81.369, 'score': 0.588}, {'word': 'APIs', 'start': 81.369, 'end': 82.049, 'score': 0.833}, {'word': 'can', 'start': 82.049, 'end': 82.269, 'score': 0.934}, {'word': 'enable', 'start': 82.269, 'end': 82.769, 'score': 0.725}, {'word': 'developers', 'start': 82.769, 'end': 83.369, 'score': 0.741}]}, {'start': 83.709, 'end': 85.871, 'text': ' to very quickly build.', 'words': [{'word': 'to', 'start': 83.709, 'end': 83.869, 'score': 0.865}, {'word': 'very', 'start': 83.869, 'end': 84.21, 'score': 0.956}, {'word': 'quickly', 'start': 84.21, 'end': 84.69, 'score': 0.937}, {'word': 'build.', 'start': 84.69, 'end': 85.13, 'score': 0.788}]}, {'start': 85.871, 'end': 94.818, 'text': \"So in this course, we'll share with you some of the possibilities for what you can do as well as best practices for how you can do them.\", 'words': [{'word': 'So', 'start': 85.871, 'end': 86.051, 'score': 0.684}, {'word': 'in', 'start': 86.051, 'end': 86.251, 'score': 0.904}, {'word': 'this', 'start': 86.251, 'end': 86.391, 'score': 0.616}, {'word': 'course,', 'start': 86.391, 'end': 86.752, 'score': 0.842}, {'word': \"we'll\", 'start': 86.752, 'end': 87.432, 'score': 0.754}, {'word': 'share', 'start': 87.432, 'end': 87.692, 'score': 0.877}, {'word': 'with', 'start': 87.692, 'end': 87.812, 'score': 0.258}, {'word': 'you', 'start': 87.812, 'end': 88.013, 'score': 0.786}, {'word': 'some', 'start': 88.013, 'end': 88.293, 'score': 0.786}, {'word': 'of', 'start': 88.293, 'end': 88.353, 'score': 0.986}, {'word': 'the', 'start': 88.353, 'end': 88.493, 'score': 0.75}, {'word': 'possibilities', 'start': 88.493, 'end': 89.354, 'score': 0.895}, {'word': 'for', 'start': 89.354, 'end': 89.594, 'score': 0.984}, {'word': 'what', 'start': 89.594, 'end': 89.814, 'score': 0.906}, {'word': 'you', 'start': 89.814, 'end': 90.194, 'score': 0.908}, {'word': 'can', 'start': 90.194, 'end': 90.354, 'score': 0.974}, {'word': 'do', 'start': 90.354, 'end': 90.555, 'score': 0.796}, {'word': 'as', 'start': 90.555, 'end': 90.975, 'score': 0.813}, {'word': 'well', 'start': 90.975, 'end': 91.175, 'score': 0.86}, {'word': 'as', 'start': 91.175, 'end': 91.255, 'score': 0.833}, {'word': 'best', 'start': 91.255, 'end': 91.535, 'score': 0.583}, {'word': 'practices', 'start': 91.535, 'end': 92.036, 'score': 0.845}, {'word': 'for', 'start': 92.036, 'end': 92.316, 'score': 0.957}, {'word': 'how', 'start': 92.316, 'end': 92.756, 'score': 0.887}, {'word': 'you', 'start': 92.756, 'end': 93.097, 'score': 0.932}, {'word': 'can', 'start': 93.097, 'end': 93.237, 'score': 0.997}, {'word': 'do', 'start': 93.237, 'end': 93.417, 'score': 0.733}, {'word': 'them.', 'start': 93.417, 'end': 93.617, 'score': 0.889}]}, {'start': 94.818, 'end': 97.0, 'text': \"There's a lot of material to cover.\", 'words': [{'word': \"There's\", 'start': 94.818, 'end': 95.018, 'score': 0.859}, {'word': 'a', 'start': 95.018, 'end': 95.078, 'score': 0.748}, {'word': 'lot', 'start': 95.078, 'end': 95.238, 'score': 0.754}, {'word': 'of', 'start': 95.238, 'end': 95.318, 'score': 0.832}, {'word': 'material', 'start': 95.318, 'end': 95.799, 'score': 0.804}, {'word': 'to', 'start': 95.799, 'end': 95.939, 'score': 0.723}, {'word': 'cover.', 'start': 95.939, 'end': 96.339, 'score': 0.708}]}, {'start': 97.0, 'end': 101.243, 'text': \"First, you'll learn best, some prompting best practices for software development.\", 'words': [{'word': 'First,', 'start': 97.0, 'end': 97.38, 'score': 0.786}, {'word': \"you'll\", 'start': 97.38, 'end': 97.6, 'score': 0.728}, {'word': 'learn', 'start': 97.6, 'end': 97.96, 'score': 0.678}, {'word': 'best,', 'start': 97.96, 'end': 98.341, 'score': 0.916}, {'word': 'some', 'start': 98.341, 'end': 98.821, 'score': 0.701}, {'word': 'prompting', 'start': 98.821, 'end': 99.201, 'score': 0.808}, {'word': 'best', 'start': 99.201, 'end': 99.422, 'score': 0.882}, {'word': 'practices', 'start': 99.422, 'end': 99.882, 'score': 0.857}, {'word': 'for', 'start': 99.882, 'end': 100.022, 'score': 0.848}, {'word': 'software', 'start': 100.022, 'end': 100.362, 'score': 0.487}, {'word': 'development.', 'start': 100.362, 'end': 100.883, 'score': 0.859}]}, {'start': 101.243, 'end': 109.269, 'text': \"Then we'll cover some common use cases, summarizing, inferring, transforming, expanding, and then you'll build a chatbot using an LLM.\", 'words': [{'word': 'Then', 'start': 101.243, 'end': 101.383, 'score': 0.684}, {'word': \"we'll\", 'start': 101.383, 'end': 101.543, 'score': 0.639}, {'word': 'cover', 'start': 101.543, 'end': 101.944, 'score': 0.962}, {'word': 'some', 'start': 101.944, 'end': 102.304, 'score': 0.829}, {'word': 'common', 'start': 102.304, 'end': 102.604, 'score': 0.974}, {'word': 'use', 'start': 102.604, 'end': 102.784, 'score': 0.797}, {'word': 'cases,', 'start': 102.784, 'end': 103.305, 'score': 0.778}, {'word': 'summarizing,', 'start': 103.305, 'end': 104.406, 'score': 0.872}, {'word': 'inferring,', 'start': 104.406, 'end': 105.106, 'score': 0.807}, {'word': 'transforming,', 'start': 105.106, 'end': 106.107, 'score': 0.858}, {'word': 'expanding,', 'start': 106.107, 'end': 106.807, 'score': 0.813}, {'word': 'and', 'start': 106.807, 'end': 107.228, 'score': 0.861}, {'word': 'then', 'start': 107.228, 'end': 107.448, 'score': 0.832}, {'word': \"you'll\", 'start': 107.448, 'end': 107.648, 'score': 0.782}, {'word': 'build', 'start': 107.648, 'end': 107.968, 'score': 0.794}, {'word': 'a', 'start': 107.968, 'end': 108.209, 'score': 0.755}, {'word': 'chatbot', 'start': 108.209, 'end': 108.649, 'score': 0.861}, {'word': 'using', 'start': 108.649, 'end': 108.989, 'score': 0.893}, {'word': 'an', 'start': 108.989, 'end': 109.069, 'score': 0.827}, {'word': 'LLM.', 'start': 109.069, 'end': 109.269, 'score': 0.602}]}, {'start': 110.4, 'end': 115.303, 'text': ' We hope that this will spark your imagination about new applications that you can build.', 'words': [{'word': 'We', 'start': 110.4, 'end': 110.5, 'score': 0.91}, {'word': 'hope', 'start': 110.5, 'end': 110.74, 'score': 0.918}, {'word': 'that', 'start': 110.74, 'end': 111.04, 'score': 0.867}, {'word': 'this', 'start': 111.04, 'end': 111.241, 'score': 0.845}, {'word': 'will', 'start': 111.241, 'end': 111.401, 'score': 0.991}, {'word': 'spark', 'start': 111.401, 'end': 111.701, 'score': 0.848}, {'word': 'your', 'start': 111.701, 'end': 111.841, 'score': 0.906}, {'word': 'imagination', 'start': 111.841, 'end': 112.501, 'score': 0.726}, {'word': 'about', 'start': 112.501, 'end': 112.861, 'score': 0.708}, {'word': 'new', 'start': 112.861, 'end': 113.021, 'score': 0.817}, {'word': 'applications', 'start': 113.021, 'end': 113.602, 'score': 0.877}, {'word': 'that', 'start': 113.602, 'end': 113.762, 'score': 0.905}, {'word': 'you', 'start': 113.762, 'end': 113.922, 'score': 0.811}, {'word': 'can', 'start': 113.922, 'end': 114.042, 'score': 0.93}, {'word': 'build.', 'start': 114.042, 'end': 114.302, 'score': 0.734}]}, {'start': 115.303, 'end': 126.129, 'text': \"So in the development of large language models or LLMs, there have been broadly two types of LLMs, which I'm going to refer to as base LLMs and instruction-tuned LLMs.\", 'words': [{'word': 'So', 'start': 115.303, 'end': 115.483, 'score': 0.887}, {'word': 'in', 'start': 115.483, 'end': 115.643, 'score': 0.812}, {'word': 'the', 'start': 115.643, 'end': 115.743, 'score': 0.888}, {'word': 'development', 'start': 115.743, 'end': 116.303, 'score': 0.795}, {'word': 'of', 'start': 116.303, 'end': 116.423, 'score': 0.727}, {'word': 'large', 'start': 116.423, 'end': 116.924, 'score': 0.722}, {'word': 'language', 'start': 116.924, 'end': 117.244, 'score': 0.702}, {'word': 'models', 'start': 117.244, 'end': 117.604, 'score': 0.852}, {'word': 'or', 'start': 117.604, 'end': 117.724, 'score': 0.981}, {'word': 'LLMs,', 'start': 117.724, 'end': 118.224, 'score': 0.793}, {'word': 'there', 'start': 118.224, 'end': 118.344, 'score': 0.161}, {'word': 'have', 'start': 118.344, 'end': 118.464, 'score': 0.249}, {'word': 'been', 'start': 118.464, 'end': 118.685, 'score': 0.84}, {'word': 'broadly', 'start': 118.685, 'end': 119.165, 'score': 0.87}, {'word': 'two', 'start': 119.165, 'end': 119.545, 'score': 0.906}, {'word': 'types', 'start': 119.545, 'end': 119.885, 'score': 0.906}, {'word': 'of', 'start': 119.885, 'end': 120.005, 'score': 0.754}, {'word': 'LLMs,', 'start': 120.005, 'end': 120.586, 'score': 0.669}, {'word': 'which', 'start': 120.586, 'end': 120.886, 'score': 0.85}, {'word': \"I'm\", 'start': 120.886, 'end': 121.106, 'score': 0.5}, {'word': 'going', 'start': 121.106, 'end': 121.286, 'score': 0.782}, {'word': 'to', 'start': 121.286, 'end': 121.386, 'score': 0.708}, {'word': 'refer', 'start': 121.386, 'end': 121.766, 'score': 0.821}, {'word': 'to', 'start': 121.766, 'end': 121.966, 'score': 0.838}, {'word': 'as', 'start': 121.966, 'end': 122.167, 'score': 0.771}, {'word': 'base', 'start': 122.167, 'end': 122.567, 'score': 0.9}, {'word': 'LLMs', 'start': 122.567, 'end': 123.047, 'score': 0.716}, {'word': 'and', 'start': 123.047, 'end': 123.727, 'score': 0.811}, {'word': 'instruction-tuned', 'start': 123.727, 'end': 124.648, 'score': 0.802}, {'word': 'LLMs.', 'start': 124.648, 'end': 125.208, 'score': 0.603}]}, {'start': 126.129, 'end': 138.576, 'text': \"So base LLM has been trained to predict the next word based on text training data, often trained on a large amount of data from the internet and other sources to figure out what's the next most likely word to follow.\", 'words': [{'word': 'So', 'start': 126.129, 'end': 126.289, 'score': 0.86}, {'word': 'base', 'start': 126.289, 'end': 126.549, 'score': 0.879}, {'word': 'LLM', 'start': 126.549, 'end': 126.969, 'score': 0.802}, {'word': 'has', 'start': 126.969, 'end': 127.289, 'score': 0.88}, {'word': 'been', 'start': 127.289, 'end': 127.489, 'score': 0.748}, {'word': 'trained', 'start': 127.489, 'end': 127.87, 'score': 0.806}, {'word': 'to', 'start': 127.87, 'end': 128.29, 'score': 0.824}, {'word': 'predict', 'start': 128.29, 'end': 128.73, 'score': 0.842}, {'word': 'the', 'start': 128.73, 'end': 128.87, 'score': 0.758}, {'word': 'next', 'start': 128.87, 'end': 129.15, 'score': 0.836}, {'word': 'word', 'start': 129.15, 'end': 129.471, 'score': 0.788}, {'word': 'based', 'start': 129.471, 'end': 130.171, 'score': 0.75}, {'word': 'on', 'start': 130.171, 'end': 130.371, 'score': 0.849}, {'word': 'text', 'start': 130.371, 'end': 130.791, 'score': 0.904}, {'word': 'training', 'start': 130.791, 'end': 131.211, 'score': 0.852}, {'word': 'data,', 'start': 131.211, 'end': 131.572, 'score': 0.986}, {'word': 'often', 'start': 131.572, 'end': 132.012, 'score': 0.972}, {'word': 'trained', 'start': 132.012, 'end': 132.272, 'score': 0.751}, {'word': 'on', 'start': 132.272, 'end': 132.392, 'score': 0.776}, {'word': 'a', 'start': 132.392, 'end': 132.452, 'score': 0.176}, {'word': 'large', 'start': 132.452, 'end': 132.712, 'score': 0.64}, {'word': 'amount', 'start': 132.712, 'end': 132.992, 'score': 0.856}, {'word': 'of', 'start': 132.992, 'end': 133.073, 'score': 0.963}, {'word': 'data', 'start': 133.073, 'end': 133.473, 'score': 0.879}, {'word': 'from', 'start': 133.473, 'end': 133.933, 'score': 0.938}, {'word': 'the', 'start': 133.933, 'end': 134.053, 'score': 0.754}, {'word': 'internet', 'start': 134.053, 'end': 134.473, 'score': 0.744}, {'word': 'and', 'start': 134.473, 'end': 134.593, 'score': 0.832}, {'word': 'other', 'start': 134.593, 'end': 134.813, 'score': 0.79}, {'word': 'sources', 'start': 134.813, 'end': 135.274, 'score': 0.798}, {'word': 'to', 'start': 135.274, 'end': 135.574, 'score': 0.982}, {'word': 'figure', 'start': 135.574, 'end': 135.834, 'score': 0.903}, {'word': 'out', 'start': 135.834, 'end': 135.954, 'score': 0.334}, {'word': \"what's\", 'start': 135.954, 'end': 136.154, 'score': 0.58}, {'word': 'the', 'start': 136.154, 'end': 136.294, 'score': 0.848}, {'word': 'next', 'start': 136.294, 'end': 136.915, 'score': 0.784}, {'word': 'most', 'start': 136.915, 'end': 137.175, 'score': 0.961}, {'word': 'likely', 'start': 137.175, 'end': 137.575, 'score': 0.915}, {'word': 'word', 'start': 137.575, 'end': 137.835, 'score': 0.746}, {'word': 'to', 'start': 137.835, 'end': 138.215, 'score': 0.975}, {'word': 'follow.', 'start': 138.215, 'end': 138.576, 'score': 0.959}]}, {'start': 139.616, 'end': 150.468, 'text': ' So for example, if you were to prompt this, once upon a time there was a unicorn, it may complete this, that is, it may predict the next several words are that live in a magical forest with all unicorn friends.', 'words': [{'word': 'So', 'start': 139.616, 'end': 139.816, 'score': 0.9}, {'word': 'for', 'start': 139.816, 'end': 139.997, 'score': 0.836}, {'word': 'example,', 'start': 139.997, 'end': 140.417, 'score': 0.784}, {'word': 'if', 'start': 140.417, 'end': 140.637, 'score': 0.962}, {'word': 'you', 'start': 140.637, 'end': 140.777, 'score': 0.757}, {'word': 'were', 'start': 140.777, 'end': 140.918, 'score': 0.877}, {'word': 'to', 'start': 140.918, 'end': 141.018, 'score': 0.803}, {'word': 'prompt', 'start': 141.018, 'end': 141.378, 'score': 0.79}, {'word': 'this,', 'start': 141.378, 'end': 141.578, 'score': 0.626}, {'word': 'once', 'start': 141.578, 'end': 142.119, 'score': 0.806}, {'word': 'upon', 'start': 142.119, 'end': 142.379, 'score': 0.924}, {'word': 'a', 'start': 142.379, 'end': 142.459, 'score': 0.672}, {'word': 'time', 'start': 142.459, 'end': 142.72, 'score': 0.78}, {'word': 'there', 'start': 142.72, 'end': 142.9, 'score': 0.884}, {'word': 'was', 'start': 142.9, 'end': 143.08, 'score': 0.694}, {'word': 'a', 'start': 143.08, 'end': 143.16, 'score': 0.494}, {'word': 'unicorn,', 'start': 143.16, 'end': 143.761, 'score': 0.832}, {'word': 'it', 'start': 143.761, 'end': 144.401, 'score': 0.892}, {'word': 'may', 'start': 144.401, 'end': 144.662, 'score': 0.853}, {'word': 'complete', 'start': 144.662, 'end': 145.302, 'score': 0.933}, {'word': 'this,', 'start': 145.302, 'end': 145.663, 'score': 0.676}, {'word': 'that', 'start': 145.663, 'end': 145.863, 'score': 0.875}, {'word': 'is,', 'start': 145.863, 'end': 146.003, 'score': 0.74}, {'word': 'it', 'start': 146.003, 'end': 146.123, 'score': 0.483}, {'word': 'may', 'start': 146.123, 'end': 146.303, 'score': 0.848}, {'word': 'predict', 'start': 146.303, 'end': 146.684, 'score': 0.947}, {'word': 'the', 'start': 146.684, 'end': 146.804, 'score': 0.803}, {'word': 'next', 'start': 146.804, 'end': 147.044, 'score': 0.676}, {'word': 'several', 'start': 147.044, 'end': 147.465, 'score': 0.869}, {'word': 'words', 'start': 147.465, 'end': 147.765, 'score': 0.547}, {'word': 'are', 'start': 147.765, 'end': 147.965, 'score': 0.659}, {'word': 'that', 'start': 147.965, 'end': 148.486, 'score': 0.898}, {'word': 'live', 'start': 148.486, 'end': 148.666, 'score': 0.819}, {'word': 'in', 'start': 148.666, 'end': 148.726, 'score': 0.033}, {'word': 'a', 'start': 148.726, 'end': 148.806, 'score': 0.286}, {'word': 'magical', 'start': 148.806, 'end': 149.146, 'score': 0.912}, {'word': 'forest', 'start': 149.146, 'end': 149.447, 'score': 0.788}, {'word': 'with', 'start': 149.447, 'end': 149.607, 'score': 0.724}, {'word': 'all', 'start': 149.607, 'end': 149.787, 'score': 0.531}, {'word': 'unicorn', 'start': 149.787, 'end': 150.187, 'score': 0.782}, {'word': 'friends.', 'start': 150.187, 'end': 150.468, 'score': 0.827}]}, {'start': 151.94, 'end': 174.34, 'text': \" But if you were to prompt us with what is the capital of France, then based on what articles on the internet might have, it's quite possible that the base LLM will complete this with what is France's largest city, what is France's population and so on, because articles on the internet could quite plausibly be lists of quiz questions about the country of France.\", 'words': [{'word': 'But', 'start': 151.94, 'end': 152.08, 'score': 0.993}, {'word': 'if', 'start': 152.08, 'end': 152.24, 'score': 0.784}, {'word': 'you', 'start': 152.24, 'end': 152.36, 'score': 0.547}, {'word': 'were', 'start': 152.36, 'end': 152.541, 'score': 0.922}, {'word': 'to', 'start': 152.541, 'end': 152.661, 'score': 0.885}, {'word': 'prompt', 'start': 152.661, 'end': 153.001, 'score': 0.776}, {'word': 'us', 'start': 153.001, 'end': 153.181, 'score': 0.816}, {'word': 'with', 'start': 153.181, 'end': 153.441, 'score': 0.784}, {'word': 'what', 'start': 153.441, 'end': 153.802, 'score': 0.902}, {'word': 'is', 'start': 153.802, 'end': 153.922, 'score': 0.833}, {'word': 'the', 'start': 153.922, 'end': 154.042, 'score': 0.71}, {'word': 'capital', 'start': 154.042, 'end': 154.462, 'score': 0.939}, {'word': 'of', 'start': 154.462, 'end': 154.522, 'score': 0.983}, {'word': 'France,', 'start': 154.522, 'end': 155.003, 'score': 0.852}, {'word': 'then', 'start': 155.003, 'end': 155.763, 'score': 0.84}, {'word': 'based', 'start': 155.763, 'end': 156.544, 'score': 0.811}, {'word': 'on', 'start': 156.544, 'end': 156.744, 'score': 0.825}, {'word': 'what', 'start': 156.744, 'end': 157.085, 'score': 0.87}, {'word': 'articles', 'start': 157.085, 'end': 158.186, 'score': 0.881}, {'word': 'on', 'start': 158.186, 'end': 158.426, 'score': 0.821}, {'word': 'the', 'start': 158.426, 'end': 158.846, 'score': 0.846}, {'word': 'internet', 'start': 158.846, 'end': 159.347, 'score': 0.734}, {'word': 'might', 'start': 159.347, 'end': 159.647, 'score': 0.945}, {'word': 'have,', 'start': 159.647, 'end': 159.967, 'score': 0.781}, {'word': \"it's\", 'start': 159.967, 'end': 160.287, 'score': 0.515}, {'word': 'quite', 'start': 160.287, 'end': 160.688, 'score': 0.742}, {'word': 'possible', 'start': 160.688, 'end': 161.188, 'score': 0.909}, {'word': 'that', 'start': 161.188, 'end': 161.408, 'score': 0.335}, {'word': 'the', 'start': 161.408, 'end': 161.488, 'score': 0.804}, {'word': 'base', 'start': 161.488, 'end': 161.709, 'score': 0.851}, {'word': 'LLM', 'start': 161.709, 'end': 162.129, 'score': 0.672}, {'word': 'will', 'start': 162.129, 'end': 162.97, 'score': 0.813}, {'word': 'complete', 'start': 162.97, 'end': 163.37, 'score': 0.89}, {'word': 'this', 'start': 163.37, 'end': 163.67, 'score': 0.839}, {'word': 'with', 'start': 163.67, 'end': 163.991, 'score': 0.887}, {'word': 'what', 'start': 163.991, 'end': 164.691, 'score': 0.641}, {'word': 'is', 'start': 164.691, 'end': 164.831, 'score': 0.492}, {'word': \"France's\", 'start': 164.831, 'end': 165.192, 'score': 0.664}, {'word': 'largest', 'start': 165.192, 'end': 165.512, 'score': 0.671}, {'word': 'city,', 'start': 165.512, 'end': 165.772, 'score': 0.64}, {'word': 'what', 'start': 165.772, 'end': 165.932, 'score': 0.578}, {'word': 'is', 'start': 165.932, 'end': 166.032, 'score': 0.637}, {'word': \"France's\", 'start': 166.032, 'end': 166.353, 'score': 0.57}, {'word': 'population', 'start': 166.353, 'end': 166.913, 'score': 0.948}, {'word': 'and', 'start': 166.913, 'end': 167.033, 'score': 0.824}, {'word': 'so', 'start': 167.033, 'end': 167.193, 'score': 0.987}, {'word': 'on,', 'start': 167.193, 'end': 167.434, 'score': 0.836}, {'word': 'because', 'start': 167.434, 'end': 168.274, 'score': 0.827}, {'word': 'articles', 'start': 168.274, 'end': 168.775, 'score': 0.865}, {'word': 'on', 'start': 168.775, 'end': 168.895, 'score': 0.84}, {'word': 'the', 'start': 168.895, 'end': 168.995, 'score': 0.868}, {'word': 'internet', 'start': 168.995, 'end': 169.435, 'score': 0.751}, {'word': 'could', 'start': 169.435, 'end': 169.936, 'score': 0.939}, {'word': 'quite', 'start': 169.936, 'end': 170.216, 'score': 0.891}, {'word': 'plausibly', 'start': 170.216, 'end': 170.877, 'score': 0.804}, {'word': 'be', 'start': 170.877, 'end': 171.337, 'score': 0.916}, {'word': 'lists', 'start': 171.337, 'end': 171.617, 'score': 0.669}, {'word': 'of', 'start': 171.617, 'end': 171.778, 'score': 0.786}, {'word': 'quiz', 'start': 171.778, 'end': 172.038, 'score': 0.877}, {'word': 'questions', 'start': 172.038, 'end': 172.518, 'score': 0.875}, {'word': 'about', 'start': 172.518, 'end': 172.878, 'score': 0.929}, {'word': 'the', 'start': 172.878, 'end': 173.459, 'score': 0.902}, {'word': 'country', 'start': 173.459, 'end': 173.959, 'score': 0.976}, {'word': 'of', 'start': 173.959, 'end': 174.06, 'score': 0.674}, {'word': 'France.', 'start': 174.06, 'end': 174.34, 'score': 0.912}]}, {'start': 175.676, 'end': 188.06, 'text': ' In contrast, an instruction-tuned LLM, which is where a lot of momentum of LLM research and practice has been going, an instruction-tuned LLM has been trained to follow instructions.', 'words': [{'word': 'In', 'start': 175.676, 'end': 175.756, 'score': 0.899}, {'word': 'contrast,', 'start': 175.756, 'end': 176.356, 'score': 0.856}, {'word': 'an', 'start': 176.356, 'end': 176.856, 'score': 0.925}, {'word': 'instruction-tuned', 'start': 176.856, 'end': 177.817, 'score': 0.774}, {'word': 'LLM,', 'start': 177.817, 'end': 178.277, 'score': 0.592}, {'word': 'which', 'start': 178.277, 'end': 178.897, 'score': 0.977}, {'word': 'is', 'start': 178.897, 'end': 179.037, 'score': 0.582}, {'word': 'where', 'start': 179.037, 'end': 179.297, 'score': 0.715}, {'word': 'a', 'start': 179.297, 'end': 179.377, 'score': 0.843}, {'word': 'lot', 'start': 179.377, 'end': 179.597, 'score': 0.753}, {'word': 'of', 'start': 179.597, 'end': 179.657, 'score': 0.043}, {'word': 'momentum', 'start': 179.657, 'end': 180.338, 'score': 0.892}, {'word': 'of', 'start': 180.338, 'end': 180.618, 'score': 0.968}, {'word': 'LLM', 'start': 180.618, 'end': 181.198, 'score': 0.619}, {'word': 'research', 'start': 181.198, 'end': 181.758, 'score': 0.949}, {'word': 'and', 'start': 181.758, 'end': 182.198, 'score': 0.77}, {'word': 'practice', 'start': 182.198, 'end': 182.799, 'score': 0.936}, {'word': 'has', 'start': 182.799, 'end': 183.099, 'score': 0.889}, {'word': 'been', 'start': 183.099, 'end': 183.299, 'score': 0.845}, {'word': 'going,', 'start': 183.299, 'end': 183.639, 'score': 0.804}, {'word': 'an', 'start': 183.639, 'end': 184.299, 'score': 0.915}, {'word': 'instruction-tuned', 'start': 184.299, 'end': 185.199, 'score': 0.814}, {'word': 'LLM', 'start': 185.199, 'end': 185.479, 'score': 0.517}, {'word': 'has', 'start': 185.479, 'end': 185.7, 'score': 0.872}, {'word': 'been', 'start': 185.7, 'end': 185.86, 'score': 0.962}, {'word': 'trained', 'start': 185.86, 'end': 186.16, 'score': 0.842}, {'word': 'to', 'start': 186.16, 'end': 186.3, 'score': 0.875}, {'word': 'follow', 'start': 186.3, 'end': 186.68, 'score': 0.921}, {'word': 'instructions.', 'start': 186.68, 'end': 187.38, 'score': 0.851}]}, {'start': 188.06, 'end': 190.661, 'text': 'So if you were to ask it, what is the capital of France?', 'words': [{'word': 'So', 'start': 188.06, 'end': 188.26, 'score': 0.895}, {'word': 'if', 'start': 188.26, 'end': 188.481, 'score': 0.856}, {'word': 'you', 'start': 188.481, 'end': 188.581, 'score': 0.972}, {'word': 'were', 'start': 188.581, 'end': 188.741, 'score': 0.802}, {'word': 'to', 'start': 188.741, 'end': 188.801, 'score': 0.944}, {'word': 'ask', 'start': 188.801, 'end': 189.141, 'score': 0.822}, {'word': 'it,', 'start': 189.141, 'end': 189.261, 'score': 0.428}, {'word': 'what', 'start': 189.261, 'end': 189.461, 'score': 0.463}, {'word': 'is', 'start': 189.461, 'end': 189.561, 'score': 0.634}, {'word': 'the', 'start': 189.561, 'end': 189.661, 'score': 0.512}, {'word': 'capital', 'start': 189.661, 'end': 190.021, 'score': 0.785}, {'word': 'of', 'start': 190.021, 'end': 190.081, 'score': 0.929}, {'word': 'France?', 'start': 190.081, 'end': 190.421, 'score': 0.86}]}, {'start': 190.661, 'end': 194.923, 'text': \"It's much more likely to output something like the capital of France is Paris.\", 'words': [{'word': \"It's\", 'start': 190.661, 'end': 190.761, 'score': 0.494}, {'word': 'much', 'start': 190.761, 'end': 191.141, 'score': 0.756}, {'word': 'more', 'start': 191.141, 'end': 191.321, 'score': 0.658}, {'word': 'likely', 'start': 191.321, 'end': 191.722, 'score': 0.873}, {'word': 'to', 'start': 191.722, 'end': 191.782, 'score': 0.333}, {'word': 'output', 'start': 191.782, 'end': 192.302, 'score': 0.655}, {'word': 'something', 'start': 192.302, 'end': 192.602, 'score': 0.908}, {'word': 'like', 'start': 192.602, 'end': 192.842, 'score': 0.737}, {'word': 'the', 'start': 192.842, 'end': 193.262, 'score': 0.593}, {'word': 'capital', 'start': 193.262, 'end': 193.522, 'score': 0.548}, {'word': 'of', 'start': 193.522, 'end': 193.602, 'score': 0.268}, {'word': 'France', 'start': 193.602, 'end': 193.962, 'score': 0.882}, {'word': 'is', 'start': 193.962, 'end': 194.102, 'score': 0.4}, {'word': 'Paris.', 'start': 194.102, 'end': 194.923, 'score': 0.964}]}, {'start': 195.803, 'end': 212.634, 'text': \" So the way that instruction-tuned OLMs are typically trained is you start off with a base OLM that's been trained on a huge amount of text data and further train it, further fine-tune it with inputs and outputs that are instructions and good attempts to follow those instructions.\", 'words': [{'word': 'So', 'start': 195.803, 'end': 195.943, 'score': 0.879}, {'word': 'the', 'start': 195.943, 'end': 196.083, 'score': 0.816}, {'word': 'way', 'start': 196.083, 'end': 196.223, 'score': 0.911}, {'word': 'that', 'start': 196.223, 'end': 196.383, 'score': 0.89}, {'word': 'instruction-tuned', 'start': 196.383, 'end': 197.304, 'score': 0.785}, {'word': 'OLMs', 'start': 197.304, 'end': 197.864, 'score': 0.756}, {'word': 'are', 'start': 197.864, 'end': 198.105, 'score': 0.821}, {'word': 'typically', 'start': 198.105, 'end': 198.585, 'score': 0.728}, {'word': 'trained', 'start': 198.585, 'end': 199.125, 'score': 0.874}, {'word': 'is', 'start': 199.125, 'end': 199.365, 'score': 0.71}, {'word': 'you', 'start': 199.365, 'end': 199.846, 'score': 0.629}, {'word': 'start', 'start': 199.846, 'end': 200.166, 'score': 0.76}, {'word': 'off', 'start': 200.166, 'end': 200.346, 'score': 0.994}, {'word': 'with', 'start': 200.346, 'end': 200.526, 'score': 0.854}, {'word': 'a', 'start': 200.526, 'end': 200.566, 'score': 0.938}, {'word': 'base', 'start': 200.566, 'end': 200.966, 'score': 0.925}, {'word': 'OLM', 'start': 200.966, 'end': 201.427, 'score': 0.951}, {'word': \"that's\", 'start': 201.427, 'end': 201.627, 'score': 0.31}, {'word': 'been', 'start': 201.627, 'end': 201.747, 'score': 0.063}, {'word': 'trained', 'start': 201.747, 'end': 201.967, 'score': 0.67}, {'word': 'on', 'start': 201.967, 'end': 202.087, 'score': 0.741}, {'word': 'a', 'start': 202.087, 'end': 202.147, 'score': 0.028}, {'word': 'huge', 'start': 202.147, 'end': 202.407, 'score': 0.795}, {'word': 'amount', 'start': 202.407, 'end': 202.688, 'score': 0.872}, {'word': 'of', 'start': 202.688, 'end': 202.788, 'score': 0.814}, {'word': 'text', 'start': 202.788, 'end': 203.268, 'score': 0.794}, {'word': 'data', 'start': 203.268, 'end': 203.728, 'score': 0.684}, {'word': 'and', 'start': 203.728, 'end': 204.329, 'score': 0.85}, {'word': 'further', 'start': 204.329, 'end': 204.909, 'score': 0.808}, {'word': 'train', 'start': 204.909, 'end': 205.529, 'score': 0.828}, {'word': 'it,', 'start': 205.529, 'end': 205.689, 'score': 0.798}, {'word': 'further', 'start': 205.689, 'end': 206.01, 'score': 0.761}, {'word': 'fine-tune', 'start': 206.01, 'end': 206.61, 'score': 0.742}, {'word': 'it', 'start': 206.61, 'end': 206.79, 'score': 0.728}, {'word': 'with', 'start': 206.79, 'end': 207.351, 'score': 0.928}, {'word': 'inputs', 'start': 207.351, 'end': 207.751, 'score': 0.754}, {'word': 'and', 'start': 207.751, 'end': 207.871, 'score': 0.572}, {'word': 'outputs', 'start': 207.871, 'end': 208.311, 'score': 0.571}, {'word': 'that', 'start': 208.311, 'end': 208.531, 'score': 0.656}, {'word': 'are', 'start': 208.531, 'end': 208.631, 'score': 0.315}, {'word': 'instructions', 'start': 208.631, 'end': 209.372, 'score': 0.803}, {'word': 'and', 'start': 209.372, 'end': 209.892, 'score': 0.772}, {'word': 'good', 'start': 209.892, 'end': 210.312, 'score': 0.909}, {'word': 'attempts', 'start': 210.312, 'end': 210.793, 'score': 0.742}, {'word': 'to', 'start': 210.793, 'end': 210.933, 'score': 0.764}, {'word': 'follow', 'start': 210.933, 'end': 211.273, 'score': 0.883}, {'word': 'those', 'start': 211.273, 'end': 211.493, 'score': 0.759}, {'word': 'instructions.', 'start': 211.493, 'end': 212.114, 'score': 0.793}]}, {'start': 212.634, 'end': 223.021, 'text': 'And then often further refine using a technique called RLHF, reinforcement learning from human feedback, to make the system better able to be helpful and follow instructions.', 'words': [{'word': 'And', 'start': 212.634, 'end': 212.714, 'score': 0.905}, {'word': 'then', 'start': 212.714, 'end': 212.814, 'score': 0.236}, {'word': 'often', 'start': 212.814, 'end': 213.174, 'score': 0.944}, {'word': 'further', 'start': 213.174, 'end': 213.535, 'score': 0.788}, {'word': 'refine', 'start': 213.535, 'end': 213.955, 'score': 0.805}, {'word': 'using', 'start': 213.955, 'end': 214.295, 'score': 0.782}, {'word': 'a', 'start': 214.295, 'end': 214.355, 'score': 0.738}, {'word': 'technique', 'start': 214.355, 'end': 214.855, 'score': 0.732}, {'word': 'called', 'start': 214.855, 'end': 215.176, 'score': 0.785}, {'word': 'RLHF,', 'start': 215.176, 'end': 216.076, 'score': 0.787}, {'word': 'reinforcement', 'start': 216.076, 'end': 216.737, 'score': 0.666}, {'word': 'learning', 'start': 216.737, 'end': 217.037, 'score': 0.88}, {'word': 'from', 'start': 217.037, 'end': 217.537, 'score': 0.887}, {'word': 'human', 'start': 217.537, 'end': 217.877, 'score': 0.857}, {'word': 'feedback,', 'start': 217.877, 'end': 218.358, 'score': 0.752}, {'word': 'to', 'start': 218.358, 'end': 218.838, 'score': 0.959}, {'word': 'make', 'start': 218.838, 'end': 219.038, 'score': 0.883}, {'word': 'the', 'start': 219.038, 'end': 219.178, 'score': 0.825}, {'word': 'system', 'start': 219.178, 'end': 219.679, 'score': 0.867}, {'word': 'better', 'start': 219.679, 'end': 220.559, 'score': 0.972}, {'word': 'able', 'start': 220.559, 'end': 220.959, 'score': 0.807}, {'word': 'to', 'start': 220.959, 'end': 221.36, 'score': 0.86}, {'word': 'be', 'start': 221.36, 'end': 221.66, 'score': 0.932}, {'word': 'helpful', 'start': 221.66, 'end': 222.04, 'score': 0.924}, {'word': 'and', 'start': 222.04, 'end': 222.16, 'score': 0.824}, {'word': 'follow', 'start': 222.16, 'end': 222.44, 'score': 0.83}, {'word': 'instructions.', 'start': 222.44, 'end': 223.021, 'score': 0.886}]}, {'start': 223.621, 'end': 240.494, 'text': \" Because instruction-tuned LLMs have been trained to be helpful, honest, and harmless, so, for example, they're less likely to output problematic text, such as toxic outputs, compared to base LLM, a lot of the practical usage scenarios have been shifting toward instruction-tuned LLMs.\", 'words': [{'word': 'Because', 'start': 223.621, 'end': 223.841, 'score': 0.93}, {'word': 'instruction-tuned', 'start': 223.841, 'end': 224.722, 'score': 0.714}, {'word': 'LLMs', 'start': 224.722, 'end': 224.982, 'score': 0.61}, {'word': 'have', 'start': 224.982, 'end': 225.342, 'score': 0.82}, {'word': 'been', 'start': 225.342, 'end': 225.543, 'score': 0.891}, {'word': 'trained', 'start': 225.543, 'end': 225.963, 'score': 0.78}, {'word': 'to', 'start': 225.963, 'end': 226.463, 'score': 0.898}, {'word': 'be', 'start': 226.463, 'end': 226.643, 'score': 0.876}, {'word': 'helpful,', 'start': 226.643, 'end': 227.244, 'score': 0.621}, {'word': 'honest,', 'start': 227.244, 'end': 227.664, 'score': 0.693}, {'word': 'and', 'start': 227.664, 'end': 227.824, 'score': 0.694}, {'word': 'harmless,', 'start': 227.824, 'end': 228.505, 'score': 0.844}, {'word': 'so,', 'start': 228.505, 'end': 229.246, 'score': 0.985}, {'word': 'for', 'start': 229.246, 'end': 229.386, 'score': 0.865}, {'word': 'example,', 'start': 229.386, 'end': 229.786, 'score': 0.886}, {'word': \"they're\", 'start': 229.786, 'end': 230.126, 'score': 0.51}, {'word': 'less', 'start': 230.126, 'end': 230.487, 'score': 0.86}, {'word': 'likely', 'start': 230.487, 'end': 230.847, 'score': 0.876}, {'word': 'to', 'start': 230.847, 'end': 230.967, 'score': 0.725}, {'word': 'output', 'start': 230.967, 'end': 231.367, 'score': 0.669}, {'word': 'problematic', 'start': 231.367, 'end': 232.208, 'score': 0.939}, {'word': 'text,', 'start': 232.208, 'end': 232.748, 'score': 0.81}, {'word': 'such', 'start': 232.748, 'end': 232.928, 'score': 0.895}, {'word': 'as', 'start': 232.928, 'end': 233.029, 'score': 0.908}, {'word': 'toxic', 'start': 233.029, 'end': 233.409, 'score': 0.893}, {'word': 'outputs,', 'start': 233.409, 'end': 233.809, 'score': 0.65}, {'word': 'compared', 'start': 233.809, 'end': 234.41, 'score': 0.958}, {'word': 'to', 'start': 234.41, 'end': 234.53, 'score': 0.757}, {'word': 'base', 'start': 234.53, 'end': 234.75, 'score': 0.694}, {'word': 'LLM,', 'start': 234.75, 'end': 235.21, 'score': 0.778}, {'word': 'a', 'start': 235.21, 'end': 236.011, 'score': 0.76}, {'word': 'lot', 'start': 236.011, 'end': 236.271, 'score': 0.765}, {'word': 'of', 'start': 236.271, 'end': 236.391, 'score': 0.833}, {'word': 'the', 'start': 236.391, 'end': 236.631, 'score': 0.897}, {'word': 'practical', 'start': 236.631, 'end': 237.332, 'score': 0.905}, {'word': 'usage', 'start': 237.332, 'end': 237.872, 'score': 0.828}, {'word': 'scenarios', 'start': 237.872, 'end': 238.413, 'score': 0.809}, {'word': 'have', 'start': 238.413, 'end': 238.553, 'score': 0.822}, {'word': 'been', 'start': 238.553, 'end': 238.693, 'score': 0.999}, {'word': 'shifting', 'start': 238.693, 'end': 239.053, 'score': 0.834}, {'word': 'toward', 'start': 239.053, 'end': 239.394, 'score': 0.772}, {'word': 'instruction-tuned', 'start': 239.394, 'end': 240.234, 'score': 0.791}, {'word': 'LLMs.', 'start': 240.234, 'end': 240.494, 'score': 0.672}]}, {'start': 241.275, 'end': 262.839, 'text': ' Some of the best practices you find on the internet may be more suited for a base LLM, but for most practical applications today, we would recommend most people instead focus on instruction-tuned LLMs, which are easier to use, and also because of the work of OpenAI and other LLM companies becoming safer and more aligned.', 'words': [{'word': 'Some', 'start': 241.275, 'end': 241.415, 'score': 0.874}, {'word': 'of', 'start': 241.415, 'end': 241.495, 'score': 0.827}, {'word': 'the', 'start': 241.495, 'end': 241.575, 'score': 0.998}, {'word': 'best', 'start': 241.575, 'end': 241.775, 'score': 0.812}, {'word': 'practices', 'start': 241.775, 'end': 242.315, 'score': 0.802}, {'word': 'you', 'start': 242.315, 'end': 242.475, 'score': 0.84}, {'word': 'find', 'start': 242.475, 'end': 242.695, 'score': 0.787}, {'word': 'on', 'start': 242.695, 'end': 242.835, 'score': 0.686}, {'word': 'the', 'start': 242.835, 'end': 242.935, 'score': 0.926}, {'word': 'internet', 'start': 242.935, 'end': 243.455, 'score': 0.712}, {'word': 'may', 'start': 243.455, 'end': 243.976, 'score': 0.982}, {'word': 'be', 'start': 243.976, 'end': 244.096, 'score': 0.744}, {'word': 'more', 'start': 244.096, 'end': 244.296, 'score': 0.842}, {'word': 'suited', 'start': 244.296, 'end': 244.736, 'score': 0.904}, {'word': 'for', 'start': 244.736, 'end': 244.996, 'score': 0.707}, {'word': 'a', 'start': 244.996, 'end': 245.036, 'score': 0.971}, {'word': 'base', 'start': 245.036, 'end': 245.456, 'score': 0.944}, {'word': 'LLM,', 'start': 245.456, 'end': 245.976, 'score': 0.772}, {'word': 'but', 'start': 245.976, 'end': 246.536, 'score': 0.984}, {'word': 'for', 'start': 246.536, 'end': 246.756, 'score': 0.754}, {'word': 'most', 'start': 246.756, 'end': 247.016, 'score': 0.718}, {'word': 'practical', 'start': 247.016, 'end': 247.536, 'score': 0.884}, {'word': 'applications', 'start': 247.536, 'end': 248.096, 'score': 0.889}, {'word': 'today,', 'start': 248.096, 'end': 248.436, 'score': 0.799}, {'word': 'we', 'start': 248.436, 'end': 248.496, 'score': 0.333}, {'word': 'would', 'start': 248.496, 'end': 248.896, 'score': 0.901}, {'word': 'recommend', 'start': 248.896, 'end': 249.457, 'score': 0.827}, {'word': 'most', 'start': 249.457, 'end': 249.877, 'score': 0.847}, {'word': 'people', 'start': 249.877, 'end': 250.257, 'score': 0.853}, {'word': 'instead', 'start': 250.257, 'end': 250.777, 'score': 0.812}, {'word': 'focus', 'start': 250.777, 'end': 251.317, 'score': 0.887}, {'word': 'on', 'start': 251.317, 'end': 251.497, 'score': 0.817}, {'word': 'instruction-tuned', 'start': 251.497, 'end': 252.997, 'score': 0.801}, {'word': 'LLMs,', 'start': 252.997, 'end': 253.557, 'score': 0.739}, {'word': 'which', 'start': 253.557, 'end': 253.877, 'score': 0.94}, {'word': 'are', 'start': 253.877, 'end': 254.077, 'score': 0.786}, {'word': 'easier', 'start': 254.077, 'end': 254.658, 'score': 0.88}, {'word': 'to', 'start': 254.658, 'end': 254.778, 'score': 0.88}, {'word': 'use,', 'start': 254.778, 'end': 255.078, 'score': 0.794}, {'word': 'and', 'start': 255.078, 'end': 255.378, 'score': 0.741}, {'word': 'also', 'start': 255.378, 'end': 255.818, 'score': 0.826}, {'word': 'because', 'start': 255.818, 'end': 256.298, 'score': 0.941}, {'word': 'of', 'start': 256.298, 'end': 256.358, 'score': 0.336}, {'word': 'the', 'start': 256.358, 'end': 256.478, 'score': 0.898}, {'word': 'work', 'start': 256.478, 'end': 256.778, 'score': 0.817}, {'word': 'of', 'start': 256.778, 'end': 257.078, 'score': 0.828}, {'word': 'OpenAI', 'start': 257.078, 'end': 257.758, 'score': 0.736}, {'word': 'and', 'start': 257.758, 'end': 257.938, 'score': 0.898}, {'word': 'other', 'start': 257.938, 'end': 258.258, 'score': 0.879}, {'word': 'LLM', 'start': 258.258, 'end': 258.698, 'score': 0.729}, {'word': 'companies', 'start': 258.698, 'end': 259.438, 'score': 0.919}, {'word': 'becoming', 'start': 259.438, 'end': 260.279, 'score': 0.935}, {'word': 'safer', 'start': 260.279, 'end': 260.839, 'score': 0.885}, {'word': 'and', 'start': 260.839, 'end': 261.019, 'score': 0.83}, {'word': 'more', 'start': 261.019, 'end': 261.279, 'score': 0.878}, {'word': 'aligned.', 'start': 261.279, 'end': 261.699, 'score': 0.772}]}, {'start': 262.839, 'end': 267.28, 'text': 'So this course will focus on best practices for instruction-tuned LLMs.', 'words': [{'word': 'So', 'start': 262.839, 'end': 263.019, 'score': 0.796}, {'word': 'this', 'start': 263.019, 'end': 263.259, 'score': 0.789}, {'word': 'course', 'start': 263.259, 'end': 263.659, 'score': 0.927}, {'word': 'will', 'start': 263.659, 'end': 263.939, 'score': 0.947}, {'word': 'focus', 'start': 263.939, 'end': 264.399, 'score': 0.91}, {'word': 'on', 'start': 264.399, 'end': 264.759, 'score': 0.85}, {'word': 'best', 'start': 264.759, 'end': 265.119, 'score': 0.872}, {'word': 'practices', 'start': 265.119, 'end': 265.72, 'score': 0.865}, {'word': 'for', 'start': 265.72, 'end': 266.04, 'score': 0.886}, {'word': 'instruction-tuned', 'start': 266.04, 'end': 267.02, 'score': 0.765}, {'word': 'LLMs.', 'start': 267.02, 'end': 267.28, 'score': 0.57}]}, {'start': 268.263, 'end': 271.601, 'text': ' which is what we recommend you use for most of your applications.', 'words': [{'word': 'which', 'start': 268.263, 'end': 268.404, 'score': 0.899}, {'word': 'is', 'start': 268.404, 'end': 268.544, 'score': 0.663}, {'word': 'what', 'start': 268.544, 'end': 268.725, 'score': 0.934}, {'word': 'we', 'start': 268.725, 'end': 268.906, 'score': 0.906}, {'word': 'recommend', 'start': 268.906, 'end': 269.489, 'score': 0.88}, {'word': 'you', 'start': 269.489, 'end': 270.093, 'score': 0.922}, {'word': 'use', 'start': 270.093, 'end': 270.314, 'score': 0.861}, {'word': 'for', 'start': 270.314, 'end': 270.515, 'score': 0.776}, {'word': 'most', 'start': 270.515, 'end': 270.796, 'score': 0.798}, {'word': 'of', 'start': 270.796, 'end': 270.877, 'score': 0.946}, {'word': 'your', 'start': 270.877, 'end': 270.997, 'score': 0.656}, {'word': 'applications.', 'start': 270.997, 'end': 271.601, 'score': 0.76}]}, {'start': 272.431, 'end': 281.574, 'text': ' Before moving on, I just want to acknowledge the team from OpenAI and DeepLearning.AI that had contributed to the materials that Yizi and I will be presenting.', 'words': [{'word': 'Before', 'start': 272.431, 'end': 272.651, 'score': 0.904}, {'word': 'moving', 'start': 272.651, 'end': 272.931, 'score': 0.835}, {'word': 'on,', 'start': 272.931, 'end': 273.151, 'score': 0.79}, {'word': 'I', 'start': 273.151, 'end': 273.311, 'score': 0.796}, {'word': 'just', 'start': 273.311, 'end': 273.471, 'score': 0.507}, {'word': 'want', 'start': 273.471, 'end': 273.591, 'score': 0.353}, {'word': 'to', 'start': 273.591, 'end': 273.651, 'score': 0.041}, {'word': 'acknowledge', 'start': 273.651, 'end': 274.332, 'score': 0.812}, {'word': 'the', 'start': 274.332, 'end': 274.692, 'score': 0.756}, {'word': 'team', 'start': 274.692, 'end': 275.312, 'score': 0.884}, {'word': 'from', 'start': 275.312, 'end': 275.612, 'score': 0.882}, {'word': 'OpenAI', 'start': 275.612, 'end': 276.252, 'score': 0.719}, {'word': 'and', 'start': 276.252, 'end': 276.412, 'score': 0.894}, {'word': 'DeepLearning.AI', 'start': 276.412, 'end': 277.373, 'score': 0.602}, {'word': 'that', 'start': 277.373, 'end': 277.573, 'score': 0.868}, {'word': 'had', 'start': 277.573, 'end': 277.713, 'score': 0.711}, {'word': 'contributed', 'start': 277.713, 'end': 278.353, 'score': 0.904}, {'word': 'to', 'start': 278.353, 'end': 278.593, 'score': 0.888}, {'word': 'the', 'start': 278.593, 'end': 278.693, 'score': 0.855}, {'word': 'materials', 'start': 278.693, 'end': 279.273, 'score': 0.752}, {'word': 'that', 'start': 279.273, 'end': 279.493, 'score': 0.852}, {'word': 'Yizi', 'start': 279.493, 'end': 280.074, 'score': 0.698}, {'word': 'and', 'start': 280.074, 'end': 280.214, 'score': 0.775}, {'word': 'I', 'start': 280.214, 'end': 280.374, 'score': 0.899}, {'word': 'will', 'start': 280.374, 'end': 280.814, 'score': 0.558}, {'word': 'be', 'start': 280.814, 'end': 280.954, 'score': 0.802}, {'word': 'presenting.', 'start': 280.954, 'end': 281.554, 'score': 0.913}]}, {'start': 281.574, 'end': 295.539, 'text': \"I'm very grateful to Andrew Main, Joe Palermo, Boris Power, Ted Sanders, and Lillian Wang from OpenAI that were very involved with us brainstorming materials, vetting the materials to put together the curriculum for this short course.\", 'words': [{'word': \"I'm\", 'start': 281.574, 'end': 281.874, 'score': 0.507}, {'word': 'very', 'start': 281.874, 'end': 282.114, 'score': 0.785}, {'word': 'grateful', 'start': 282.114, 'end': 282.535, 'score': 0.914}, {'word': 'to', 'start': 282.535, 'end': 282.755, 'score': 0.775}, {'word': 'Andrew', 'start': 282.755, 'end': 283.455, 'score': 0.88}, {'word': 'Main,', 'start': 283.455, 'end': 283.775, 'score': 0.766}, {'word': 'Joe', 'start': 283.775, 'end': 284.015, 'score': 0.739}, {'word': 'Palermo,', 'start': 284.015, 'end': 284.535, 'score': 0.63}, {'word': 'Boris', 'start': 284.535, 'end': 284.835, 'score': 0.584}, {'word': 'Power,', 'start': 284.835, 'end': 285.295, 'score': 0.902}, {'word': 'Ted', 'start': 285.295, 'end': 285.736, 'score': 0.811}, {'word': 'Sanders,', 'start': 285.736, 'end': 286.276, 'score': 0.71}, {'word': 'and', 'start': 286.276, 'end': 286.856, 'score': 0.876}, {'word': 'Lillian', 'start': 286.856, 'end': 287.216, 'score': 0.654}, {'word': 'Wang', 'start': 287.216, 'end': 287.496, 'score': 0.599}, {'word': 'from', 'start': 287.496, 'end': 287.996, 'score': 0.7}, {'word': 'OpenAI', 'start': 287.996, 'end': 288.657, 'score': 0.677}, {'word': 'that', 'start': 288.657, 'end': 288.877, 'score': 0.632}, {'word': 'were', 'start': 288.877, 'end': 289.057, 'score': 0.857}, {'word': 'very', 'start': 289.057, 'end': 289.337, 'score': 0.827}, {'word': 'involved', 'start': 289.337, 'end': 289.737, 'score': 0.881}, {'word': 'with', 'start': 289.737, 'end': 289.977, 'score': 0.93}, {'word': 'us', 'start': 289.977, 'end': 290.157, 'score': 0.742}, {'word': 'brainstorming', 'start': 290.157, 'end': 290.857, 'score': 0.751}, {'word': 'materials,', 'start': 290.857, 'end': 291.298, 'score': 0.793}, {'word': 'vetting', 'start': 291.298, 'end': 291.618, 'score': 0.876}, {'word': 'the', 'start': 291.618, 'end': 291.718, 'score': 0.692}, {'word': 'materials', 'start': 291.718, 'end': 292.158, 'score': 0.742}, {'word': 'to', 'start': 292.158, 'end': 292.258, 'score': 0.737}, {'word': 'put', 'start': 292.258, 'end': 292.418, 'score': 0.89}, {'word': 'together', 'start': 292.418, 'end': 292.838, 'score': 0.754}, {'word': 'the', 'start': 292.838, 'end': 293.578, 'score': 0.874}, {'word': 'curriculum', 'start': 293.578, 'end': 294.239, 'score': 0.683}, {'word': 'for', 'start': 294.239, 'end': 294.379, 'score': 0.905}, {'word': 'this', 'start': 294.379, 'end': 294.539, 'score': 0.837}, {'word': 'short', 'start': 294.539, 'end': 294.779, 'score': 0.947}, {'word': 'course.', 'start': 294.779, 'end': 295.119, 'score': 0.898}]}, {'start': 295.539, 'end': 300.801, 'text': \"And I'm also grateful on the deep learning side for the work of Jeff Ludwig, Eddie Hsu, and Tommy Nelson.\", 'words': [{'word': 'And', 'start': 295.539, 'end': 295.639, 'score': 0.706}, {'word': \"I'm\", 'start': 295.639, 'end': 295.919, 'score': 0.688}, {'word': 'also', 'start': 295.919, 'end': 296.139, 'score': 0.472}, {'word': 'grateful', 'start': 296.139, 'end': 296.659, 'score': 0.623}, {'word': 'on', 'start': 296.659, 'end': 296.839, 'score': 0.84}, {'word': 'the', 'start': 296.839, 'end': 296.94, 'score': 0.884}, {'word': 'deep', 'start': 296.94, 'end': 297.14, 'score': 0.803}, {'word': 'learning', 'start': 297.14, 'end': 297.44, 'score': 0.908}, {'word': 'side', 'start': 297.44, 'end': 297.78, 'score': 0.908}, {'word': 'for', 'start': 297.78, 'end': 298.08, 'score': 0.912}, {'word': 'the', 'start': 298.08, 'end': 298.18, 'score': 0.866}, {'word': 'work', 'start': 298.18, 'end': 298.44, 'score': 0.804}, {'word': 'of', 'start': 298.44, 'end': 298.58, 'score': 0.743}, {'word': 'Jeff', 'start': 298.58, 'end': 298.82, 'score': 0.611}, {'word': 'Ludwig,', 'start': 298.82, 'end': 299.3, 'score': 0.67}, {'word': 'Eddie', 'start': 299.3, 'end': 299.58, 'score': 0.732}, {'word': 'Hsu,', 'start': 299.58, 'end': 299.861, 'score': 0.274}, {'word': 'and', 'start': 299.861, 'end': 300.041, 'score': 0.888}, {'word': 'Tommy', 'start': 300.041, 'end': 300.381, 'score': 0.924}, {'word': 'Nelson.', 'start': 300.381, 'end': 300.801, 'score': 0.894}]}, {'start': 301.501, 'end': 312.546, 'text': \" So when you use an instruction-tuned LLM, think of giving instructions to another person, say someone that's smart but doesn't know the specifics of your task.\", 'words': [{'word': 'So', 'start': 301.501, 'end': 301.701, 'score': 0.903}, {'word': 'when', 'start': 301.701, 'end': 301.981, 'score': 0.907}, {'word': 'you', 'start': 301.981, 'end': 302.101, 'score': 0.851}, {'word': 'use', 'start': 302.101, 'end': 302.421, 'score': 0.768}, {'word': 'an', 'start': 302.421, 'end': 302.502, 'score': 0.567}, {'word': 'instruction-tuned', 'start': 302.502, 'end': 303.782, 'score': 0.734}, {'word': 'LLM,', 'start': 303.782, 'end': 304.262, 'score': 0.718}, {'word': 'think', 'start': 304.262, 'end': 305.343, 'score': 0.88}, {'word': 'of', 'start': 305.343, 'end': 305.483, 'score': 0.94}, {'word': 'giving', 'start': 305.483, 'end': 305.903, 'score': 0.846}, {'word': 'instructions', 'start': 305.903, 'end': 306.563, 'score': 0.802}, {'word': 'to', 'start': 306.563, 'end': 306.723, 'score': 0.89}, {'word': 'another', 'start': 306.723, 'end': 307.063, 'score': 0.837}, {'word': 'person,', 'start': 307.063, 'end': 307.644, 'score': 0.835}, {'word': 'say', 'start': 307.644, 'end': 308.444, 'score': 0.886}, {'word': 'someone', 'start': 308.444, 'end': 308.884, 'score': 0.802}, {'word': \"that's\", 'start': 308.884, 'end': 309.124, 'score': 0.383}, {'word': 'smart', 'start': 309.124, 'end': 309.544, 'score': 0.913}, {'word': 'but', 'start': 309.544, 'end': 309.885, 'score': 0.812}, {'word': \"doesn't\", 'start': 309.885, 'end': 310.225, 'score': 0.687}, {'word': 'know', 'start': 310.225, 'end': 310.405, 'score': 0.933}, {'word': 'the', 'start': 310.405, 'end': 310.545, 'score': 0.814}, {'word': 'specifics', 'start': 310.545, 'end': 311.165, 'score': 0.898}, {'word': 'of', 'start': 311.165, 'end': 311.305, 'score': 0.78}, {'word': 'your', 'start': 311.305, 'end': 311.625, 'score': 0.748}, {'word': 'task.', 'start': 311.625, 'end': 311.985, 'score': 0.908}]}, {'start': 312.546, 'end': 317.708, 'text': \"So when an LLM doesn't work, sometimes it's because the instructions weren't clear enough.\", 'words': [{'word': 'So', 'start': 312.546, 'end': 312.746, 'score': 0.992}, {'word': 'when', 'start': 312.746, 'end': 313.046, 'score': 0.821}, {'word': 'an', 'start': 313.046, 'end': 313.126, 'score': 0.77}, {'word': 'LLM', 'start': 313.126, 'end': 313.626, 'score': 0.79}, {'word': \"doesn't\", 'start': 313.626, 'end': 314.186, 'score': 0.879}, {'word': 'work,', 'start': 314.186, 'end': 314.486, 'score': 0.845}, {'word': 'sometimes', 'start': 314.486, 'end': 315.427, 'score': 0.848}, {'word': \"it's\", 'start': 315.427, 'end': 315.567, 'score': 0.534}, {'word': 'because', 'start': 315.567, 'end': 315.867, 'score': 0.903}, {'word': 'the', 'start': 315.867, 'end': 316.007, 'score': 0.86}, {'word': 'instructions', 'start': 316.007, 'end': 316.627, 'score': 0.862}, {'word': \"weren't\", 'start': 316.627, 'end': 316.947, 'score': 0.781}, {'word': 'clear', 'start': 316.947, 'end': 317.248, 'score': 0.796}, {'word': 'enough.', 'start': 317.248, 'end': 317.528, 'score': 0.875}]}, {'start': 317.708, 'end': 322.67, 'text': 'For example, if you were to say, please write me something about Alan Turing.', 'words': [{'word': 'For', 'start': 317.708, 'end': 317.808, 'score': 0.78}, {'word': 'example,', 'start': 317.808, 'end': 318.228, 'score': 0.812}, {'word': 'if', 'start': 318.228, 'end': 318.888, 'score': 0.989}, {'word': 'you', 'start': 318.888, 'end': 319.028, 'score': 0.784}, {'word': 'were', 'start': 319.028, 'end': 319.208, 'score': 0.928}, {'word': 'to', 'start': 319.208, 'end': 319.328, 'score': 0.538}, {'word': 'say,', 'start': 319.328, 'end': 319.669, 'score': 0.761}, {'word': 'please', 'start': 319.669, 'end': 320.149, 'score': 0.853}, {'word': 'write', 'start': 320.149, 'end': 320.389, 'score': 0.8}, {'word': 'me', 'start': 320.389, 'end': 320.509, 'score': 0.891}, {'word': 'something', 'start': 320.509, 'end': 320.869, 'score': 0.831}, {'word': 'about', 'start': 320.869, 'end': 321.169, 'score': 0.862}, {'word': 'Alan', 'start': 321.169, 'end': 321.569, 'score': 0.828}, {'word': 'Turing.', 'start': 321.569, 'end': 322.07, 'score': 0.854}]}, {'start': 322.67, 'end': 327.452, 'text': 'Well, in addition to that, it can be helpful to be clear about whether you want', 'words': [{'word': 'Well,', 'start': 322.67, 'end': 322.87, 'score': 0.971}, {'word': 'in', 'start': 322.87, 'end': 323.39, 'score': 0.972}, {'word': 'addition', 'start': 323.39, 'end': 323.77, 'score': 0.821}, {'word': 'to', 'start': 323.77, 'end': 323.89, 'score': 0.97}, {'word': 'that,', 'start': 323.89, 'end': 324.13, 'score': 0.84}, {'word': 'it', 'start': 324.13, 'end': 324.39, 'score': 0.945}, {'word': 'can', 'start': 324.39, 'end': 324.651, 'score': 0.888}, {'word': 'be', 'start': 324.651, 'end': 324.751, 'score': 0.947}, {'word': 'helpful', 'start': 324.751, 'end': 325.191, 'score': 0.808}, {'word': 'to', 'start': 325.191, 'end': 325.411, 'score': 0.746}, {'word': 'be', 'start': 325.411, 'end': 325.551, 'score': 0.918}, {'word': 'clear', 'start': 325.551, 'end': 326.171, 'score': 0.75}, {'word': 'about', 'start': 326.171, 'end': 326.831, 'score': 0.978}, {'word': 'whether', 'start': 326.831, 'end': 327.112, 'score': 0.86}, {'word': 'you', 'start': 327.112, 'end': 327.252, 'score': 0.915}, {'word': 'want', 'start': 327.252, 'end': 327.452, 'score': 0.861}]}, {'start': 328.192, 'end': 343.014, 'text': ' the text to focus on his scientific work or his personal life or his role in history or something else and if you specify what you want the tone of the text to be should it take on the tone like a professional journalist would write', 'words': [{'word': 'the', 'start': 328.192, 'end': 328.352, 'score': 0.897}, {'word': 'text', 'start': 328.352, 'end': 328.933, 'score': 0.924}, {'word': 'to', 'start': 328.933, 'end': 329.073, 'score': 0.758}, {'word': 'focus', 'start': 329.073, 'end': 329.494, 'score': 0.917}, {'word': 'on', 'start': 329.494, 'end': 329.674, 'score': 0.966}, {'word': 'his', 'start': 329.674, 'end': 329.875, 'score': 0.683}, {'word': 'scientific', 'start': 329.875, 'end': 330.395, 'score': 0.82}, {'word': 'work', 'start': 330.395, 'end': 330.656, 'score': 0.929}, {'word': 'or', 'start': 330.656, 'end': 330.916, 'score': 0.799}, {'word': 'his', 'start': 330.916, 'end': 331.076, 'score': 0.742}, {'word': 'personal', 'start': 331.076, 'end': 331.497, 'score': 0.839}, {'word': 'life', 'start': 331.497, 'end': 331.737, 'score': 0.89}, {'word': 'or', 'start': 331.737, 'end': 331.938, 'score': 0.941}, {'word': 'his', 'start': 331.938, 'end': 332.098, 'score': 0.635}, {'word': 'role', 'start': 332.098, 'end': 332.398, 'score': 0.68}, {'word': 'in', 'start': 332.398, 'end': 332.839, 'score': 0.879}, {'word': 'history', 'start': 332.839, 'end': 333.32, 'score': 0.818}, {'word': 'or', 'start': 333.32, 'end': 333.74, 'score': 0.666}, {'word': 'something', 'start': 333.74, 'end': 334.041, 'score': 0.858}, {'word': 'else', 'start': 334.041, 'end': 334.301, 'score': 0.771}, {'word': 'and', 'start': 334.301, 'end': 335.202, 'score': 0.738}, {'word': 'if', 'start': 335.202, 'end': 335.503, 'score': 0.855}, {'word': 'you', 'start': 335.503, 'end': 335.723, 'score': 0.8}, {'word': 'specify', 'start': 335.723, 'end': 336.464, 'score': 0.91}, {'word': 'what', 'start': 336.464, 'end': 336.985, 'score': 0.901}, {'word': 'you', 'start': 336.985, 'end': 337.205, 'score': 0.806}, {'word': 'want', 'start': 337.205, 'end': 337.486, 'score': 0.712}, {'word': 'the', 'start': 337.486, 'end': 337.726, 'score': 0.825}, {'word': 'tone', 'start': 337.726, 'end': 338.327, 'score': 0.866}, {'word': 'of', 'start': 338.327, 'end': 338.587, 'score': 0.876}, {'word': 'the', 'start': 338.587, 'end': 338.708, 'score': 0.862}, {'word': 'text', 'start': 338.708, 'end': 339.008, 'score': 0.829}, {'word': 'to', 'start': 339.008, 'end': 339.148, 'score': 0.91}, {'word': 'be', 'start': 339.148, 'end': 339.328, 'score': 0.88}, {'word': 'should', 'start': 339.328, 'end': 339.949, 'score': 0.953}, {'word': 'it', 'start': 339.949, 'end': 340.03, 'score': 0.832}, {'word': 'take', 'start': 340.03, 'end': 340.23, 'score': 0.716}, {'word': 'on', 'start': 340.23, 'end': 340.31, 'score': 0.688}, {'word': 'the', 'start': 340.31, 'end': 340.43, 'score': 0.332}, {'word': 'tone', 'start': 340.43, 'end': 340.851, 'score': 0.886}, {'word': 'like', 'start': 340.851, 'end': 341.131, 'score': 0.754}, {'word': 'a', 'start': 341.131, 'end': 341.271, 'score': 0.667}, {'word': 'professional', 'start': 341.271, 'end': 341.932, 'score': 0.826}, {'word': 'journalist', 'start': 341.932, 'end': 342.613, 'score': 0.762}, {'word': 'would', 'start': 342.613, 'end': 342.794, 'score': 0.924}, {'word': 'write', 'start': 342.794, 'end': 343.014, 'score': 0.894}]}, {'start': 343.134, 'end': 346.176, 'text': ' Or is it more of a casual note that you dash off to a friend?', 'words': [{'word': 'Or', 'start': 343.134, 'end': 343.194, 'score': 0.224}, {'word': 'is', 'start': 343.194, 'end': 343.314, 'score': 0.486}, {'word': 'it', 'start': 343.314, 'end': 343.454, 'score': 0.87}, {'word': 'more', 'start': 343.454, 'end': 343.614, 'score': 0.894}, {'word': 'of', 'start': 343.614, 'end': 343.694, 'score': 0.772}, {'word': 'a', 'start': 343.694, 'end': 343.794, 'score': 0.256}, {'word': 'casual', 'start': 343.794, 'end': 344.235, 'score': 0.943}, {'word': 'note', 'start': 344.235, 'end': 344.455, 'score': 0.819}, {'word': 'that', 'start': 344.455, 'end': 344.855, 'score': 0.911}, {'word': 'you', 'start': 344.855, 'end': 345.015, 'score': 0.977}, {'word': 'dash', 'start': 345.015, 'end': 345.315, 'score': 0.798}, {'word': 'off', 'start': 345.315, 'end': 345.515, 'score': 0.997}, {'word': 'to', 'start': 345.515, 'end': 345.695, 'score': 0.74}, {'word': 'a', 'start': 345.695, 'end': 345.735, 'score': 0.83}, {'word': 'friend?', 'start': 345.735, 'end': 346.035, 'score': 0.738}]}, {'start': 346.176, 'end': 347.056, 'text': 'That holds.', 'words': [{'word': 'That', 'start': 346.176, 'end': 346.356, 'score': 0.889}, {'word': 'holds.', 'start': 346.356, 'end': 346.736, 'score': 0.747}]}, {'start': 347.056, 'end': 349.297, 'text': 'The OM generate what you want.', 'words': [{'word': 'The', 'start': 347.056, 'end': 347.196, 'score': 0.728}, {'word': 'OM', 'start': 347.196, 'end': 347.576, 'score': 0.93}, {'word': 'generate', 'start': 347.576, 'end': 347.976, 'score': 0.884}, {'word': 'what', 'start': 347.976, 'end': 348.137, 'score': 0.863}, {'word': 'you', 'start': 348.137, 'end': 348.357, 'score': 0.836}, {'word': 'want.', 'start': 348.357, 'end': 348.637, 'score': 0.759}]}, {'start': 349.297, 'end': 368.247, 'text': 'And of course, if you picture yourself asking, say, a fresh college graduate to carry out this task for you, if you can even specify what snippets of text they should read in advance to write this text about Alan Turing, then that even better sets up that fresh college grad for success to carry out this task for you.', 'words': [{'word': 'And', 'start': 349.297, 'end': 349.437, 'score': 0.656}, {'word': 'of', 'start': 349.437, 'end': 349.917, 'score': 0.935}, {'word': 'course,', 'start': 349.917, 'end': 350.258, 'score': 0.805}, {'word': 'if', 'start': 350.258, 'end': 350.578, 'score': 0.961}, {'word': 'you', 'start': 350.578, 'end': 350.958, 'score': 0.943}, {'word': 'picture', 'start': 350.958, 'end': 351.218, 'score': 0.918}, {'word': 'yourself', 'start': 351.218, 'end': 351.598, 'score': 0.884}, {'word': 'asking,', 'start': 351.598, 'end': 352.299, 'score': 0.809}, {'word': 'say,', 'start': 352.299, 'end': 352.559, 'score': 0.684}, {'word': 'a', 'start': 352.559, 'end': 352.639, 'score': 0.493}, {'word': 'fresh', 'start': 352.639, 'end': 352.899, 'score': 0.87}, {'word': 'college', 'start': 352.899, 'end': 353.239, 'score': 0.876}, {'word': 'graduate', 'start': 353.239, 'end': 353.719, 'score': 0.717}, {'word': 'to', 'start': 353.719, 'end': 354.36, 'score': 0.907}, {'word': 'carry', 'start': 354.36, 'end': 354.78, 'score': 0.985}, {'word': 'out', 'start': 354.78, 'end': 354.92, 'score': 0.795}, {'word': 'this', 'start': 354.92, 'end': 355.08, 'score': 0.774}, {'word': 'task', 'start': 355.08, 'end': 355.36, 'score': 0.641}, {'word': 'for', 'start': 355.36, 'end': 355.6, 'score': 0.774}, {'word': 'you,', 'start': 355.6, 'end': 355.84, 'score': 0.701}, {'word': 'if', 'start': 355.84, 'end': 356.521, 'score': 0.989}, {'word': 'you', 'start': 356.521, 'end': 356.681, 'score': 0.842}, {'word': 'can', 'start': 356.681, 'end': 356.841, 'score': 0.797}, {'word': 'even', 'start': 356.841, 'end': 357.081, 'score': 0.822}, {'word': 'specify', 'start': 357.081, 'end': 357.781, 'score': 0.866}, {'word': 'what', 'start': 357.781, 'end': 358.202, 'score': 0.895}, {'word': 'snippets', 'start': 358.202, 'end': 358.662, 'score': 0.816}, {'word': 'of', 'start': 358.662, 'end': 358.782, 'score': 0.922}, {'word': 'text', 'start': 358.782, 'end': 359.182, 'score': 0.879}, {'word': 'they', 'start': 359.182, 'end': 359.422, 'score': 0.699}, {'word': 'should', 'start': 359.422, 'end': 359.602, 'score': 0.863}, {'word': 'read', 'start': 359.602, 'end': 359.842, 'score': 0.855}, {'word': 'in', 'start': 359.842, 'end': 359.982, 'score': 0.443}, {'word': 'advance', 'start': 359.982, 'end': 360.403, 'score': 0.867}, {'word': 'to', 'start': 360.403, 'end': 360.563, 'score': 0.991}, {'word': 'write', 'start': 360.563, 'end': 360.883, 'score': 0.78}, {'word': 'this', 'start': 360.883, 'end': 361.143, 'score': 0.486}, {'word': 'text', 'start': 361.143, 'end': 361.403, 'score': 0.84}, {'word': 'about', 'start': 361.403, 'end': 361.603, 'score': 0.37}, {'word': 'Alan', 'start': 361.603, 'end': 361.883, 'score': 0.707}, {'word': 'Turing,', 'start': 361.883, 'end': 362.284, 'score': 0.7}, {'word': 'then', 'start': 362.284, 'end': 362.784, 'score': 0.787}, {'word': 'that', 'start': 362.784, 'end': 362.984, 'score': 0.819}, {'word': 'even', 'start': 362.984, 'end': 363.424, 'score': 0.805}, {'word': 'better', 'start': 363.424, 'end': 363.964, 'score': 0.929}, {'word': 'sets', 'start': 363.964, 'end': 364.385, 'score': 0.901}, {'word': 'up', 'start': 364.385, 'end': 364.585, 'score': 0.836}, {'word': 'that', 'start': 364.585, 'end': 364.965, 'score': 0.875}, {'word': 'fresh', 'start': 364.965, 'end': 365.245, 'score': 0.909}, {'word': 'college', 'start': 365.245, 'end': 365.585, 'score': 0.806}, {'word': 'grad', 'start': 365.585, 'end': 365.805, 'score': 0.684}, {'word': 'for', 'start': 365.805, 'end': 365.966, 'score': 0.911}, {'word': 'success', 'start': 365.966, 'end': 366.406, 'score': 0.849}, {'word': 'to', 'start': 366.406, 'end': 366.606, 'score': 0.867}, {'word': 'carry', 'start': 366.606, 'end': 366.906, 'score': 0.846}, {'word': 'out', 'start': 366.906, 'end': 367.046, 'score': 0.973}, {'word': 'this', 'start': 367.046, 'end': 367.206, 'score': 0.705}, {'word': 'task', 'start': 367.206, 'end': 367.506, 'score': 0.714}, {'word': 'for', 'start': 367.506, 'end': 368.087, 'score': 0.871}, {'word': 'you.', 'start': 368.087, 'end': 368.247, 'score': 0.818}]}, {'start': 369.187, 'end': 377.913, 'text': ' So in the next video, you see examples of how to be clear and specific, which is an important principle of prompting LLMs.', 'words': [{'word': 'So', 'start': 369.187, 'end': 369.387, 'score': 0.902}, {'word': 'in', 'start': 369.387, 'end': 369.647, 'score': 0.903}, {'word': 'the', 'start': 369.647, 'end': 369.868, 'score': 0.865}, {'word': 'next', 'start': 369.868, 'end': 370.128, 'score': 0.799}, {'word': 'video,', 'start': 370.128, 'end': 370.488, 'score': 0.814}, {'word': 'you', 'start': 370.488, 'end': 370.708, 'score': 0.878}, {'word': 'see', 'start': 370.708, 'end': 370.948, 'score': 0.912}, {'word': 'examples', 'start': 370.948, 'end': 371.689, 'score': 0.827}, {'word': 'of', 'start': 371.689, 'end': 371.869, 'score': 0.86}, {'word': 'how', 'start': 371.869, 'end': 372.209, 'score': 0.952}, {'word': 'to', 'start': 372.209, 'end': 372.369, 'score': 0.957}, {'word': 'be', 'start': 372.369, 'end': 372.57, 'score': 0.807}, {'word': 'clear', 'start': 372.57, 'end': 373.29, 'score': 0.714}, {'word': 'and', 'start': 373.29, 'end': 373.55, 'score': 0.872}, {'word': 'specific,', 'start': 373.55, 'end': 374.251, 'score': 0.851}, {'word': 'which', 'start': 374.251, 'end': 374.531, 'score': 0.912}, {'word': 'is', 'start': 374.531, 'end': 374.691, 'score': 0.701}, {'word': 'an', 'start': 374.691, 'end': 374.951, 'score': 0.505}, {'word': 'important', 'start': 374.951, 'end': 375.432, 'score': 0.887}, {'word': 'principle', 'start': 375.432, 'end': 375.992, 'score': 0.842}, {'word': 'of', 'start': 375.992, 'end': 376.192, 'score': 0.959}, {'word': 'prompting', 'start': 376.192, 'end': 376.833, 'score': 0.857}, {'word': 'LLMs.', 'start': 376.833, 'end': 377.373, 'score': 0.756}]}, {'start': 377.913, 'end': 384.958, 'text': 'And you also learn from Isa a second principle of prompting that is giving a DLM time to think.', 'words': [{'word': 'And', 'start': 377.913, 'end': 378.013, 'score': 0.912}, {'word': 'you', 'start': 378.013, 'end': 378.294, 'score': 0.541}, {'word': 'also', 'start': 378.294, 'end': 378.714, 'score': 0.617}, {'word': 'learn', 'start': 378.714, 'end': 378.954, 'score': 0.82}, {'word': 'from', 'start': 378.954, 'end': 379.114, 'score': 0.824}, {'word': 'Isa', 'start': 379.114, 'end': 379.474, 'score': 0.714}, {'word': 'a', 'start': 379.474, 'end': 379.875, 'score': 0.964}, {'word': 'second', 'start': 379.875, 'end': 380.275, 'score': 0.793}, {'word': 'principle', 'start': 380.275, 'end': 380.835, 'score': 0.88}, {'word': 'of', 'start': 380.835, 'end': 380.995, 'score': 0.993}, {'word': 'prompting', 'start': 380.995, 'end': 381.556, 'score': 0.782}, {'word': 'that', 'start': 381.556, 'end': 382.056, 'score': 0.96}, {'word': 'is', 'start': 382.056, 'end': 382.236, 'score': 0.742}, {'word': 'giving', 'start': 382.236, 'end': 382.617, 'score': 0.909}, {'word': 'a', 'start': 382.617, 'end': 382.857, 'score': 0.812}, {'word': 'DLM', 'start': 382.857, 'end': 383.437, 'score': 0.814}, {'word': 'time', 'start': 383.437, 'end': 383.857, 'score': 0.993}, {'word': 'to', 'start': 383.857, 'end': 384.138, 'score': 0.793}, {'word': 'think.', 'start': 384.138, 'end': 384.378, 'score': 0.83}]}, {'start': 384.958, 'end': 387.14, 'text': \"So with that, let's go on to the next video.\", 'words': [{'word': 'So', 'start': 384.958, 'end': 385.078, 'score': 0.999}, {'word': 'with', 'start': 385.078, 'end': 385.278, 'score': 0.665}, {'word': 'that,', 'start': 385.278, 'end': 385.439, 'score': 0.329}, {'word': \"let's\", 'start': 385.439, 'end': 385.779, 'score': 0.96}, {'word': 'go', 'start': 385.779, 'end': 385.939, 'score': 0.663}, {'word': 'on', 'start': 385.939, 'end': 386.119, 'score': 0.954}, {'word': 'to', 'start': 386.119, 'end': 386.579, 'score': 0.804}, {'word': 'the', 'start': 386.579, 'end': 386.659, 'score': 0.971}, {'word': 'next', 'start': 386.659, 'end': 386.86, 'score': 0.72}, {'word': 'video.', 'start': 386.86, 'end': 387.14, 'score': 0.694}]}]\n"
          ]
        }
      ],
      "source": [
        "import whisperx\n",
        "import gc \n",
        "\n",
        "device = \"cuda\" \n",
        "audio_file = \"audio.wav\"\n",
        "batch_size = 16 # reduce if low on GPU mem\n",
        "compute_type = \"float16\" # change to \"int8\" if low on GPU mem (may reduce accuracy)\n",
        "\n",
        "# 1. Transcribe with original whisper (batched)\n",
        "model = whisperx.load_model(\"large-v2\", device, compute_type=compute_type)\n",
        "\n",
        "audio = whisperx.load_audio(audio_file)\n",
        "result1 = model.transcribe(audio, batch_size=batch_size)\n",
        "print(result1[\"segments\"]) # before alignment\n",
        "\n",
        "# delete model if low on GPU resources\n",
        "# import gc; gc.collect(); torch.cuda.empty_cache(); del model\n",
        "\n",
        "# 2. Align whisper output\n",
        "model_a, metadata = whisperx.load_align_model(language_code=result1[\"language\"], device=device)\n",
        "result = whisperx.align(result1[\"segments\"], model_a, metadata, audio, device, return_char_alignments=False)\n",
        "\n",
        "print(result[\"segments\"]) # after alignment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SeXdIBi7qpNf"
      },
      "outputs": [],
      "source": [
        "word_timestamps = result[\"word_segments\"]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result[\"word_segments\"][-1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3flxq_HzwILQ",
        "outputId": "4b5b8058-3da0-4225-e68b-c77cbaaa7b29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'word': 'video.', 'start': 386.86, 'end': 387.14, 'score': 0.694}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4JT4whSehBVm"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import os\n",
        "from faster_whisper import WhisperModel\n",
        "import whisperx\n",
        "import torch\n",
        "import librosa\n",
        "import soundfile\n",
        "from nemo.collections.asr.models.msdd_models import NeuralDiarizer\n",
        "from deepmultilingualpunctuation import PunctuationModel\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n4FR0f5Xk37L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1a3018d-182b-40fe-a2cf-c68b483c1d71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NeMo I 2023-05-11 16:56:55 msdd_models:1092] Loading pretrained diar_msdd_telephonic model from NGC\n",
            "[NeMo I 2023-05-11 16:56:55 cloud:68] Downloading from: https://api.ngc.nvidia.com/v2/models/nvidia/nemo/diar_msdd_telephonic/versions/1.0.1/files/diar_msdd_telephonic.nemo to /root/.cache/torch/NeMo/NeMo_1.17.0/diar_msdd_telephonic/3c3697a0a46f945574fa407149975a13/diar_msdd_telephonic.nemo\n",
            "[NeMo I 2023-05-11 16:56:59 common:913] Instantiating model from pre-trained checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[NeMo W 2023-05-11 16:57:01 modelPT:161] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
            "    Train config : \n",
            "    manifest_filepath: null\n",
            "    emb_dir: null\n",
            "    sample_rate: 16000\n",
            "    num_spks: 2\n",
            "    soft_label_thres: 0.5\n",
            "    labels: null\n",
            "    batch_size: 15\n",
            "    emb_batch_size: 0\n",
            "    shuffle: true\n",
            "    \n",
            "[NeMo W 2023-05-11 16:57:01 modelPT:168] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
            "    Validation config : \n",
            "    manifest_filepath: null\n",
            "    emb_dir: null\n",
            "    sample_rate: 16000\n",
            "    num_spks: 2\n",
            "    soft_label_thres: 0.5\n",
            "    labels: null\n",
            "    batch_size: 15\n",
            "    emb_batch_size: 0\n",
            "    shuffle: false\n",
            "    \n",
            "[NeMo W 2023-05-11 16:57:01 modelPT:174] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
            "    Test config : \n",
            "    manifest_filepath: null\n",
            "    emb_dir: null\n",
            "    sample_rate: 16000\n",
            "    num_spks: 2\n",
            "    soft_label_thres: 0.5\n",
            "    labels: null\n",
            "    batch_size: 15\n",
            "    emb_batch_size: 0\n",
            "    shuffle: false\n",
            "    seq_eval_mode: false\n",
            "    \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NeMo I 2023-05-11 16:57:01 features:287] PADDING: 16\n",
            "[NeMo I 2023-05-11 16:57:01 features:287] PADDING: 16\n",
            "[NeMo I 2023-05-11 16:57:02 save_restore_connector:247] Model EncDecDiarLabelModel was successfully restored from /root/.cache/torch/NeMo/NeMo_1.17.0/diar_msdd_telephonic/3c3697a0a46f945574fa407149975a13/diar_msdd_telephonic.nemo.\n",
            "[NeMo I 2023-05-11 16:57:02 features:287] PADDING: 16\n",
            "[NeMo I 2023-05-11 16:57:03 clustering_diarizer:127] Loading pretrained vad_multilingual_marblenet model from NGC\n",
            "[NeMo I 2023-05-11 16:57:03 cloud:68] Downloading from: https://api.ngc.nvidia.com/v2/models/nvidia/nemo/vad_multilingual_marblenet/versions/1.10.0/files/vad_multilingual_marblenet.nemo to /root/.cache/torch/NeMo/NeMo_1.17.0/vad_multilingual_marblenet/670f425c7f186060b7a7268ba6dfacb2/vad_multilingual_marblenet.nemo\n",
            "[NeMo I 2023-05-11 16:57:04 common:913] Instantiating model from pre-trained checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[NeMo W 2023-05-11 16:57:04 modelPT:161] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
            "    Train config : \n",
            "    manifest_filepath: /manifests/ami_train_0.63.json,/manifests/freesound_background_train.json,/manifests/freesound_laughter_train.json,/manifests/fisher_2004_background.json,/manifests/fisher_2004_speech_sampled.json,/manifests/google_train_manifest.json,/manifests/icsi_all_0.63.json,/manifests/musan_freesound_train.json,/manifests/musan_music_train.json,/manifests/musan_soundbible_train.json,/manifests/mandarin_train_sample.json,/manifests/german_train_sample.json,/manifests/spanish_train_sample.json,/manifests/french_train_sample.json,/manifests/russian_train_sample.json\n",
            "    sample_rate: 16000\n",
            "    labels:\n",
            "    - background\n",
            "    - speech\n",
            "    batch_size: 256\n",
            "    shuffle: true\n",
            "    is_tarred: false\n",
            "    tarred_audio_filepaths: null\n",
            "    tarred_shard_strategy: scatter\n",
            "    augmentor:\n",
            "      shift:\n",
            "        prob: 0.5\n",
            "        min_shift_ms: -10.0\n",
            "        max_shift_ms: 10.0\n",
            "      white_noise:\n",
            "        prob: 0.5\n",
            "        min_level: -90\n",
            "        max_level: -46\n",
            "        norm: true\n",
            "      noise:\n",
            "        prob: 0.5\n",
            "        manifest_path: /manifests/noise_0_1_musan_fs.json\n",
            "        min_snr_db: 0\n",
            "        max_snr_db: 30\n",
            "        max_gain_db: 300.0\n",
            "        norm: true\n",
            "      gain:\n",
            "        prob: 0.5\n",
            "        min_gain_dbfs: -10.0\n",
            "        max_gain_dbfs: 10.0\n",
            "        norm: true\n",
            "    num_workers: 16\n",
            "    pin_memory: true\n",
            "    \n",
            "[NeMo W 2023-05-11 16:57:04 modelPT:168] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
            "    Validation config : \n",
            "    manifest_filepath: /manifests/ami_dev_0.63.json,/manifests/freesound_background_dev.json,/manifests/freesound_laughter_dev.json,/manifests/ch120_moved_0.63.json,/manifests/fisher_2005_500_speech_sampled.json,/manifests/google_dev_manifest.json,/manifests/musan_music_dev.json,/manifests/mandarin_dev.json,/manifests/german_dev.json,/manifests/spanish_dev.json,/manifests/french_dev.json,/manifests/russian_dev.json\n",
            "    sample_rate: 16000\n",
            "    labels:\n",
            "    - background\n",
            "    - speech\n",
            "    batch_size: 256\n",
            "    shuffle: false\n",
            "    val_loss_idx: 0\n",
            "    num_workers: 16\n",
            "    pin_memory: true\n",
            "    \n",
            "[NeMo W 2023-05-11 16:57:04 modelPT:174] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
            "    Test config : \n",
            "    manifest_filepath: null\n",
            "    sample_rate: 16000\n",
            "    labels:\n",
            "    - background\n",
            "    - speech\n",
            "    batch_size: 128\n",
            "    shuffle: false\n",
            "    test_loss_idx: 0\n",
            "    \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NeMo I 2023-05-11 16:57:04 features:287] PADDING: 16\n",
            "[NeMo I 2023-05-11 16:57:05 save_restore_connector:247] Model EncDecClassificationModel was successfully restored from /root/.cache/torch/NeMo/NeMo_1.17.0/vad_multilingual_marblenet/670f425c7f186060b7a7268ba6dfacb2/vad_multilingual_marblenet.nemo.\n",
            "[NeMo I 2023-05-11 16:57:05 msdd_models:864] Multiscale Weights: [1, 1, 1, 1, 1]\n",
            "[NeMo I 2023-05-11 16:57:05 msdd_models:865] Clustering Parameters: {\n",
            "        \"oracle_num_speakers\": false,\n",
            "        \"max_num_speakers\": 8,\n",
            "        \"enhanced_count_thres\": 80,\n",
            "        \"max_rp_threshold\": 0.25,\n",
            "        \"sparse_search_volume\": 30,\n",
            "        \"maj_vote_spk_count\": false\n",
            "    }\n",
            "[NeMo I 2023-05-11 16:57:05 speaker_utils:93] Number of files to diarize: 1\n",
            "[NeMo I 2023-05-11 16:57:05 clustering_diarizer:309] Split long audio file to avoid CUDA memory issue\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "splitting manifest: 100%|██████████| 1/1 [00:00<00:00,  1.75it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NeMo I 2023-05-11 16:57:05 classification_models:263] Perform streaming frame-level VAD\n",
            "[NeMo I 2023-05-11 16:57:05 collections:298] Filtered duration for loading collection is 0.000000.\n",
            "[NeMo I 2023-05-11 16:57:05 collections:301] Dataset loaded with 8 items, total duration of  0.11 hours.\n",
            "[NeMo I 2023-05-11 16:57:05 collections:303] # 8 files loaded accounting to # 1 labels\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "vad: 100%|██████████| 8/8 [00:02<00:00,  3.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NeMo I 2023-05-11 16:57:08 clustering_diarizer:250] Generating predictions with overlapping input segments\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "                                                               "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NeMo I 2023-05-11 16:57:11 clustering_diarizer:262] Converting frame level prediction to speech/no-speech segment in start and end times format.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "creating speech segments: 100%|██████████| 1/1 [00:00<00:00,  3.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NeMo I 2023-05-11 16:57:11 clustering_diarizer:287] Subsegmentation for embedding extraction: scale0, nemo_outputs/speaker_outputs/subsegments_scale0.json\n",
            "[NeMo I 2023-05-11 16:57:11 clustering_diarizer:343] Extracting embeddings for Diarization\n",
            "[NeMo I 2023-05-11 16:57:11 collections:298] Filtered duration for loading collection is 0.000000.\n",
            "[NeMo I 2023-05-11 16:57:11 collections:301] Dataset loaded with 365 items, total duration of  0.13 hours.\n",
            "[NeMo I 2023-05-11 16:57:11 collections:303] # 365 files loaded accounting to # 1 labels\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "[1/5] extract embeddings: 100%|██████████| 6/6 [00:01<00:00,  5.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NeMo I 2023-05-11 16:57:13 clustering_diarizer:389] Saved embedding files to nemo_outputs/speaker_outputs/embeddings\n",
            "[NeMo I 2023-05-11 16:57:13 clustering_diarizer:287] Subsegmentation for embedding extraction: scale1, nemo_outputs/speaker_outputs/subsegments_scale1.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NeMo I 2023-05-11 16:57:13 clustering_diarizer:343] Extracting embeddings for Diarization\n",
            "[NeMo I 2023-05-11 16:57:13 collections:298] Filtered duration for loading collection is 0.000000.\n",
            "[NeMo I 2023-05-11 16:57:13 collections:301] Dataset loaded with 444 items, total duration of  0.13 hours.\n",
            "[NeMo I 2023-05-11 16:57:13 collections:303] # 444 files loaded accounting to # 1 labels\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[2/5] extract embeddings: 100%|██████████| 7/7 [00:01<00:00,  6.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NeMo I 2023-05-11 16:57:14 clustering_diarizer:389] Saved embedding files to nemo_outputs/speaker_outputs/embeddings\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NeMo I 2023-05-11 16:57:14 clustering_diarizer:287] Subsegmentation for embedding extraction: scale2, nemo_outputs/speaker_outputs/subsegments_scale2.json\n",
            "[NeMo I 2023-05-11 16:57:14 clustering_diarizer:343] Extracting embeddings for Diarization\n",
            "[NeMo I 2023-05-11 16:57:14 collections:298] Filtered duration for loading collection is 0.000000.\n",
            "[NeMo I 2023-05-11 16:57:14 collections:301] Dataset loaded with 554 items, total duration of  0.14 hours.\n",
            "[NeMo I 2023-05-11 16:57:14 collections:303] # 554 files loaded accounting to # 1 labels\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[3/5] extract embeddings: 100%|██████████| 9/9 [00:01<00:00,  7.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NeMo I 2023-05-11 16:57:15 clustering_diarizer:389] Saved embedding files to nemo_outputs/speaker_outputs/embeddings\n",
            "[NeMo I 2023-05-11 16:57:15 clustering_diarizer:287] Subsegmentation for embedding extraction: scale3, nemo_outputs/speaker_outputs/subsegments_scale3.json\n",
            "[NeMo I 2023-05-11 16:57:15 clustering_diarizer:343] Extracting embeddings for Diarization\n",
            "[NeMo I 2023-05-11 16:57:15 collections:298] Filtered duration for loading collection is 0.000000.\n",
            "[NeMo I 2023-05-11 16:57:15 collections:301] Dataset loaded with 745 items, total duration of  0.15 hours.\n",
            "[NeMo I 2023-05-11 16:57:15 collections:303] # 745 files loaded accounting to # 1 labels\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "[4/5] extract embeddings: 100%|██████████| 12/12 [00:01<00:00,  7.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NeMo I 2023-05-11 16:57:17 clustering_diarizer:389] Saved embedding files to nemo_outputs/speaker_outputs/embeddings\n",
            "[NeMo I 2023-05-11 16:57:17 clustering_diarizer:287] Subsegmentation for embedding extraction: scale4, nemo_outputs/speaker_outputs/subsegments_scale4.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NeMo I 2023-05-11 16:57:17 clustering_diarizer:343] Extracting embeddings for Diarization\n",
            "[NeMo I 2023-05-11 16:57:17 collections:298] Filtered duration for loading collection is 0.000000.\n",
            "[NeMo I 2023-05-11 16:57:17 collections:301] Dataset loaded with 1152 items, total duration of  0.15 hours.\n",
            "[NeMo I 2023-05-11 16:57:17 collections:303] # 1152 files loaded accounting to # 1 labels\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[5/5] extract embeddings: 100%|██████████| 18/18 [00:02<00:00,  6.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NeMo I 2023-05-11 16:57:20 clustering_diarizer:389] Saved embedding files to nemo_outputs/speaker_outputs/embeddings\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "clustering: 100%|██████████| 1/1 [00:06<00:00,  6.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NeMo I 2023-05-11 16:57:26 clustering_diarizer:464] Outputs are saved in /content/temp_outputs/nemo_outputs directory\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "[NeMo W 2023-05-11 16:57:26 der:106] Check if each ground truth RTTMs were present in the provided manifest file. Skipping calculation of Diariazation Error Rate\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NeMo I 2023-05-11 16:57:26 msdd_models:960] Loading embedding pickle file of scale:0 at nemo_outputs/speaker_outputs/embeddings/subsegments_scale0_embeddings.pkl\n",
            "[NeMo I 2023-05-11 16:57:26 msdd_models:960] Loading embedding pickle file of scale:1 at nemo_outputs/speaker_outputs/embeddings/subsegments_scale1_embeddings.pkl\n",
            "[NeMo I 2023-05-11 16:57:26 msdd_models:960] Loading embedding pickle file of scale:2 at nemo_outputs/speaker_outputs/embeddings/subsegments_scale2_embeddings.pkl\n",
            "[NeMo I 2023-05-11 16:57:26 msdd_models:960] Loading embedding pickle file of scale:3 at nemo_outputs/speaker_outputs/embeddings/subsegments_scale3_embeddings.pkl\n",
            "[NeMo I 2023-05-11 16:57:26 msdd_models:960] Loading embedding pickle file of scale:4 at nemo_outputs/speaker_outputs/embeddings/subsegments_scale4_embeddings.pkl\n",
            "[NeMo I 2023-05-11 16:57:26 msdd_models:938] Loading cluster label file from nemo_outputs/speaker_outputs/subsegments_scale4_cluster.label\n",
            "[NeMo I 2023-05-11 16:57:26 collections:612] Filtered duration for loading collection is 0.000000.\n",
            "[NeMo I 2023-05-11 16:57:26 collections:615] Total 1 session files loaded accounting to # 1 audio clips\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 25.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NeMo I 2023-05-11 16:57:26 msdd_models:1403]      [Threshold: 0.7000] [use_clus_as_main=False] [diar_window=50]\n",
            "[NeMo I 2023-05-11 16:57:26 speaker_utils:93] Number of files to diarize: 1\n",
            "[NeMo I 2023-05-11 16:57:26 speaker_utils:93] Number of files to diarize: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "[NeMo W 2023-05-11 16:57:26 der:106] Check if each ground truth RTTMs were present in the provided manifest file. Skipping calculation of Diariazation Error Rate\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NeMo I 2023-05-11 16:57:26 speaker_utils:93] Number of files to diarize: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[NeMo W 2023-05-11 16:57:26 der:106] Check if each ground truth RTTMs were present in the provided manifest file. Skipping calculation of Diariazation Error Rate\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NeMo I 2023-05-11 16:57:26 speaker_utils:93] Number of files to diarize: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[NeMo W 2023-05-11 16:57:26 der:106] Check if each ground truth RTTMs were present in the provided manifest file. Skipping calculation of Diariazation Error Rate\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NeMo I 2023-05-11 16:57:26 msdd_models:1431]   \n",
            "    \n"
          ]
        }
      ],
      "source": [
        "# convert audio to mono for NeMo combatibility\n",
        "signal, sample_rate = librosa.load(audio_file, sr=None)\n",
        "ROOT = os.getcwd()\n",
        "temp_path = os.path.join(ROOT, \"temp_outputs\")\n",
        "if not os.path.exists(temp_path):\n",
        "    os.mkdir(temp_path)\n",
        "os.chdir(temp_path)\n",
        "soundfile.write(\"mono_file.wav\", signal, sample_rate, \"PCM_24\")\n",
        "\n",
        "# Initialize NeMo MSDD diarization model\n",
        "msdd_model = NeuralDiarizer(cfg=create_config()).to(\"cuda\")\n",
        "msdd_model.diarize()\n",
        "\n",
        "del msdd_model\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Reading timestamps <> Speaker Labels mapping\n",
        "\n",
        "output_dir = \"nemo_outputs\"\n",
        "\n",
        "speaker_ts = []\n",
        "with open(f\"{output_dir}/pred_rttms/mono_file.rttm\", \"r\") as f:\n",
        "    lines = f.readlines()\n",
        "    for line in lines:\n",
        "        line_list = line.split(\" \")\n",
        "        s = int(float(line_list[5]) * 1000)\n",
        "        e = s + int(float(line_list[8]) * 1000)\n",
        "        speaker_ts.append([s, e, int(line_list[11].split(\"_\")[-1])])\n",
        "\n",
        "wsm = get_words_speaker_mapping(word_timestamps, speaker_ts, \"start\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2nAt6CmoI_7b"
      },
      "outputs": [],
      "source": [
        "wsm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W06PQ7aVJdSr"
      },
      "outputs": [],
      "source": [
        "result[\"segments\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CyP8aFc3tUSD"
      },
      "outputs": [],
      "source": [
        "if result1[\"language\"] in punct_model_langs:\n",
        "    # restoring punctuation in the transcript to help realign the sentences\n",
        "    punct_model = PunctuationModel(model=\"kredor/punctuate-all\")\n",
        "\n",
        "    words_list = list(map(lambda x: x[\"word\"], wsm))\n",
        "\n",
        "    labled_words = punct_model.predict(words_list)\n",
        "\n",
        "    ending_puncts = \".?!\"\n",
        "    model_puncts = \".,;:!?\"\n",
        "\n",
        "    # We don't want to punctuate U.S.A. with a period. Right?\n",
        "    is_acronym = lambda x: re.fullmatch(r\"\\b(?:[a-zA-Z]\\.){2,}\", x)\n",
        "\n",
        "    for word_dict, labeled_tuple in zip(wsm, labled_words):\n",
        "        word = word_dict[\"word\"]\n",
        "        if (\n",
        "                word\n",
        "                and labeled_tuple[1] in ending_puncts\n",
        "                and (word[-1] not in model_puncts or is_acronym(word))\n",
        "        ):\n",
        "            word += labeled_tuple[1]\n",
        "            if word.endswith(\"..\"):\n",
        "                word = word.rstrip(\".\")\n",
        "            word_dict[\"word\"] = word\n",
        "\n",
        "    wsm = get_realigned_ws_mapping_with_punctuation(wsm)\n",
        "else:\n",
        "    print(\n",
        "        f'Punctuation restoration is not available for {result1[\"language\"]} language.'\n",
        "    )\n",
        "\n",
        "ssm = get_sentences_speaker_mapping(wsm, speaker_ts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GYme9yk6wXEE"
      },
      "outputs": [],
      "source": [
        "ssm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result[\"segments\"]"
      ],
      "metadata": {
        "id": "4zw6xPsWZPzC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KsumJQhoMYdR"
      },
      "outputs": [],
      "source": [
        "pos=0\n",
        "out_f=[]\n",
        "for segment in result[\"segments\"]:\n",
        "  start = segment[\"start\"]\n",
        "  end = segment[\"end\"]\n",
        "  text = segment[\"text\"]\n",
        "  words = segment[\"words\"]\n",
        "  sentence = []\n",
        "  for i in range(len(words)):\n",
        "    segment_speaker=wsm[pos]\n",
        "    if i==0:\n",
        "      start=segment_speaker[\"start_time\"]\n",
        "    if (i+1)==len(words):\n",
        "      end=segment_speaker[\"end_time\"]\n",
        "    sentence.append(segment_speaker[\"word\"])\n",
        "    pos +=1\n",
        "  out_f.append({\n",
        "      \"start_time\":start,\n",
        "      \"end_time\":end,\n",
        "      \"text\":\" \".join(sentence),\n",
        "      \"speaker\":segment_speaker[\"speaker\"]\n",
        "  })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fv9AGe7TOgR9"
      },
      "outputs": [],
      "source": [
        "out_f"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#se unen los audios de un speaker \n",
        "from pydub import AudioSegment"
      ],
      "metadata": {
        "id": "6bF5nAqrwqtK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd .."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cgHWhBt0zDWG",
        "outputId": "1645724a-bb84-4509-bd88-074970375fb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_clone_voice={}\n",
        "silencio = AudioSegment.silent(duration=500)\n",
        "for speaker_data in out_f:\n",
        "  # Cargar el archivo de audio con pydub\n",
        "  audio = AudioSegment.from_wav(audio_file)\n",
        "\n",
        "  # Cortar el audio desde start_ms hasta end_ms\n",
        "  cut_audio = audio[speaker_data[\"start_time\"]:speaker_data[\"end_time\"]]\n",
        "  id_speaker=speaker_data[\"speaker\"]\n",
        "  if id_speaker not in base_clone_voice:\n",
        "    base_clone_voice[id_speaker]=cut_audio\n",
        "  else:\n",
        "    base_clone_voice[id_speaker] +=silencio+cut_audio\n",
        "for id_speaker,audio in base_clone_voice.items():\n",
        "  audio.export(\"speaker\"+str(id_speaker)+\".wav\", format=\"wav\")"
      ],
      "metadata": {
        "id": "T8nvAk1Lw0PL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Mq1R4luysNq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zbVxUlMky6Bv"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WkABJk02PRic",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a6d60642-89b0-4488-b860-9bb65afea049"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting TTS\n",
            "  Downloading TTS-0.13.3-cp310-cp310-manylinux1_x86_64.whl (655 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m655.3/655.3 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cython==0.29.28 (from TTS)\n",
            "  Downloading Cython-0.29.28-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m53.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from TTS) (1.10.1)\n",
            "Requirement already satisfied: torch>=1.7 in /usr/local/lib/python3.10/dist-packages (from TTS) (2.0.0+cu118)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (from TTS) (2.0.1+cu118)\n",
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.10/dist-packages (from TTS) (0.12.1)\n",
            "Requirement already satisfied: librosa==0.10.0.* in /usr/local/lib/python3.10/dist-packages (from TTS) (0.10.0.post2)\n",
            "Collecting inflect==5.6.0 (from TTS)\n",
            "  Downloading inflect-5.6.0-py3-none-any.whl (33 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from TTS) (4.65.0)\n",
            "Collecting anyascii (from TTS)\n",
            "  Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from TTS) (5.4.1)\n",
            "Requirement already satisfied: fsspec>=2021.04.0 in /usr/local/lib/python3.10/dist-packages (from TTS) (2023.4.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from TTS) (3.8.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from TTS) (23.1)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.10/dist-packages (from TTS) (2.2.4)\n",
            "Collecting pysbd (from TTS)\n",
            "  Downloading pysbd-0.3.4-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.1/71.1 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting umap-learn==0.5.1 (from TTS)\n",
            "  Downloading umap-learn-0.5.1.tar.gz (80 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.9/80.9 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from TTS) (1.5.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from TTS) (3.7.1)\n",
            "Collecting trainer==0.0.20 (from TTS)\n",
            "  Downloading trainer-0.0.20-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting coqpit>=0.0.16 (from TTS)\n",
            "  Downloading coqpit-0.0.17-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: jieba in /usr/local/lib/python3.10/dist-packages (from TTS) (0.42.1)\n",
            "Collecting pypinyin (from TTS)\n",
            "  Downloading pypinyin-0.48.0-py2.py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m82.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mecab-python3==1.0.5 (from TTS)\n",
            "  Downloading mecab_python3-1.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (581 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m581.1/581.1 kB\u001b[0m \u001b[31m53.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting unidic-lite==1.0.8 (from TTS)\n",
            "  Downloading unidic-lite-1.0.8.tar.gz (47.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.4/47.4 MB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gruut[de]==2.2.3 (from TTS)\n",
            "  Downloading gruut-2.2.3.tar.gz (73 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.5/73.5 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jamo (from TTS)\n",
            "  Downloading jamo-0.4.1-py3-none-any.whl (9.5 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from TTS) (3.8.1)\n",
            "Collecting g2pkk>=0.1.1 (from TTS)\n",
            "  Downloading g2pkk-0.1.2-py3-none-any.whl (25 kB)\n",
            "Collecting bangla==0.0.2 (from TTS)\n",
            "  Downloading bangla-0.0.2-py2.py3-none-any.whl (6.2 kB)\n",
            "Collecting bnnumerizer (from TTS)\n",
            "  Downloading bnnumerizer-0.0.2.tar.gz (4.7 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting bnunicodenormalizer==0.1.1 (from TTS)\n",
            "  Downloading bnunicodenormalizer-0.1.1.tar.gz (38 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from TTS) (1.22.4)\n",
            "Requirement already satisfied: numba==0.56.4 in /usr/local/lib/python3.10/dist-packages (from TTS) (0.56.4)\n",
            "Requirement already satisfied: Babel<3.0.0,>=2.8.0 in /usr/local/lib/python3.10/dist-packages (from gruut[de]==2.2.3->TTS) (2.12.1)\n",
            "Collecting dateparser~=1.1.0 (from gruut[de]==2.2.3->TTS)\n",
            "  Downloading dateparser-1.1.8-py2.py3-none-any.whl (293 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m293.8/293.8 kB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gruut-ipa<1.0,>=0.12.0 (from gruut[de]==2.2.3->TTS)\n",
            "  Downloading gruut-ipa-0.13.0.tar.gz (101 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gruut_lang_en~=2.0.0 (from gruut[de]==2.2.3->TTS)\n",
            "  Downloading gruut_lang_en-2.0.0.tar.gz (15.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.2/15.2 MB\u001b[0m \u001b[31m68.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jsonlines~=1.2.0 (from gruut[de]==2.2.3->TTS)\n",
            "  Downloading jsonlines-1.2.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Collecting networkx<3.0.0,>=2.5.0 (from gruut[de]==2.2.3->TTS)\n",
            "  Downloading networkx-2.8.8-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m60.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting num2words<1.0.0,>=0.5.10 (from gruut[de]==2.2.3->TTS)\n",
            "  Downloading num2words-0.5.12-py3-none-any.whl (125 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.2/125.2 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-crfsuite~=0.9.7 (from gruut[de]==2.2.3->TTS)\n",
            "  Downloading python_crfsuite-0.9.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (993 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m993.5/993.5 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gruut_lang_de~=2.0.0 (from gruut[de]==2.2.3->TTS)\n",
            "  Downloading gruut_lang_de-2.0.0.tar.gz (18.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.1/18.1 MB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa==0.10.0.*->TTS) (3.0.0)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from librosa==0.10.0.*->TTS) (1.2.2)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa==0.10.0.*->TTS) (1.2.0)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa==0.10.0.*->TTS) (4.4.2)\n",
            "Requirement already satisfied: pooch<1.7,>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa==0.10.0.*->TTS) (1.6.0)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa==0.10.0.*->TTS) (0.3.5)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from librosa==0.10.0.*->TTS) (4.5.0)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa==0.10.0.*->TTS) (0.2)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa==0.10.0.*->TTS) (1.0.5)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba==0.56.4->TTS) (0.39.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from numba==0.56.4->TTS) (65.5.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from trainer==0.0.20->TTS) (5.9.5)\n",
            "Requirement already satisfied: tensorboardX in /usr/local/lib/python3.10/dist-packages (from trainer==0.0.20->TTS) (2.6)\n",
            "Collecting protobuf<3.20,>=3.9.2 (from trainer==0.0.20->TTS)\n",
            "  Downloading protobuf-3.19.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m62.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pynndescent>=0.5 (from umap-learn==0.5.1->TTS)\n",
            "  Downloading pynndescent-0.5.10.tar.gz (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m59.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile->TTS) (1.15.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->TTS) (3.12.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->TTS) (1.11.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->TTS) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->TTS) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.7->TTS) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.7->TTS) (16.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->TTS) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->TTS) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->TTS) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->TTS) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->TTS) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->TTS) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->TTS) (1.3.1)\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from flask->TTS) (2.3.0)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from flask->TTS) (2.1.2)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from flask->TTS) (8.1.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->TTS) (1.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->TTS) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->TTS) (4.39.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->TTS) (1.4.4)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->TTS) (8.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->TTS) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->TTS) (2.8.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->TTS) (2022.10.31)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->TTS) (2022.7.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile->TTS) (2.21)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.10/dist-packages (from dateparser~=1.1.0->gruut[de]==2.2.3->TTS) (4.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.7->TTS) (2.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from jsonlines~=1.2.0->gruut[de]==2.2.3->TTS) (1.16.0)\n",
            "Requirement already satisfied: docopt>=0.6.2 in /usr/local/lib/python3.10/dist-packages (from num2words<1.0.0,>=0.5.10->gruut[de]==2.2.3->TTS) (0.6.2)\n",
            "Requirement already satisfied: appdirs>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from pooch<1.7,>=1.0->librosa==0.10.0.*->TTS) (1.4.4)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pooch<1.7,>=1.0->librosa==0.10.0.*->TTS) (2.27.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->librosa==0.10.0.*->TTS) (3.1.0)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.0->aiohttp->TTS) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.7->TTS) (1.3.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch<1.7,>=1.0->librosa==0.10.0.*->TTS) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch<1.7,>=1.0->librosa==0.10.0.*->TTS) (2022.12.7)\n",
            "Requirement already satisfied: pytz-deprecation-shim in /usr/local/lib/python3.10/dist-packages (from tzlocal->dateparser~=1.1.0->gruut[de]==2.2.3->TTS) (0.1.0.post0)\n",
            "Requirement already satisfied: tzdata in /usr/local/lib/python3.10/dist-packages (from pytz-deprecation-shim->tzlocal->dateparser~=1.1.0->gruut[de]==2.2.3->TTS) (2023.3)\n",
            "Building wheels for collected packages: bnunicodenormalizer, umap-learn, unidic-lite, bnnumerizer, gruut-ipa, gruut_lang_de, gruut_lang_en, pynndescent, gruut\n",
            "  Building wheel for bnunicodenormalizer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bnunicodenormalizer: filename=bnunicodenormalizer-0.1.1-py3-none-any.whl size=21895 sha256=faff34bc37bf296135c933cf4586304d1ca9b92b75a4e21fb2ceadccf836d407\n",
            "  Stored in directory: /root/.cache/pip/wheels/b4/f6/01/9e68ecec7c7ea85fc9431cfac42eba1c5a5f6debe5070de5c7\n",
            "  Building wheel for umap-learn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for umap-learn: filename=umap_learn-0.5.1-py3-none-any.whl size=76548 sha256=a81ba8ae0a6eeb8a15f14381d3cad93bb3db00b1c25d5d6749e28a1fa8a013ac\n",
            "  Stored in directory: /root/.cache/pip/wheels/69/21/8e/802cb9c4c606a67139f538cb17bf3bf1b98b739a7900469953\n",
            "  Building wheel for unidic-lite (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for unidic-lite: filename=unidic_lite-1.0.8-py3-none-any.whl size=47658818 sha256=d9759e164225efa4e20c8068854b0ce0a394225514a05b139e5b7f0789deee11\n",
            "  Stored in directory: /root/.cache/pip/wheels/89/e8/68/f9ac36b8cc6c8b3c96888cd57434abed96595d444f42243853\n",
            "  Building wheel for bnnumerizer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bnnumerizer: filename=bnnumerizer-0.0.2-py3-none-any.whl size=5259 sha256=80af6489553c36ed87d4c8ec7f2b002deab50c30955d24f3ca9a06b1c846c66e\n",
            "  Stored in directory: /root/.cache/pip/wheels/59/6b/e8/223172e7d5c9f72df3ea1a0d9258f3a8ab5b28e827728edef5\n",
            "  Building wheel for gruut-ipa (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gruut-ipa: filename=gruut_ipa-0.13.0-py3-none-any.whl size=104870 sha256=e96e42d9c0bad2a49724239b2f5cece3167594782d82dd339be581ffa9274799\n",
            "  Stored in directory: /root/.cache/pip/wheels/7b/18/49/e4f500ecdf0babe757953f844e4d7cd1ea81c5503c09bfe984\n",
            "  Building wheel for gruut_lang_de (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gruut_lang_de: filename=gruut_lang_de-2.0.0-py3-none-any.whl size=18498182 sha256=4f273bf2fe851922b63ea588a3c41b0951e01f3f06bc206ea1285b116831494c\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/9a/05/cfce98f0c41a1a540f15708c4a02df190b82d84cf91ef6bc7f\n",
            "  Building wheel for gruut_lang_en (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gruut_lang_en: filename=gruut_lang_en-2.0.0-py3-none-any.whl size=15297178 sha256=99dbc1ad934c88aa463bf149097b641c9ce366af07abfa7c6d76bb4a51f9880c\n",
            "  Stored in directory: /root/.cache/pip/wheels/10/9c/fb/77c655a9fbd78cdb9935d0ab65d80ddd0a3bcf7dbe18261650\n",
            "  Building wheel for pynndescent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pynndescent: filename=pynndescent-0.5.10-py3-none-any.whl size=55622 sha256=2cbd344393eeb29a0eb2f048dd33b17c1f5d9175f2e434a1bcba89783b8c4369\n",
            "  Stored in directory: /root/.cache/pip/wheels/4a/38/5d/f60a40a66a9512b7e5e83517ebc2d1b42d857be97d135f1096\n",
            "  Building wheel for gruut (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gruut: filename=gruut-2.2.3-py3-none-any.whl size=75799 sha256=ce198a1ee91499595177a4e2cbbbf29587bd09e358372f71900379dada4526e6\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/57/a8/f9de532daf5214f53644f20f3a9e6f69269453c87df9c0a817\n",
            "Successfully built bnunicodenormalizer umap-learn unidic-lite bnnumerizer gruut-ipa gruut_lang_de gruut_lang_en pynndescent gruut\n",
            "Installing collected packages: unidic-lite, python-crfsuite, mecab-python3, jamo, gruut_lang_en, gruut_lang_de, bnunicodenormalizer, bnnumerizer, bangla, pysbd, pypinyin, protobuf, num2words, networkx, jsonlines, inflect, gruut-ipa, cython, coqpit, anyascii, g2pkk, pynndescent, dateparser, umap-learn, gruut, trainer, TTS\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "  Attempting uninstall: networkx\n",
            "    Found existing installation: networkx 3.1\n",
            "    Uninstalling networkx-3.1:\n",
            "      Successfully uninstalled networkx-3.1\n",
            "  Attempting uninstall: inflect\n",
            "    Found existing installation: inflect 6.0.4\n",
            "    Uninstalling inflect-6.0.4:\n",
            "      Successfully uninstalled inflect-6.0.4\n",
            "  Attempting uninstall: cython\n",
            "    Found existing installation: Cython 0.29.34\n",
            "    Uninstalling Cython-0.29.34:\n",
            "      Successfully uninstalled Cython-0.29.34\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "onnx 1.14.0 requires protobuf>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "tensorflow 2.12.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 3.19.6 which is incompatible.\n",
            "tensorflow-datasets 4.9.2 requires protobuf>=3.20, but you have protobuf 3.19.6 which is incompatible.\n",
            "tensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 3.19.6 which is incompatible.\n",
            "whisperx 3.1.0 requires setuptools==65.6.3, but you have setuptools 65.5.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed TTS-0.13.3 anyascii-0.3.2 bangla-0.0.2 bnnumerizer-0.0.2 bnunicodenormalizer-0.1.1 coqpit-0.0.17 cython-0.29.28 dateparser-1.1.8 g2pkk-0.1.2 gruut-2.2.3 gruut-ipa-0.13.0 gruut_lang_de-2.0.0 gruut_lang_en-2.0.0 inflect-5.6.0 jamo-0.4.1 jsonlines-1.2.0 mecab-python3-1.0.5 networkx-2.8.8 num2words-0.5.12 protobuf-3.19.6 pynndescent-0.5.10 pypinyin-0.48.0 pysbd-0.3.4 python-crfsuite-0.9.9 trainer-0.0.20 umap-learn-0.5.1 unidic-lite-1.0.8\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google",
                  "inflect",
                  "networkx"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting argostranslate\n",
            "  Downloading argostranslate-1.8.0-py3-none-any.whl (27 kB)\n",
            "Collecting ctranslate2==2.24.0 (from argostranslate)\n",
            "  Downloading ctranslate2-2.24.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentencepiece==0.1.96 (from argostranslate)\n",
            "  Downloading sentencepiece-0.1.96-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m58.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting stanza==1.1.1 (from argostranslate)\n",
            "  Downloading stanza-1.1.1-py3-none-any.whl (227 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.6/227.6 kB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from ctranslate2==2.24.0->argostranslate) (1.22.4)\n",
            "Requirement already satisfied: pyyaml<7,>=5.3 in /usr/local/lib/python3.10/dist-packages (from ctranslate2==2.24.0->argostranslate) (5.4.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from stanza==1.1.1->argostranslate) (3.19.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from stanza==1.1.1->argostranslate) (2.27.1)\n",
            "Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from stanza==1.1.1->argostranslate) (2.0.0+cu118)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from stanza==1.1.1->argostranslate) (4.65.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza==1.1.1->argostranslate) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza==1.1.1->argostranslate) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza==1.1.1->argostranslate) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza==1.1.1->argostranslate) (2.8.8)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza==1.1.1->argostranslate) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza==1.1.1->argostranslate) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.3.0->stanza==1.1.1->argostranslate) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.3.0->stanza==1.1.1->argostranslate) (16.0.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->stanza==1.1.1->argostranslate) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->stanza==1.1.1->argostranslate) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->stanza==1.1.1->argostranslate) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->stanza==1.1.1->argostranslate) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.3.0->stanza==1.1.1->argostranslate) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.3.0->stanza==1.1.1->argostranslate) (1.3.0)\n",
            "Installing collected packages: sentencepiece, ctranslate2, stanza, argostranslate\n",
            "  Attempting uninstall: sentencepiece\n",
            "    Found existing installation: sentencepiece 0.1.99\n",
            "    Uninstalling sentencepiece-0.1.99:\n",
            "      Successfully uninstalled sentencepiece-0.1.99\n",
            "  Attempting uninstall: ctranslate2\n",
            "    Found existing installation: ctranslate2 3.13.0\n",
            "    Uninstalling ctranslate2-3.13.0:\n",
            "      Successfully uninstalled ctranslate2-3.13.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "faster-whisper 0.5.1 requires ctranslate2<4,>=3.10, but you have ctranslate2 2.24.0 which is incompatible.\n",
            "whisperx 3.1.0 requires setuptools==65.6.3, but you have setuptools 65.5.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed argostranslate-1.8.0 ctranslate2-2.24.0 sentencepiece-0.1.96 stanza-1.1.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "ctranslate2",
                  "sentencepiece"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install TTS\n",
        "!pip install argostranslate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NtWfB1jEP9r5"
      },
      "outputs": [],
      "source": [
        "import gc; gc.collect(); torch.cuda.empty_cache(); del model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yBXbtxUn6ATf"
      },
      "outputs": [],
      "source": [
        "import argostranslate.package\n",
        "import argostranslate.translate\n",
        "\n",
        "from_code = result1[\"language\"]\n",
        "to_code = \"es\"\n",
        "\n",
        "# Download and install Argos Translate package\n",
        "argostranslate.package.update_package_index()\n",
        "available_packages = argostranslate.package.get_available_packages()\n",
        "package_to_install = next(\n",
        "    filter(\n",
        "        lambda x: x.from_code == from_code and x.to_code == to_code, available_packages\n",
        "    )\n",
        ")\n",
        "argostranslate.package.install_from_path(package_to_install.download())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MA5a4mik6I66",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0245c775-355a-42e1-d179-1825855f05c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " > Downloading model to /root/.local/share/tts/tts_models--es--css10--vits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 101M/101M [00:02<00:00, 38.8MiB/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " > Model's license - bsd-3-clause\n",
            " > Check https://opensource.org/licenses for more info.\n",
            " > Using model: vits\n",
            " > Setting up Audio Processor...\n",
            " | > sample_rate:22050\n",
            " | > resample:False\n",
            " | > num_mels:80\n",
            " | > log_func:np.log10\n",
            " | > min_level_db:0\n",
            " | > frame_shift_ms:None\n",
            " | > frame_length_ms:None\n",
            " | > ref_level_db:None\n",
            " | > fft_size:1024\n",
            " | > power:None\n",
            " | > preemphasis:0.0\n",
            " | > griffin_lim_iters:None\n",
            " | > signal_norm:None\n",
            " | > symmetric_norm:None\n",
            " | > mel_fmin:0\n",
            " | > mel_fmax:None\n",
            " | > pitch_fmin:None\n",
            " | > pitch_fmax:None\n",
            " | > spec_gain:20.0\n",
            " | > stft_pad_mode:reflect\n",
            " | > max_norm:1.0\n",
            " | > clip_norm:True\n",
            " | > do_trim_silence:False\n",
            " | > trim_db:60\n",
            " | > do_sound_norm:False\n",
            " | > do_amp_to_db_linear:True\n",
            " | > do_amp_to_db_mel:True\n",
            " | > do_rms_norm:False\n",
            " | > db_level:None\n",
            " | > stats_path:None\n",
            " | > base:10\n",
            " | > hop_length:256\n",
            " | > win_length:1024\n",
            " > initialization of speaker-embedding layers.\n",
            " > initialization of language-embedding layers.\n"
          ]
        }
      ],
      "source": [
        "from TTS.api import TTS\n",
        "tts = TTS(\"tts_models/es/css10/vits\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DonZP2f97Wie"
      },
      "outputs": [],
      "source": [
        "from pydub import effects"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def speed_change(sound, speed=1.0):\n",
        "    # Manually override the frame_rate. This tells the computer how many\n",
        "    # samples to play per second\n",
        "    sound_with_altered_frame_rate = sound._spawn(sound.raw_data, overrides={\n",
        "        \"frame_rate\": int(sound.frame_rate * speed)\n",
        "    })\n",
        "\n",
        "    # convert the sound with altered frame rate to a standard frame rate\n",
        "    # so that regular playback programs will work right. They often only\n",
        "    # know how to play audio at standard frame rate (like 44.1k)\n",
        "    return sound_with_altered_frame_rate.set_frame_rate(sound.frame_rate)"
      ],
      "metadata": {
        "id": "v0eDVnV_YJGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o70GMuqMxxlH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d586cbc-8e74-406b-b2ff-6b72f093a185"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bienvenidos a este curso sobre ChatGPT Prompt Engineering para Desarrolladores.\n",
            " > Text splitted to sentences.\n",
            "['Bienvenidos a este curso sobre ChatGPT Prompt Engineering para Desarrolladores.']\n",
            " > Processing time: 2.222442865371704\n",
            " > Real-time factor: 0.37237739499579087\n",
            "cambiando velocidad: 1.62\n",
            "Estoy emocionado de tener conmigo Iza Fulford para enseñar esto junto conmigo.\n",
            " > Text splitted to sentences.\n",
            "['Estoy emocionado de tener conmigo Iza Fulford para enseñar esto junto conmigo.']\n",
            " > Processing time: 0.8036644458770752\n",
            " > Real-time factor: 0.13926192185016276\n",
            "cambiando velocidad: 1.25\n",
            "Es miembro del personal técnico de OpenAI y había construido el popular plugin de ChatGPT Retrieval.\n",
            " > Text splitted to sentences.\n",
            "['Es miembro del personal técnico de OpenAI y había construido el popular plugin de ChatGPT Retrieval.']\n",
            " > Processing time: 1.0344252586364746\n",
            " > Real-time factor: 0.13477992904967304\n",
            "cambiando velocidad: 1.28\n",
            "Y una gran parte de su trabajo ha estado enseñando a la gente cómo utilizar la tecnología OLM o el modelo de lenguaje grande en los productos.\n",
            " > Text splitted to sentences.\n",
            "['Y una gran parte de su trabajo ha estado enseñando a la gente cómo utilizar la tecnología OLM o el modelo de lenguaje grande en los productos.']\n",
            " > Processing time: 1.7948706150054932\n",
            " > Real-time factor: 0.192990252500932\n",
            "cambiando velocidad: 1.5\n",
            "También ha contribuido al OpenAI Cookbook que enseña a la gente a pedir.\n",
            " > Text splitted to sentences.\n",
            "['También ha contribuido al OpenAI Cookbook que enseña a la gente a pedir.']\n",
            " > Processing time: 0.6611628532409668\n",
            " > Real-time factor: 0.1471282184922828\n",
            "cambiando velocidad: 1.2\n",
            "Tan emocionada de tenerte conmigo.\n",
            " > Text splitted to sentences.\n",
            "['Tan emocionada de tenerte conmigo.']\n",
            " > Processing time: 0.47426772117614746\n",
            " > Real-time factor: 0.13939010519212588\n",
            "cambiando velocidad: 3.52\n",
            "Y estoy emocionada de estar aquí y compartir algunas mejores prácticas con todos ustedes.\n",
            " > Text splitted to sentences.\n",
            "['Y estoy emocionada de estar aquí y compartir algunas mejores prácticas con todos ustedes.']\n",
            " > Processing time: 0.9016127586364746\n",
            " > Real-time factor: 0.13915724974755198\n",
            "cambiando velocidad: 1.7\n",
            "Así que ha habido un montón de material en Internet para pedir con artículos como 30 impulsos que todos tienen que saber.\n",
            " > Text splitted to sentences.\n",
            "['Así que ha habido un montón de material en Internet para pedir con artículos como 30 impulsos que todos tienen que saber.']\n",
            " > Processing time: 1.673769235610962\n",
            " > Real-time factor: 0.22736946553241566\n",
            "cambiando velocidad: 1.12\n",
            "Mucho de eso se ha centrado en la interfaz de usuario web de chat GPT, que muchas personas están utilizando para hacer tareas específicas y a menudo únicas.\n",
            " > Text splitted to sentences.\n",
            "['Mucho de eso se ha centrado en la interfaz de usuario web de chat GPT, que muchas personas están utilizando para hacer tareas específicas y a menudo únicas.']\n",
            " > Processing time: 1.344970464706421\n",
            " > Real-time factor: 0.14570689581585852\n",
            "cambiando velocidad: 0.98\n",
            "Pero creo que el poder de LLMs, los modelos de lenguaje grande, como una herramienta de desarrollador que está usando llamadas API a LLMs para construir rápidamente aplicaciones de software, creo que todavía está muy poco apreciado.\n",
            " > Text splitted to sentences.\n",
            "['Pero creo que el poder de LLMs, los modelos de lenguaje grande, como una herramienta de desarrollador que está usando llamadas API a LLMs para construir rápidamente aplicaciones de software, creo que todavía está muy poco apreciado.']\n",
            " > Processing time: 2.708406925201416\n",
            " > Real-time factor: 0.21639070634779997\n",
            "cambiando velocidad: 0.99\n",
            "De hecho, mi equipo de AI Fund, que es una empresa hermana de Deep Learning AI, ha estado trabajando con muchas startups en la aplicación de estas tecnologías a muchas aplicaciones diferentes.\n",
            " > Text splitted to sentences.\n",
            "['De hecho, mi equipo de AI Fund, que es una empresa hermana de Deep Learning AI, ha estado trabajando con muchas startups en la aplicación de estas tecnologías a muchas aplicaciones diferentes.']\n",
            " > Processing time: 1.562666654586792\n",
            " > Real-time factor: 0.1474125527655844\n",
            "cambiando velocidad: 1.05\n",
            "Y ha sido emocionante ver qué API LLM puede permitir a los desarrolladores\n",
            " > Text splitted to sentences.\n",
            "['Y ha sido emocionante ver qué API LLM puede permitir a los desarrolladores']\n",
            " > Processing time: 0.649914026260376\n",
            " > Real-time factor: 0.14241735847353804\n",
            "cambiando velocidad: 0.97\n",
            "para construir rápidamente.\n",
            " > Text splitted to sentences.\n",
            "['para construir rápidamente.']\n",
            " > Processing time: 0.6752564907073975\n",
            " > Real-time factor: 0.21222071864449993\n",
            "cambiando velocidad: 2.22\n",
            "Así que en este curso, compartiremos con ustedes algunas de las posibilidades para lo que pueden hacer, así como las mejores prácticas para cómo pueden hacerlo.\n",
            " > Text splitted to sentences.\n",
            "['Así que en este curso, compartiremos con ustedes algunas de las posibilidades para lo que pueden hacer, así como las mejores prácticas para cómo pueden hacerlo.']\n",
            " > Processing time: 1.344182014465332\n",
            " > Real-time factor: 0.1406675403360191\n",
            "cambiando velocidad: 1.23\n",
            "Hay mucho material que cubrir.\n",
            " > Text splitted to sentences.\n",
            "['Hay mucho material que cubrir.']\n",
            " > Processing time: 0.469310998916626\n",
            " > Real-time factor: 0.13037073581575795\n",
            "cambiando velocidad: 2.35\n",
            "En primer lugar, usted aprenderá mejor, algunos que incitan a las mejores prácticas para el desarrollo de software.\n",
            " > Text splitted to sentences.\n",
            "['En primer lugar, usted aprenderá mejor, algunos que incitan a las mejores prácticas para el desarrollo de software.']\n",
            " > Processing time: 1.7379391193389893\n",
            " > Real-time factor: 0.22174774084242613\n",
            "cambiando velocidad: 2.01\n",
            "Luego cubriremos algunos casos de uso común, resumiendo, inferir, transformar, expandir, y luego construirás un chatbot usando un LLM.\n",
            " > Text splitted to sentences.\n",
            "['Luego cubriremos algunos casos de uso común, resumiendo, inferir, transformar, expandir, y luego construirás un chatbot usando un LLM.']\n",
            " > Processing time: 1.373215675354004\n",
            " > Real-time factor: 0.14673667152029438\n",
            "cambiando velocidad: 1.16\n",
            "Esperamos que esto brille su imaginación sobre nuevas aplicaciones que usted puede construir.\n",
            " > Text splitted to sentences.\n",
            "['Esperamos que esto brille su imaginación sobre nuevas aplicaciones que usted puede construir.']\n",
            " > Processing time: 0.7828209400177002\n",
            " > Real-time factor: 0.13730751024079077\n",
            "cambiando velocidad: 1.46\n",
            "Así que en el desarrollo de modelos de idiomas grandes o LLMs, se han producido amplios dos tipos de LLM, a los que voy a referirme como LLMs base y LLMs de instrucción.\n",
            " > Text splitted to sentences.\n",
            "['Así que en el desarrollo de modelos de idiomas grandes o LLMs, se han producido amplios dos tipos de LLM, a los que voy a referirme como LLMs base y LLMs de instrucción.']\n",
            " > Processing time: 1.3448262214660645\n",
            " > Real-time factor: 0.1415951284634365\n",
            "cambiando velocidad: 0.96\n",
            "Así que la base LLM ha sido entrenada para predecir la siguiente palabra basada en datos de formación de texto, a menudo entrenados en una gran cantidad de datos de Internet y otras fuentes para averiguar cuál es la siguiente palabra más probable a seguir.\n",
            " > Text splitted to sentences.\n",
            "['Así que la base LLM ha sido entrenada para predecir la siguiente palabra basada en datos de formación de texto, a menudo entrenados en una gran cantidad de datos de Internet y otras fuentes para averiguar cuál es la siguiente palabra más probable a seguir.']\n",
            " > Processing time: 2.3491437435150146\n",
            " > Real-time factor: 0.16516574264229528\n",
            "cambiando velocidad: 1.14\n",
            "Así que, por ejemplo, si usted iba a pedir esto, una vez que había unicornio, puede completar esto, es decir, puede predecir las siguientes palabras son que viven en un bosque mágico con todos los amigos unicornios.\n",
            " > Text splitted to sentences.\n",
            "['Así que, por ejemplo, si usted iba a pedir esto, una vez que había unicornio, puede completar esto, es decir, puede predecir las siguientes palabras son que viven en un bosque mágico con todos los amigos unicornios.']\n",
            " > Processing time: 2.3232171535491943\n",
            " > Real-time factor: 0.17582973473199975\n",
            "cambiando velocidad: 1.22\n",
            "Pero si fueras a incitarnos con lo que es la capital de Francia, entonces basado en lo que los artículos en internet podrían tener, es bastante posible que la base LLM complete esto con lo que es la ciudad más grande de Francia, lo que es la población de Francia y así sucesivamente, porque los artículos en internet podrían ser bastante plausiblemente listas de preguntas sobre el país de Francia.\n",
            " > Text splitted to sentences.\n",
            "['Pero si fueras a incitarnos con lo que es la capital de Francia, entonces basado en lo que los artículos en internet podrían tener, es bastante posible que la base LLM complete esto con lo que es la ciudad más grande de Francia, lo que es la población de Francia y así sucesivamente, porque los artículos en internet podrían ser bastante plausiblemente listas de preguntas sobre el país de Francia.']\n",
            " > Processing time: 7.504166126251221\n",
            " > Real-time factor: 0.36310799981531416\n",
            "cambiando velocidad: 0.92\n",
            "En cambio, se ha impartido capacitación a un LLM, que es el lugar donde se ha producido un gran impulso de la investigación y la práctica de la LLM, y se ha impartido capacitación para seguir instrucciones.\n",
            " > Text splitted to sentences.\n",
            "['En cambio, se ha impartido capacitación a un LLM, que es el lugar donde se ha producido un gran impulso de la investigación y la práctica de la LLM, y se ha impartido capacitación para seguir instrucciones.']\n",
            " > Processing time: 1.6891934871673584\n",
            " > Real-time factor: 0.14891062332901656\n",
            "cambiando velocidad: 0.97\n",
            "Así que si lo preguntas, ¿cuál es la capital de Francia?\n",
            " > Text splitted to sentences.\n",
            "['Así que si lo preguntas, ¿cuál es la capital de Francia?']\n",
            "['<BLNK>', 'a', '<BLNK>', 's', '<BLNK>', 'í', '<BLNK>', ' ', '<BLNK>', 'q', '<BLNK>', 'u', '<BLNK>', 'e', '<BLNK>', ' ', '<BLNK>', 's', '<BLNK>', 'i', '<BLNK>', ' ', '<BLNK>', 'l', '<BLNK>', 'o', '<BLNK>', ' ', '<BLNK>', 'p', '<BLNK>', 'r', '<BLNK>', 'e', '<BLNK>', 'g', '<BLNK>', 'u', '<BLNK>', 'n', '<BLNK>', 't', '<BLNK>', 'a', '<BLNK>', 's', '<BLNK>', ',', '<BLNK>', ' ', '<BLNK>', '¿', '<BLNK>', 'c', '<BLNK>', 'u', '<BLNK>', 'á', '<BLNK>', 'l', '<BLNK>', ' ', '<BLNK>', 'e', '<BLNK>', 's', '<BLNK>', ' ', '<BLNK>', 'l', '<BLNK>', 'a', '<BLNK>', ' ', '<BLNK>', 'c', '<BLNK>', 'a', '<BLNK>', 'p', '<BLNK>', 'i', '<BLNK>', 't', '<BLNK>', 'a', '<BLNK>', 'l', '<BLNK>', ' ', '<BLNK>', 'd', '<BLNK>', 'e', '<BLNK>', ' ', '<BLNK>', 'f', '<BLNK>', 'r', '<BLNK>', 'a', '<BLNK>', 'n', '<BLNK>', 'c', '<BLNK>', 'i', '<BLNK>', 'a', '<BLNK>', '?', '<BLNK>']\n",
            " [!] Character '¿' not found in the vocabulary. Discarding it.\n",
            " > Processing time: 0.5696806907653809\n",
            " > Real-time factor: 0.13977055402546562\n",
            "cambiando velocidad: 1.72\n",
            "Es mucho más probable que surja algo como la capital de Francia es París.\n",
            " > Text splitted to sentences.\n",
            "['Es mucho más probable que surja algo como la capital de Francia es París.']\n",
            " > Processing time: 0.7050352096557617\n",
            " > Real-time factor: 0.14055065070256714\n",
            "cambiando velocidad: 1.17\n",
            "Así que la forma en que las OLMs confinadas por la instrucción son normalmente entrenadas es que empiezas con un OLM base que ha sido entrenado en una gran cantidad de datos de texto y lo entrena más, más bien ajustarlo con entradas y salidas que son instrucciones y buenos intentos de seguir esas instrucciones.\n",
            " > Text splitted to sentences.\n",
            "['Así que la forma en que las OLMs confinadas por la instrucción son normalmente entrenadas es que empiezas con un OLM base que ha sido entrenado en una gran cantidad de datos de texto y lo entrena más, más bien ajustarlo con entradas y salidas que son instrucciones y buenos intentos de seguir esas instrucciones.']\n",
            " > Processing time: 4.992154598236084\n",
            " > Real-time factor: 0.2836217609636024\n",
            "cambiando velocidad: 1.08\n",
            "Y a menudo refina más a menudo utilizando una técnica llamada RLHF, el aprendizaje de refuerzo de la retroalimentación humana, para hacer el sistema más capaz de ser útil y seguir instrucciones.\n",
            " > Text splitted to sentences.\n",
            "['Y a menudo refina más a menudo utilizando una técnica llamada RLHF, el aprendizaje de refuerzo de la retroalimentación humana, para hacer el sistema más capaz de ser útil y seguir instrucciones.']\n",
            " > Processing time: 2.6669089794158936\n",
            " > Real-time factor: 0.22855132997061925\n",
            "cambiando velocidad: 1.12\n",
            "Debido a que se ha capacitado a las LLM en estudio para ser útiles, honestas e inofensivas, por ejemplo, son menos propensos a producir texto problemático, como salidas tóxicas, en comparación con la LLM base, muchos de los escenarios de uso práctico han estado cambiando hacia las LLM en estudio.\n",
            " > Text splitted to sentences.\n",
            "['Debido a que se ha capacitado a las LLM en estudio para ser útiles, honestas e inofensivas, por ejemplo, son menos propensos a producir texto problemático, como salidas tóxicas, en comparación con la LLM base, muchos de los escenarios de uso práctico han estado cambiando hacia las LLM en estudio.']\n",
            " > Processing time: 2.748847246170044\n",
            " > Real-time factor: 0.16637775533084861\n",
            "cambiando velocidad: 0.98\n",
            "Algunas de las mejores prácticas que encuentras en Internet pueden ser más adecuadas para una base LLM, pero para la mayoría de las aplicaciones prácticas hoy en día, recomendaríamos a la mayoría de las personas en lugar de centrarse en las LLMs de instrucción, que son más fáciles de usar, y también debido a la labor de OpenAI y otras empresas LLM que se vuelven más seguras y alineadas.\n",
            " > Text splitted to sentences.\n",
            "['Algunas de las mejores prácticas que encuentras en Internet pueden ser más adecuadas para una base LLM, pero para la mayoría de las aplicaciones prácticas hoy en día, recomendaríamos a la mayoría de las personas en lugar de centrarse en las LLMs de instrucción, que son más fáciles de usar, y también debido a la labor de OpenAI y otras empresas LLM que se vuelven más seguras y alineadas.']\n",
            " > Processing time: 3.5131993293762207\n",
            " > Real-time factor: 0.171829074273223\n",
            "Por lo tanto, este curso se centrará en las mejores prácticas para las LM instruidas.\n",
            " > Text splitted to sentences.\n",
            "['Por lo tanto, este curso se centrará en las mejores prácticas para las LM instruidas.']\n",
            " > Processing time: 1.272825002670288\n",
            " > Real-time factor: 0.22371023553181876\n",
            "cambiando velocidad: 1.28\n",
            "que es lo que le recomendamos utilizar para la mayoría de sus aplicaciones.\n",
            " > Text splitted to sentences.\n",
            "['que es lo que le recomendamos utilizar para la mayoría de sus aplicaciones.']\n",
            " > Processing time: 0.7313332557678223\n",
            " > Real-time factor: 0.13903554188233275\n",
            "cambiando velocidad: 1.57\n",
            "Antes de seguir adelante, solo quiero reconocer al equipo de OpenAI y DeepLearning. AI que había contribuido a los materiales que Yizi y yo presentaremos.\n",
            " > Text splitted to sentences.\n",
            "['Antes de seguir adelante, solo quiero reconocer al equipo de OpenAI y DeepLearning.', 'AI que había contribuido a los materiales que Yizi y yo presentaremos.']\n",
            " > Processing time: 2.1681206226348877\n",
            " > Real-time factor: 0.19906337328905427\n",
            "cambiando velocidad: 1.19\n",
            "Estoy muy agradecida a Andrew Main, Joe Palermo, Boris Power, Ted Sanders y Lillian Wang de OpenAI que estaban muy involucrados con nosotros materiales de almacenamiento de cerebros, revisando los materiales para reunir el plan de estudios para este breve curso.\n",
            " > Text splitted to sentences.\n",
            "['Estoy muy agradecida a Andrew Main, Joe Palermo, Boris Power, Ted Sanders y Lillian Wang de OpenAI que estaban muy involucrados con nosotros materiales de almacenamiento de cerebros, revisando los materiales para reunir el plan de estudios para este breve curso.']\n",
            " > Processing time: 2.2744760513305664\n",
            " > Real-time factor: 0.1565925117769864\n",
            "cambiando velocidad: 1.07\n",
            "Y también estoy agradecido por el lado del aprendizaje profundo por el trabajo de Jeff Ludwig, Eddie Hsu y Tommy Nelson.\n",
            " > Text splitted to sentences.\n",
            "['Y también estoy agradecido por el lado del aprendizaje profundo por el trabajo de Jeff Ludwig, Eddie Hsu y Tommy Nelson.']\n",
            " > Processing time: 1.777679204940796\n",
            " > Real-time factor: 0.22953847599634913\n",
            "cambiando velocidad: 1.47\n",
            "Así que cuando usas un LLM ajustado por instrucciones, piensa dar instrucciones a otra persona, decir alguien que es inteligente pero no conoce los detalles de tu tarea.\n",
            " > Text splitted to sentences.\n",
            "['Así que cuando usas un LLM ajustado por instrucciones, piensa dar instrucciones a otra persona, decir alguien que es inteligente pero no conoce los detalles de tu tarea.']\n",
            " > Processing time: 2.2035372257232666\n",
            " > Real-time factor: 0.22593183090542943\n",
            "cambiando velocidad: 0.93\n",
            "Así que cuando un LLM no funciona, a veces es porque las instrucciones no eran lo suficientemente claras.\n",
            " > Text splitted to sentences.\n",
            "['Así que cuando un LLM no funciona, a veces es porque las instrucciones no eran lo suficientemente claras.']\n",
            " > Processing time: 1.5871849060058594\n",
            " > Real-time factor: 0.22669076573627578\n",
            "cambiando velocidad: 1.4\n",
            "Por ejemplo, si dijeras, por favor escríbeme algo sobre Alan Turing.\n",
            " > Text splitted to sentences.\n",
            "['Por ejemplo, si dijeras, por favor escríbeme algo sobre Alan Turing.']\n",
            " > Processing time: 0.6318056583404541\n",
            " > Real-time factor: 0.13880235499767865\n",
            "cambiando velocidad: 1.04\n",
            "Bueno, además de eso, puede ser útil estar claro si quieres\n",
            " > Text splitted to sentences.\n",
            "['Bueno, además de eso, puede ser útil estar claro si quieres']\n",
            " > Processing time: 0.5682957172393799\n",
            " > Real-time factor: 0.13824934427546695\n",
            "cambiando velocidad: 0.86\n",
            "el texto para centrarse en su trabajo científico o su vida personal o su papel en la historia o algo más y si usted especifica lo que quiere que el tono del texto sea si toma el tono como un periodista profesional escribiría\n",
            " > Text splitted to sentences.\n",
            "['el texto para centrarse en su trabajo científico o su vida personal o su papel en la historia o algo más y si usted especifica lo que quiere que el tono del texto sea si toma el tono como un periodista profesional escribiría']\n",
            " > Processing time: 2.8738629817962646\n",
            " > Real-time factor: 0.23663395003811777\n",
            "cambiando velocidad: 0.82\n",
            "¿O es más de una nota casual que le das a un amigo?\n",
            " > Text splitted to sentences.\n",
            "['¿O es más de una nota casual que le das a un amigo?']\n",
            " > Processing time: 0.6979031562805176\n",
            " > Real-time factor: 0.21540823902555167\n",
            "cambiando velocidad: 1.11\n",
            "Eso sostiene.\n",
            " > Text splitted to sentences.\n",
            "['Eso sostiene.']\n",
            " > Processing time: 0.3675057888031006\n",
            " > Real-time factor: 0.1478741358231454\n",
            "cambiando velocidad: 4.43\n",
            "La OM genera lo que quieres.\n",
            " > Text splitted to sentences.\n",
            "['La OM genera lo que quieres.']\n",
            " > Processing time: 0.6453981399536133\n",
            " > Real-time factor: 0.2066061118753945\n",
            "cambiando velocidad: 1.96\n",
            "Y por supuesto, si te imaginas preguntando, digamos, un recién graduado de la universidad para llevar a cabo esta tarea para usted, si incluso puede especificar qué fragmentos de texto deben leer por adelantado para escribir este texto sobre Alan Turing, entonces eso aún mejor establece ese grad de la universidad fresca para el éxito para llevar a cabo esta tarea para usted.\n",
            " > Text splitted to sentences.\n",
            "['Y por supuesto, si te imaginas preguntando, digamos, un recién graduado de la universidad para llevar a cabo esta tarea para usted, si incluso puede especificar qué fragmentos de texto deben leer por adelantado para escribir este texto sobre Alan Turing, entonces eso aún mejor establece ese grad de la universidad fresca para el éxito para llevar a cabo esta tarea para usted.']\n",
            " > Processing time: 3.5294196605682373\n",
            " > Real-time factor: 0.16557107498112816\n",
            "cambiando velocidad: 1.12\n",
            "Así que en el próximo video, se ven ejemplos de cómo ser claro y específico, que es un principio importante de impulsar las LLMs.\n",
            " > Text splitted to sentences.\n",
            "['Así que en el próximo video, se ven ejemplos de cómo ser claro y específico, que es un principio importante de impulsar las LLMs.']\n",
            " > Processing time: 1.7679619789123535\n",
            " > Real-time factor: 0.22524476307558355\n",
            "cambiando velocidad: 0.96\n",
            "Y usted también aprende de Isa un segundo principio de incitar que está dando un tiempo DLM para pensar.\n",
            " > Text splitted to sentences.\n",
            "['Y usted también aprende de Isa un segundo principio de incitar que está dando un tiempo DLM para pensar.']\n",
            " > Processing time: 1.5586283206939697\n",
            " > Real-time factor: 0.23024811388748817\n",
            "cambiando velocidad: 1.05\n",
            "Así que con eso, vamos al siguiente video.\n",
            " > Text splitted to sentences.\n",
            "['Así que con eso, vamos al siguiente video.']\n",
            " > Processing time: 0.48567771911621094\n",
            " > Real-time factor: 0.13193861751567676\n",
            "cambiando velocidad: 1.68\n"
          ]
        }
      ],
      "source": [
        "#for speach in ssm:\n",
        "voices =[]\n",
        "for speach in out_f:\n",
        "  # Ruta del archivo de audio\n",
        "  #audio_file = \"audio.wav\"\n",
        "\n",
        "  # Tiempo de inicio y fin en milisegundos\n",
        "  start_ms = speach['start_time']\n",
        "  end_ms = speach['end_time']\n",
        "  speaker = speach['speaker']\n",
        "\n",
        "  # Cargar el archivo de audio con pydub\n",
        "  audio = AudioSegment.from_wav(audio_file)\n",
        "\n",
        "  # Cortar el audio desde start_ms hasta end_ms\n",
        "  cut_audio = audio[start_ms:end_ms]\n",
        "  # Translate\n",
        "  translatedText = argostranslate.translate.translate(speach[\"text\"], from_code, to_code)\n",
        "  print(translatedText)\n",
        "  tiempo_audio = (speach['end_time']-speach['start_time'])/1000\n",
        "  voice = tts.tts_with_vc_to_file(\n",
        "      translatedText,\n",
        "      speaker_wav=\"speaker\"+str(speaker)+\".wav\",\n",
        "      file_path=\"ouptut.wav\"\n",
        "  )\n",
        "  voicedub = AudioSegment.from_wav(\"ouptut.wav\")\n",
        "  duracion_actual =voicedub.duration_seconds # No puede estar por debajo de 1.0\n",
        "  velocidad = round(duracion_actual/tiempo_audio,2)\n",
        "  if velocidad!=1:\n",
        "    print(\"cambiando velocidad: \"+str(velocidad))\n",
        "    voicedub=speed_change(voicedub, velocidad)\n",
        "  #if velocidad>=1:\n",
        "  #  voicedub = voicedub.speedup(velocidad )\n",
        "  #else:\n",
        "  #  voicedub = voicedub.speed_down(velocidad )\n",
        "  voices.append({\n",
        "      \"start_time\":start_ms,\n",
        "      \"end_time\":end_ms,\n",
        "      \"segment\":voicedub\n",
        "  })\n",
        "  gc.collect(); torch.cuda.empty_cache();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AM0QT_2cTRIs"
      },
      "outputs": [],
      "source": [
        "duration=voices[0][\"start_time\"]\n",
        "silencio = AudioSegment.silent(duration=duration)\n",
        "audio_final=silencio+voices[0][\"segment\"]\n",
        "for i in range(len(voices)-1):\n",
        "  duration=voices[i+1][\"start_time\"]-voices[i][\"end_time\"]\n",
        "  silencio = AudioSegment.silent(duration=duration)\n",
        "  audio_final +=silencio+voices[i+1][\"segment\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QKqFHxuUSUXP",
        "outputId": "5c2813b1-c1d6-48a2-f8f7-b9dbefd18e62"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_io.BufferedRandom name='audio_final.wav'>"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "audio_final.export(\"audio_final.wav\", format=\"wav\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "audio_final.duration_seconds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5mR-MN897BT9",
        "outputId": "b18ad118-61bd-46ec-942c-d2d2d5dfe72b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "386.97833333333335"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "mr1JF85eVtzV"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Audio\n",
        "wn = Audio('audio_final.wav', autoplay=False)\n",
        "display(wn)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ffmpeg -i video.mp4 -i audio_final.wav -map 0:v -map 1:a -c:v copy -c:a aac -shortest output_video.mp4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SGnU1vMm4R-o",
        "outputId": "04a85c12-4d5b-48f4-d9ce-935d8f0c53bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ffmpeg version 4.2.7-0ubuntu0.1 Copyright (c) 2000-2022 the FFmpeg developers\n",
            "  built with gcc 9 (Ubuntu 9.4.0-1ubuntu1~20.04.1)\n",
            "  configuration: --prefix=/usr --extra-version=0ubuntu0.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-avresample --disable-filter=resample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librsvg --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-nvenc --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
            "  libavutil      56. 31.100 / 56. 31.100\n",
            "  libavcodec     58. 54.100 / 58. 54.100\n",
            "  libavformat    58. 29.100 / 58. 29.100\n",
            "  libavdevice    58.  8.100 / 58.  8.100\n",
            "  libavfilter     7. 57.100 /  7. 57.100\n",
            "  libavresample   4.  0.  0 /  4.  0.  0\n",
            "  libswscale      5.  5.100 /  5.  5.100\n",
            "  libswresample   3.  5.100 /  3.  5.100\n",
            "  libpostproc    55.  5.100 / 55.  5.100\n",
            "Input #0, mov,mp4,m4a,3gp,3g2,mj2, from 'video.mp4':\n",
            "  Metadata:\n",
            "    major_brand     : mp42\n",
            "    minor_version   : 0\n",
            "    compatible_brands: mp42mp41\n",
            "    creation_time   : 2023-04-29T04:26:57.000000Z\n",
            "  Duration: 00:06:27.88, start: 0.000000, bitrate: 2008 kb/s\n",
            "    Stream #0:0(eng): Video: h264 (Main) (avc1 / 0x31637661), yuv420p(tv, bt709), 1920x1080, 1813 kb/s, 29.97 fps, 29.97 tbr, 30k tbn, 60k tbc (default)\n",
            "    Metadata:\n",
            "      creation_time   : 2023-04-29T04:26:57.000000Z\n",
            "      handler_name    : ?Mainconcept Video Media Handler\n",
            "      encoder         : AVC Coding\n",
            "    Stream #0:1(eng): Audio: aac (LC) (mp4a / 0x6134706D), 48000 Hz, stereo, fltp, 189 kb/s (default)\n",
            "    Metadata:\n",
            "      creation_time   : 2023-04-29T04:26:57.000000Z\n",
            "      handler_name    : #Mainconcept MP4 Sound Media Handler\n",
            "\u001b[0;33mGuessed Channel Layout for Input Stream #1.0 : mono\n",
            "\u001b[0mInput #1, wav, from 'audio_final.wav':\n",
            "  Duration: 00:06:26.98, bitrate: 384 kb/s\n",
            "    Stream #1:0: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 24000 Hz, mono, s16, 384 kb/s\n",
            "Stream mapping:\n",
            "  Stream #0:0 -> #0:0 (copy)\n",
            "  Stream #1:0 -> #0:1 (pcm_s16le (native) -> aac (native))\n",
            "Press [q] to stop, [?] for help\n",
            "Output #0, mp4, to 'output_video.mp4':\n",
            "  Metadata:\n",
            "    major_brand     : mp42\n",
            "    minor_version   : 0\n",
            "    compatible_brands: mp42mp41\n",
            "    encoder         : Lavf58.29.100\n",
            "    Stream #0:0(eng): Video: h264 (Main) (avc1 / 0x31637661), yuv420p(tv, bt709), 1920x1080, q=2-31, 1813 kb/s, 29.97 fps, 29.97 tbr, 30k tbn, 30k tbc (default)\n",
            "    Metadata:\n",
            "      creation_time   : 2023-04-29T04:26:57.000000Z\n",
            "      handler_name    : ?Mainconcept Video Media Handler\n",
            "      encoder         : AVC Coding\n",
            "    Stream #0:1: Audio: aac (LC) (mp4a / 0x6134706D), 24000 Hz, mono, fltp, 69 kb/s\n",
            "    Metadata:\n",
            "      encoder         : Lavc58.54.100 aac\n",
            "frame=11599 fps=4570 q=-1.0 Lsize=   88828kB time=00:06:26.98 bitrate=1880.4kbits/s speed= 152x    \n",
            "video:85431kB audio:3093kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.342936%\n",
            "\u001b[1;36m[aac @ 0x564001e6efc0] \u001b[0mQavg: 5940.302\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3773fb143de346e69212e6df96662f46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5214b15778924e52b1d56c57c2348659",
              "IPY_MODEL_4db24865b7f3465e8d4dddf5e33848f1",
              "IPY_MODEL_cc0a3360cc0d4498a4db438cb5000fd5"
            ],
            "layout": "IPY_MODEL_18cade3631fd4b91a5ba5201ce300832"
          }
        },
        "5214b15778924e52b1d56c57c2348659": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0249095d9236437d8760ea041bce6dab",
            "placeholder": "​",
            "style": "IPY_MODEL_3e75f9740e7c4936ac1a71ceaae6eb50",
            "value": "Downloading (…)7a179508/config.json: 100%"
          }
        },
        "4db24865b7f3465e8d4dddf5e33848f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f9bec7c53e984f9dba4d41ad85ba895c",
            "max": 2796,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c1d7b1cb65c94b92a167a23336386cb0",
            "value": 2796
          }
        },
        "cc0a3360cc0d4498a4db438cb5000fd5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c3b0bf3319e640d7a017f6be6c02ba45",
            "placeholder": "​",
            "style": "IPY_MODEL_682a3957f82a48c6826855cea5aeadfc",
            "value": " 2.80k/2.80k [00:00&lt;00:00, 58.9kB/s]"
          }
        },
        "18cade3631fd4b91a5ba5201ce300832": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0249095d9236437d8760ea041bce6dab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3e75f9740e7c4936ac1a71ceaae6eb50": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f9bec7c53e984f9dba4d41ad85ba895c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c1d7b1cb65c94b92a167a23336386cb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c3b0bf3319e640d7a017f6be6c02ba45": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "682a3957f82a48c6826855cea5aeadfc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "57aa0e9a55b347e895209fb9115c6513": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2cccf41982634129b51fd5dde7d48862",
              "IPY_MODEL_7e84773afe36483a877403c1afdc8453",
              "IPY_MODEL_b6f4254416144b228a6dd7164bae69d9"
            ],
            "layout": "IPY_MODEL_429669e094b049d38485f0f4d9b4f9c1"
          }
        },
        "2cccf41982634129b51fd5dde7d48862": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5d109e51c5f94965978a8c12f4d0861b",
            "placeholder": "​",
            "style": "IPY_MODEL_51b161891a2c4c268385a723155da2c3",
            "value": "Downloading model.bin: 100%"
          }
        },
        "7e84773afe36483a877403c1afdc8453": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8cfd49af459543c1b69772d937d9cee6",
            "max": 3086912962,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6c9a9dc7aed742e68b982aad753a7dae",
            "value": 3086912962
          }
        },
        "b6f4254416144b228a6dd7164bae69d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5c6a159fe5474ee6b8f5993d6d260bc7",
            "placeholder": "​",
            "style": "IPY_MODEL_b886f32c5aa84c479960b5b8e05895dd",
            "value": " 3.09G/3.09G [00:52&lt;00:00, 147MB/s]"
          }
        },
        "429669e094b049d38485f0f4d9b4f9c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5d109e51c5f94965978a8c12f4d0861b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "51b161891a2c4c268385a723155da2c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8cfd49af459543c1b69772d937d9cee6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c9a9dc7aed742e68b982aad753a7dae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5c6a159fe5474ee6b8f5993d6d260bc7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b886f32c5aa84c479960b5b8e05895dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "968be373cf3c424da921e4aa3d2ab8a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d2f04589571c4a4cad3321cb2dcc3bf9",
              "IPY_MODEL_bfe1cf170bfa423d8d26a297f2aea398",
              "IPY_MODEL_34685e5b4bcc409c82e96cac6d6a9ca3"
            ],
            "layout": "IPY_MODEL_ac5ecd58c65946af8c6a3f9ba9546203"
          }
        },
        "d2f04589571c4a4cad3321cb2dcc3bf9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7b3aff328b5d43b88d0a8399536f26a9",
            "placeholder": "​",
            "style": "IPY_MODEL_c4d56e37dea24a6c812e698905e02794",
            "value": "Downloading (…)79508/vocabulary.txt: 100%"
          }
        },
        "bfe1cf170bfa423d8d26a297f2aea398": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e187310dbedf42aebfec7c930e6f1329",
            "max": 459861,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_43768afed1c64e1da9cb9b5617497e4e",
            "value": 459861
          }
        },
        "34685e5b4bcc409c82e96cac6d6a9ca3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9968d21744c147059f60f361b5a40aaa",
            "placeholder": "​",
            "style": "IPY_MODEL_1efa7f9770e940e5824e04f18b1a208c",
            "value": " 460k/460k [00:00&lt;00:00, 5.49MB/s]"
          }
        },
        "ac5ecd58c65946af8c6a3f9ba9546203": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b3aff328b5d43b88d0a8399536f26a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c4d56e37dea24a6c812e698905e02794": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e187310dbedf42aebfec7c930e6f1329": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "43768afed1c64e1da9cb9b5617497e4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9968d21744c147059f60f361b5a40aaa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1efa7f9770e940e5824e04f18b1a208c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3e746ab0734d45fc94725e370711fbf1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_45bc9895384b42a4870d0ec8a2b9f5b6",
              "IPY_MODEL_5ffcc9fea8c24c61b627d9fef5bfb0ba",
              "IPY_MODEL_ebfd0fa65d2446b48bc33b93fc8b3a02"
            ],
            "layout": "IPY_MODEL_07c80d01f3814ef59e15939bf5703aa1"
          }
        },
        "45bc9895384b42a4870d0ec8a2b9f5b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9d1bb10333694b578af8cda92cc3859d",
            "placeholder": "​",
            "style": "IPY_MODEL_6939d409875b41f49ef609090d32774b",
            "value": "Downloading (…)79508/tokenizer.json: 100%"
          }
        },
        "5ffcc9fea8c24c61b627d9fef5bfb0ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ad00042d90d94169bc128ecda22def06",
            "max": 2203239,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cb8606425d6f45878472e234996b622d",
            "value": 2203239
          }
        },
        "ebfd0fa65d2446b48bc33b93fc8b3a02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_58cdc726fb094aa8ae71490642403490",
            "placeholder": "​",
            "style": "IPY_MODEL_8f6e70c1277a47449e3dde923cd878c7",
            "value": " 2.20M/2.20M [00:00&lt;00:00, 21.5MB/s]"
          }
        },
        "07c80d01f3814ef59e15939bf5703aa1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9d1bb10333694b578af8cda92cc3859d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6939d409875b41f49ef609090d32774b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ad00042d90d94169bc128ecda22def06": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb8606425d6f45878472e234996b622d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "58cdc726fb094aa8ae71490642403490": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8f6e70c1277a47449e3dde923cd878c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}